{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"vmcnet Flexible, general-purpose VMC framework, built on JAX . Installation Python 3.9 is required, and a virtual environment is recommended. After cloning, use pip install -e . to install the package in editable/develop mode, or pip install -e .[testing] if planning on running tests. If running on a GPU, CUDA needs to be set up properly to work with JAX, and you will need to install the correct jaxlib wheel. See, e.g., https://github.com/google/jax#installation. Philosophy and usage This repository is built to serve two purposes: 1. Provide a general python API for variational Monte Carlo calculations compatible with JAX, with a number of built-in neural network architectures for ready-use. 2. Provide a command-line interface exposing a large number of options for more streamlined (but somewhat less custom) experimentation with architecture/optimization/sampling hyperparameters. This repository was built as a JAX port of an internal TensorFlow project started in 2019 which itself was initially inspired by the work of David Pfau, James S. Spencer, Alexander G. D. G. Matthews, and W. M. C. Foulkes . Their repository (and its own JAX branch) can be found here . Python API The primary routine exposed by this repository which implements the VMC training loop is the train.vmc.vmc_loop function. This function implements a very generic unsupervised training loop. A skeleton of a script which performs varational Monte Carlo would look something like: import jax import vmcnet.mcmc as mcmc import vmcnet.train as train # Training hyperparameters nchains = ... nburn = ... nepochs = ... seed = ... logdir = ... checkpoint_every = ... checkpoint_dir = ... # Initialize parameters, data, and optimization state params = ... data = ... optimizer_state = ... # Walker function (get new data) def walker_fn ( params , data , key ): ... return accept_ratio , data , key # Define how the parameters are updated def update_param_fn ( params , data , optimizer_state , key ): ... return params , optimizer_state , metrics , key # (Optionally) burn samples def burning_step ( params , data , key ): ... return data , key key = jax . random . PRNGKey ( seed ) data , key = mcmc . metropolis . burn_data ( burning_step , nburn , params , data , key ) # Train! params , optimizer_state , data , key = train . vmc . vmc_loop ( params , optimizer_state , data , nchains , nepochs , walker_fn , update_param_fn , key , logdir , checkpoint_every = checkpoint_every , checkpoint_dir = checkpoint_dir , ) Note the required function signatures. A simple but complete working example can be found in the hydrogen-like atom example in the test suite. Command-line Alternatively, a command-line interface has been implemented which provides more streamlined access to subsets of the repository via setting ConfigDict objects. There are two scripts which have been exposed thus far: vmc-molecule and vmc-statistics . The primary command vmc-molecule calls train.runners.run_molecule . See train.default_config.get_default_config() to explore the options which have been exposed to the command-line. To edit these options at the command-line, use the \" --config. \" prefix. For example, vmc-molecule \\ --config.problem.ion_pos = \"((0.0, 0.0, -2.0), (0.0, 0.0, 2.0))\" \\ --config.problem.ion_charges = \"(7.0, 7.0)\" \\ --config.problem.nelec = \"(7, 7)\" \\ --config.model.ferminet.full_det = \"True\" \\ --config.logging_level = \"INFO\" will train the full single-determinant FermiNet on the nitrogen molecule at dissociating bond length 4.0 for 2e5 epochs on 2000 walkers (which are distributed across any available GPUs, if supported by the installation). You can also reload and evaluate or continue training from previous checkpoints via the \" --reload. \" prefix. The options can be seen in train.default_config.get_default_reload_config() . The reloading will only occur if --reload.logdir is set. The vmc-statistics command calls train.runners.vmc_statistics . This simple script is designed to be compatible with the output of an evaluation run with vmc-molecule , but can accept any path to a file which contains local energies (a file with nchains x nepochs energies). It computes and saves a json file containing the average energy, the sample variance, the estimated integrated autocorrelation, and the estimated standard error. The options can be viewed simply via vmc-statistics -h . Contributing See how to contribute . Cite A preprint of the paper which originally introduced this repository can be found at https://arxiv.org/abs/2112.03491, which can be cited via: @misc{lin2021explicitly, title={Explicitly antisymmetrized neural network layers for variational Monte Carlo simulation}, author={Jeffmin Lin and Gil Goldshlager and Lin Lin}, year={2021}, eprint={2112.03491}, archivePrefix={arXiv}, primaryClass={physics.comp-ph} } If citing version 0.1.0 of this GitHub repository directly, you can use the following citation: @software{vmcnet2021github, author = {Jeffmin Lin and Gil Goldshlager and Lin Lin}, title = {{VMCNet}: Flexible, general-purpose {VMC} framework, built on {JAX}}, url = {http://github.com/jeffminlin/vmcnet}, version = {0.1.0}, year = {2021}, }","title":"Home"},{"location":"#vmcnet","text":"Flexible, general-purpose VMC framework, built on JAX .","title":"vmcnet"},{"location":"#installation","text":"Python 3.9 is required, and a virtual environment is recommended. After cloning, use pip install -e . to install the package in editable/develop mode, or pip install -e .[testing] if planning on running tests. If running on a GPU, CUDA needs to be set up properly to work with JAX, and you will need to install the correct jaxlib wheel. See, e.g., https://github.com/google/jax#installation.","title":"Installation"},{"location":"#philosophy-and-usage","text":"This repository is built to serve two purposes: 1. Provide a general python API for variational Monte Carlo calculations compatible with JAX, with a number of built-in neural network architectures for ready-use. 2. Provide a command-line interface exposing a large number of options for more streamlined (but somewhat less custom) experimentation with architecture/optimization/sampling hyperparameters. This repository was built as a JAX port of an internal TensorFlow project started in 2019 which itself was initially inspired by the work of David Pfau, James S. Spencer, Alexander G. D. G. Matthews, and W. M. C. Foulkes . Their repository (and its own JAX branch) can be found here .","title":"Philosophy and usage"},{"location":"#python-api","text":"The primary routine exposed by this repository which implements the VMC training loop is the train.vmc.vmc_loop function. This function implements a very generic unsupervised training loop. A skeleton of a script which performs varational Monte Carlo would look something like: import jax import vmcnet.mcmc as mcmc import vmcnet.train as train # Training hyperparameters nchains = ... nburn = ... nepochs = ... seed = ... logdir = ... checkpoint_every = ... checkpoint_dir = ... # Initialize parameters, data, and optimization state params = ... data = ... optimizer_state = ... # Walker function (get new data) def walker_fn ( params , data , key ): ... return accept_ratio , data , key # Define how the parameters are updated def update_param_fn ( params , data , optimizer_state , key ): ... return params , optimizer_state , metrics , key # (Optionally) burn samples def burning_step ( params , data , key ): ... return data , key key = jax . random . PRNGKey ( seed ) data , key = mcmc . metropolis . burn_data ( burning_step , nburn , params , data , key ) # Train! params , optimizer_state , data , key = train . vmc . vmc_loop ( params , optimizer_state , data , nchains , nepochs , walker_fn , update_param_fn , key , logdir , checkpoint_every = checkpoint_every , checkpoint_dir = checkpoint_dir , ) Note the required function signatures. A simple but complete working example can be found in the hydrogen-like atom example in the test suite.","title":"Python API"},{"location":"#command-line","text":"Alternatively, a command-line interface has been implemented which provides more streamlined access to subsets of the repository via setting ConfigDict objects. There are two scripts which have been exposed thus far: vmc-molecule and vmc-statistics . The primary command vmc-molecule calls train.runners.run_molecule . See train.default_config.get_default_config() to explore the options which have been exposed to the command-line. To edit these options at the command-line, use the \" --config. \" prefix. For example, vmc-molecule \\ --config.problem.ion_pos = \"((0.0, 0.0, -2.0), (0.0, 0.0, 2.0))\" \\ --config.problem.ion_charges = \"(7.0, 7.0)\" \\ --config.problem.nelec = \"(7, 7)\" \\ --config.model.ferminet.full_det = \"True\" \\ --config.logging_level = \"INFO\" will train the full single-determinant FermiNet on the nitrogen molecule at dissociating bond length 4.0 for 2e5 epochs on 2000 walkers (which are distributed across any available GPUs, if supported by the installation). You can also reload and evaluate or continue training from previous checkpoints via the \" --reload. \" prefix. The options can be seen in train.default_config.get_default_reload_config() . The reloading will only occur if --reload.logdir is set. The vmc-statistics command calls train.runners.vmc_statistics . This simple script is designed to be compatible with the output of an evaluation run with vmc-molecule , but can accept any path to a file which contains local energies (a file with nchains x nepochs energies). It computes and saves a json file containing the average energy, the sample variance, the estimated integrated autocorrelation, and the estimated standard error. The options can be viewed simply via vmc-statistics -h .","title":"Command-line"},{"location":"#contributing","text":"See how to contribute .","title":"Contributing"},{"location":"#cite","text":"A preprint of the paper which originally introduced this repository can be found at https://arxiv.org/abs/2112.03491, which can be cited via: @misc{lin2021explicitly, title={Explicitly antisymmetrized neural network layers for variational Monte Carlo simulation}, author={Jeffmin Lin and Gil Goldshlager and Lin Lin}, year={2021}, eprint={2112.03491}, archivePrefix={arXiv}, primaryClass={physics.comp-ph} } If citing version 0.1.0 of this GitHub repository directly, you can use the following citation: @software{vmcnet2021github, author = {Jeffmin Lin and Gil Goldshlager and Lin Lin}, title = {{VMCNet}: Flexible, general-purpose {VMC} framework, built on {JAX}}, url = {http://github.com/jeffminlin/vmcnet}, version = {0.1.0}, year = {2021}, }","title":"Cite"},{"location":"CONTRIBUTING/","text":"Contributing We welcome contributions in the form of code, discussion, GitHub issues, or other avenues! If you wish to contribute a new feature or would like to address an outstanding todo/bug with the code, it can be very helpful to share your intentions via an issue to discuss the design plan and implementation details. If contributing code, basic knowledge of git is assumed. Please start by forking the repository, and install your fork locally using Python 3.9, making sure to include the test suite ( pip install -e .[testing] ). Add the main repository https://github.com/jeffminlin/vmcnet as an upstream remote to keep your fork synced as you develop (via fetch and rebase). Create a branch on your fork for your development, ideally with a useful name that reflects your feature/bug-fix/etc. After you implement your proposed code changes in your development branch, please run the code linters and formatters via black . and lint.sh (or have them run automatically by your editor -- we haven't yet set up git pre-commit hooks). Please also run our tests via pytest --cov=vmcnet --run_very_slow and contribute to the test suite to help maintain the overall code quality. When ready, you can submit your changes for review by opening a pull request against the main repository. We use mypy , flake8 , and pydocstyle to check code for style, and our linting configuration has been set up to follow the black formatter. We expect new contributions to include descriptive docstrings, and to be type hinted reasonably consistently. A code review and passing tests (via GitHub CI) will be required before your pull request can be merged.","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"We welcome contributions in the form of code, discussion, GitHub issues, or other avenues! If you wish to contribute a new feature or would like to address an outstanding todo/bug with the code, it can be very helpful to share your intentions via an issue to discuss the design plan and implementation details. If contributing code, basic knowledge of git is assumed. Please start by forking the repository, and install your fork locally using Python 3.9, making sure to include the test suite ( pip install -e .[testing] ). Add the main repository https://github.com/jeffminlin/vmcnet as an upstream remote to keep your fork synced as you develop (via fetch and rebase). Create a branch on your fork for your development, ideally with a useful name that reflects your feature/bug-fix/etc. After you implement your proposed code changes in your development branch, please run the code linters and formatters via black . and lint.sh (or have them run automatically by your editor -- we haven't yet set up git pre-commit hooks). Please also run our tests via pytest --cov=vmcnet --run_very_slow and contribute to the test suite to help maintain the overall code quality. When ready, you can submit your changes for review by opening a pull request against the main repository. We use mypy , flake8 , and pydocstyle to check code for style, and our linting configuration has been set up to follow the black formatter. We expect new contributions to include descriptive docstrings, and to be type hinted reasonably consistently. A code review and passing tests (via GitHub CI) will be required before your pull request can be merged.","title":"Contributing"},{"location":"api/api_nav/","text":"examples harmonic_oscillator hydrogen_like_atom mcmc dynamic_width_position_amplitude metropolis position_amplitude_core simple_position_amplitude statistics models antiequivariance antisymmetry construct core equivariance jastrow sign_symmetry weights physics core kinetic potential train default_config parse_config_flags runners vmc updates params parse_config sr utils checkpoint distribute kfac log_linear_exp pytree_helpers slog_helpers","title":"Api nav"},{"location":"api/examples/harmonic_oscillator/","text":"Harmonic oscillator model. HarmonicOscillatorOrbitals ( Module ) dataclass Create orbitals for a set of non-interacting quantum harmonic oscillators. The single-particle quantum harmonic oscillator for particle i has the Hamiltonian H = -(1/2) d^2/dx^2 + (1/2) (omega * x)^2, where the first term is the second derivative/1-D Laplacian and the second term is the harmonic oscillator potential. The corresponding energy eigenfunctions take the simple form psi_n(x) = C * exp(-omega * x^2 / 2) * H_n(sqrt(omega) * x), where C is a normalizing constant and H_n is the nth Hermite polynomial. The corresponding energy level is simply E_n = omega * (n + (1/2)). With N non-interacting particles, the total many-body Hamiltonian is simply H = sum_i H_i, and the above single-particle energy eigenfunctions become analogous to molecular orbitals. Due to the antisymmetry requirement on particle states, the N particles cannot occupy the same orbitals, so in the ground state configuration, the particles fill the lowest energy orbitals first. If some fixed spin configuration is specified, then the corresponding ground state configuration is the one where the particles fill the lowest energy orbitals per spin. For a single spin, the model evaluates the lowest n energy orbitals, where n is the number of particles for that spin. In this model, each spin corresponds to a leaf in the input pytree x. If x is a single array (spinless input), then when this omega matches the omega in the potential, then model.apply(params, x) evaluates the first n eigenfunctions of the Hamiltonian on x. If there are multiple leaves in x, each of which is a different spin, then this behavior is mapped over each leaf. In other words, the .apply method has the signature (params, [x]) -> [orbital_matrix], where x has shape (..., n, 1) and orbital_matrix has shape (..., n, n), and the bracket notation is used to denote a pytree structure of spin. For example, if the inputs are (params, {\"up\": x_up, \"down\": x_down}), then the output will be {\"up\": orbitals_up, \"down\": orbitals_down}. The params consist of a single number, a model-specific omega. Because the particles are completely non-interacting in this model, there are no interactions computed between the leaves of the input pytree (or even between particles in a single leaf). Attributes: Name Type Description omega_init jnp.float32 initial value for omega in the model; when this matches the omega in the potential energy, then these orbitals become eigenfunctions __call__ ( self , xs ) special Compute the harmonic oscillator orbitals on each leaf of xs. Source code in vmcnet/examples/harmonic_oscillator.py def __call__ ( self , xs ): \"\"\"Compute the harmonic oscillator orbitals on each leaf of xs.\"\"\" return jax . tree_map ( self . _single_leaf_call , xs ) make_hermite_polynomials ( x ) Compute the first n Hermite polynomials evaluated at n points. Because the Hermite polynomials are orthogonal, they have a three-term recurrence given by H_n(x) = 2 * x * H_{n-1}(x) - 2 * (n - 1) * H_{n-2}(x). When there are n particles x_1, ..., x_n, this function simply evaluates the first n Hermite polynomials at these positions to form an nxn matrix with the particle index along the first axis and the polynomials along the second. Parameters: Name Type Description Default x Array an array of shape (..., nparticles, 1) required Returns: Type Description Array an array of shape (..., nparticles, nparticles), where the (..., i, j)th entry corresponds to H_j(x_i). Source code in vmcnet/examples/harmonic_oscillator.py def make_hermite_polynomials ( x : Array ) -> Array : \"\"\"Compute the first n Hermite polynomials evaluated at n points. Because the Hermite polynomials are orthogonal, they have a three-term recurrence given by H_n(x) = 2 * x * H_{n-1}(x) - 2 * (n - 1) * H_{n-2}(x). When there are n particles x_1, ..., x_n, this function simply evaluates the first n Hermite polynomials at these positions to form an nxn matrix with the particle index along the first axis and the polynomials along the second. Args: x (Array): an array of shape (..., nparticles, 1) Returns: Array: an array of shape (..., nparticles, nparticles), where the (..., i, j)th entry corresponds to H_j(x_i). \"\"\" nparticles = x . shape [ - 2 ] if nparticles == 1 : return jnp . ones_like ( x ) H = [ jnp . ones_like ( x ), 2 * x ] for n in range ( 2 , nparticles ): H . append ( 2 * x * H [ n - 1 ] - 2 * ( n - 1 ) * H [ n - 2 ]) return jnp . concatenate ( H , axis =- 1 ) make_harmonic_oscillator_spin_half_model ( nspin_first , model_omega_init ) Create a spin-1/2 quantum harmonic oscillator wavefunction (two spins). Parameters: Name Type Description Default nspin_first int number of the alpha spin type, where the number of spins of each type is (alpha, beta); the model assumes that its input along the second-to-last dimension has size alpha + beta required model_omega_init jnp.float32 spring constant inside the model; when it matches the spring constant in the hamiltonian, then this model evaluates an eigenstate required Returns: Type Description models.core.Module spin-1/2 wavefunction for the quantum harmonic oscillator, with one trainable parameter (the model omega) Source code in vmcnet/examples/harmonic_oscillator.py def make_harmonic_oscillator_spin_half_model ( nspin_first : int , model_omega_init : jnp . float32 ) -> models . core . Module : \"\"\"Create a spin-1/2 quantum harmonic oscillator wavefunction (two spins). Args: nspin_first (int): number of the alpha spin type, where the number of spins of each type is (alpha, beta); the model assumes that its input along the second-to-last dimension has size alpha + beta model_omega_init (jnp.float32): spring constant inside the model; when it matches the spring constant in the hamiltonian, then this model evaluates an eigenstate Returns: models.core.Module: spin-1/2 wavefunction for the quantum harmonic oscillator, with one trainable parameter (the model omega) \"\"\" def split_spin_fn ( x ): return jnp . split ( x , [ nspin_first ], axis =- 2 ) orbitals = HarmonicOscillatorOrbitals ( model_omega_init ) logdet_fn = models . antisymmetry . logdet_product return models . core . ComposedModel ([ split_spin_fn , orbitals , logdet_fn ]) harmonic_oscillator_potential ( omega , x ) Potential energy for independent harmonic oscillators with spring constant omega. This function computes sum_i 0.5 * (omega * x_i)^2. If x has more than one axis, these are simply summed over, so this corresponds to an isotropic harmonic oscillator potential. This function should be vmapped in order to be applied to batches of inputs, as it expects the first axis of x to correspond to the particle index. Parameters: Name Type Description Default omega jnp.float32 spring constant required x Array array of particle positions, where the first axis corresponds to particle index required Returns: Type Description jnp.float32 potential energy value for this configuration x Source code in vmcnet/examples/harmonic_oscillator.py def harmonic_oscillator_potential ( omega : jnp . float32 , x : Array ) -> jnp . float32 : \"\"\"Potential energy for independent harmonic oscillators with spring constant omega. This function computes sum_i 0.5 * (omega * x_i)^2. If x has more than one axis, these are simply summed over, so this corresponds to an isotropic harmonic oscillator potential. This function should be vmapped in order to be applied to batches of inputs, as it expects the first axis of x to correspond to the particle index. Args: omega (jnp.float32): spring constant x (Array): array of particle positions, where the first axis corresponds to particle index Returns: jnp.float32: potential energy value for this configuration x \"\"\" return 0.5 * jnp . sum ( jnp . square ( omega * x )) make_harmonic_oscillator_local_energy ( omega , log_psi_apply ) Factory to create a local energy fn for the harmonic oscillator log|psi|. Parameters: Name Type Description Default omega jnp.float32 spring constant for the harmonic oscillator required log_psi_apply Callable function which evaluates log|psi| for a harmonic oscillator model wavefunction psi. Has the signature (params, x) -> log|psi(x)|. required Returns: Type Description Callable local energy function with the signature (params, x) -> local energy associated to the wavefunction psi Source code in vmcnet/examples/harmonic_oscillator.py def make_harmonic_oscillator_local_energy ( omega : jnp . float32 , log_psi_apply : ModelApply [ P ] ) -> ModelApply [ P ]: \"\"\"Factory to create a local energy fn for the harmonic oscillator log|psi|. Args: omega (jnp.float32): spring constant for the harmonic oscillator log_psi_apply (Callable): function which evaluates log|psi| for a harmonic oscillator model wavefunction psi. Has the signature (params, x) -> log|psi(x)|. Returns: Callable: local energy function with the signature (params, x) -> local energy associated to the wavefunction psi \"\"\" kinetic_fn = physics . kinetic . create_continuous_kinetic_energy ( log_psi_apply ) def potential_fn ( params : P , x : Array ): del params return harmonic_oscillator_potential ( omega , x ) potential_fn = jax . vmap ( potential_fn , in_axes = ( None , 0 ), out_axes = 0 ) return physics . core . combine_local_energy_terms ([ kinetic_fn , potential_fn ])","title":"harmonic_oscillator"},{"location":"api/examples/harmonic_oscillator/#vmcnet.examples.harmonic_oscillator.HarmonicOscillatorOrbitals","text":"Create orbitals for a set of non-interacting quantum harmonic oscillators. The single-particle quantum harmonic oscillator for particle i has the Hamiltonian H = -(1/2) d^2/dx^2 + (1/2) (omega * x)^2, where the first term is the second derivative/1-D Laplacian and the second term is the harmonic oscillator potential. The corresponding energy eigenfunctions take the simple form psi_n(x) = C * exp(-omega * x^2 / 2) * H_n(sqrt(omega) * x), where C is a normalizing constant and H_n is the nth Hermite polynomial. The corresponding energy level is simply E_n = omega * (n + (1/2)). With N non-interacting particles, the total many-body Hamiltonian is simply H = sum_i H_i, and the above single-particle energy eigenfunctions become analogous to molecular orbitals. Due to the antisymmetry requirement on particle states, the N particles cannot occupy the same orbitals, so in the ground state configuration, the particles fill the lowest energy orbitals first. If some fixed spin configuration is specified, then the corresponding ground state configuration is the one where the particles fill the lowest energy orbitals per spin. For a single spin, the model evaluates the lowest n energy orbitals, where n is the number of particles for that spin. In this model, each spin corresponds to a leaf in the input pytree x. If x is a single array (spinless input), then when this omega matches the omega in the potential, then model.apply(params, x) evaluates the first n eigenfunctions of the Hamiltonian on x. If there are multiple leaves in x, each of which is a different spin, then this behavior is mapped over each leaf. In other words, the .apply method has the signature (params, [x]) -> [orbital_matrix], where x has shape (..., n, 1) and orbital_matrix has shape (..., n, n), and the bracket notation is used to denote a pytree structure of spin. For example, if the inputs are (params, {\"up\": x_up, \"down\": x_down}), then the output will be {\"up\": orbitals_up, \"down\": orbitals_down}. The params consist of a single number, a model-specific omega. Because the particles are completely non-interacting in this model, there are no interactions computed between the leaves of the input pytree (or even between particles in a single leaf). Attributes: Name Type Description omega_init jnp.float32 initial value for omega in the model; when this matches the omega in the potential energy, then these orbitals become eigenfunctions","title":"HarmonicOscillatorOrbitals"},{"location":"api/examples/harmonic_oscillator/#vmcnet.examples.harmonic_oscillator.HarmonicOscillatorOrbitals.__call__","text":"Compute the harmonic oscillator orbitals on each leaf of xs. Source code in vmcnet/examples/harmonic_oscillator.py def __call__ ( self , xs ): \"\"\"Compute the harmonic oscillator orbitals on each leaf of xs.\"\"\" return jax . tree_map ( self . _single_leaf_call , xs )","title":"__call__()"},{"location":"api/examples/harmonic_oscillator/#vmcnet.examples.harmonic_oscillator.make_hermite_polynomials","text":"Compute the first n Hermite polynomials evaluated at n points. Because the Hermite polynomials are orthogonal, they have a three-term recurrence given by H_n(x) = 2 * x * H_{n-1}(x) - 2 * (n - 1) * H_{n-2}(x). When there are n particles x_1, ..., x_n, this function simply evaluates the first n Hermite polynomials at these positions to form an nxn matrix with the particle index along the first axis and the polynomials along the second. Parameters: Name Type Description Default x Array an array of shape (..., nparticles, 1) required Returns: Type Description Array an array of shape (..., nparticles, nparticles), where the (..., i, j)th entry corresponds to H_j(x_i). Source code in vmcnet/examples/harmonic_oscillator.py def make_hermite_polynomials ( x : Array ) -> Array : \"\"\"Compute the first n Hermite polynomials evaluated at n points. Because the Hermite polynomials are orthogonal, they have a three-term recurrence given by H_n(x) = 2 * x * H_{n-1}(x) - 2 * (n - 1) * H_{n-2}(x). When there are n particles x_1, ..., x_n, this function simply evaluates the first n Hermite polynomials at these positions to form an nxn matrix with the particle index along the first axis and the polynomials along the second. Args: x (Array): an array of shape (..., nparticles, 1) Returns: Array: an array of shape (..., nparticles, nparticles), where the (..., i, j)th entry corresponds to H_j(x_i). \"\"\" nparticles = x . shape [ - 2 ] if nparticles == 1 : return jnp . ones_like ( x ) H = [ jnp . ones_like ( x ), 2 * x ] for n in range ( 2 , nparticles ): H . append ( 2 * x * H [ n - 1 ] - 2 * ( n - 1 ) * H [ n - 2 ]) return jnp . concatenate ( H , axis =- 1 )","title":"make_hermite_polynomials()"},{"location":"api/examples/harmonic_oscillator/#vmcnet.examples.harmonic_oscillator.make_harmonic_oscillator_spin_half_model","text":"Create a spin-1/2 quantum harmonic oscillator wavefunction (two spins). Parameters: Name Type Description Default nspin_first int number of the alpha spin type, where the number of spins of each type is (alpha, beta); the model assumes that its input along the second-to-last dimension has size alpha + beta required model_omega_init jnp.float32 spring constant inside the model; when it matches the spring constant in the hamiltonian, then this model evaluates an eigenstate required Returns: Type Description models.core.Module spin-1/2 wavefunction for the quantum harmonic oscillator, with one trainable parameter (the model omega) Source code in vmcnet/examples/harmonic_oscillator.py def make_harmonic_oscillator_spin_half_model ( nspin_first : int , model_omega_init : jnp . float32 ) -> models . core . Module : \"\"\"Create a spin-1/2 quantum harmonic oscillator wavefunction (two spins). Args: nspin_first (int): number of the alpha spin type, where the number of spins of each type is (alpha, beta); the model assumes that its input along the second-to-last dimension has size alpha + beta model_omega_init (jnp.float32): spring constant inside the model; when it matches the spring constant in the hamiltonian, then this model evaluates an eigenstate Returns: models.core.Module: spin-1/2 wavefunction for the quantum harmonic oscillator, with one trainable parameter (the model omega) \"\"\" def split_spin_fn ( x ): return jnp . split ( x , [ nspin_first ], axis =- 2 ) orbitals = HarmonicOscillatorOrbitals ( model_omega_init ) logdet_fn = models . antisymmetry . logdet_product return models . core . ComposedModel ([ split_spin_fn , orbitals , logdet_fn ])","title":"make_harmonic_oscillator_spin_half_model()"},{"location":"api/examples/harmonic_oscillator/#vmcnet.examples.harmonic_oscillator.harmonic_oscillator_potential","text":"Potential energy for independent harmonic oscillators with spring constant omega. This function computes sum_i 0.5 * (omega * x_i)^2. If x has more than one axis, these are simply summed over, so this corresponds to an isotropic harmonic oscillator potential. This function should be vmapped in order to be applied to batches of inputs, as it expects the first axis of x to correspond to the particle index. Parameters: Name Type Description Default omega jnp.float32 spring constant required x Array array of particle positions, where the first axis corresponds to particle index required Returns: Type Description jnp.float32 potential energy value for this configuration x Source code in vmcnet/examples/harmonic_oscillator.py def harmonic_oscillator_potential ( omega : jnp . float32 , x : Array ) -> jnp . float32 : \"\"\"Potential energy for independent harmonic oscillators with spring constant omega. This function computes sum_i 0.5 * (omega * x_i)^2. If x has more than one axis, these are simply summed over, so this corresponds to an isotropic harmonic oscillator potential. This function should be vmapped in order to be applied to batches of inputs, as it expects the first axis of x to correspond to the particle index. Args: omega (jnp.float32): spring constant x (Array): array of particle positions, where the first axis corresponds to particle index Returns: jnp.float32: potential energy value for this configuration x \"\"\" return 0.5 * jnp . sum ( jnp . square ( omega * x ))","title":"harmonic_oscillator_potential()"},{"location":"api/examples/harmonic_oscillator/#vmcnet.examples.harmonic_oscillator.make_harmonic_oscillator_local_energy","text":"Factory to create a local energy fn for the harmonic oscillator log|psi|. Parameters: Name Type Description Default omega jnp.float32 spring constant for the harmonic oscillator required log_psi_apply Callable function which evaluates log|psi| for a harmonic oscillator model wavefunction psi. Has the signature (params, x) -> log|psi(x)|. required Returns: Type Description Callable local energy function with the signature (params, x) -> local energy associated to the wavefunction psi Source code in vmcnet/examples/harmonic_oscillator.py def make_harmonic_oscillator_local_energy ( omega : jnp . float32 , log_psi_apply : ModelApply [ P ] ) -> ModelApply [ P ]: \"\"\"Factory to create a local energy fn for the harmonic oscillator log|psi|. Args: omega (jnp.float32): spring constant for the harmonic oscillator log_psi_apply (Callable): function which evaluates log|psi| for a harmonic oscillator model wavefunction psi. Has the signature (params, x) -> log|psi(x)|. Returns: Callable: local energy function with the signature (params, x) -> local energy associated to the wavefunction psi \"\"\" kinetic_fn = physics . kinetic . create_continuous_kinetic_energy ( log_psi_apply ) def potential_fn ( params : P , x : Array ): del params return harmonic_oscillator_potential ( omega , x ) potential_fn = jax . vmap ( potential_fn , in_axes = ( None , 0 ), out_axes = 0 ) return physics . core . combine_local_energy_terms ([ kinetic_fn , potential_fn ])","title":"make_harmonic_oscillator_local_energy()"},{"location":"api/examples/hydrogen_like_atom/","text":"Exactly solvable single-electron hydrogen-like atom. HydrogenLikeWavefunction ( Module ) dataclass Model which computes -decay_rate * r, with trainable decay_rate. This model returns log|psi(x)|, where psi is the 1-s orbital exp(-decay_rate * r). This psi is an eigenfunction of the hydrogen-like Hamiltonian (one ion and one electron) when the decay rate is equal to 2 * nuclear_charge / (d - 1), with d > 1, where d is the dimension of the system. Thus when the system is 3-d, then this model computes the ground-state wavefunction precisely when decay_rate = nuclear_charge. Attributes: Name Type Description decay_rate jnp.float32 initial decay rate in the model __call__ ( self , x ) special Log of isotropic exponential decay. Computes -decay_rate * ||x||. Parameters: Name Type Description Default x Array single electron positions, of shape (..., 1, d), where d is the dimension of the system required Returns: Type Description Array log of exponential decay wavefunction, with shape x.shape[:-2] Source code in vmcnet/examples/hydrogen_like_atom.py @flax . linen . compact def __call__ ( self , x : Array ) -> Array : # type: ignore[override] \"\"\"Log of isotropic exponential decay. Computes -decay_rate * ||x||. Args: x (Array): single electron positions, of shape (..., 1, d), where d is the dimension of the system Returns: Array: log of exponential decay wavefunction, with shape x.shape[:-2] \"\"\" r = jnp . linalg . norm ( x , axis =- 1 ) scaled_r = models . core . Dense ( 1 , kernel_init = lambda key , shape , ** kwargs : jnp . array ( [[ self . init_decay_rate ]] ), use_bias = False , )( r ) return - jnp . squeeze ( scaled_r , axis =- 1 ) make_hydrogen_like_local_energy ( log_psi_apply , charge , d = 3 ) Local energy calculation for the hydrogen-like atom in general dimension d. Parameters: Name Type Description Default log_psi_apply Callable a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| required charge jnp.float32 charge of the nucleus required d int Dimension of the system (number of coordinates that each ion and electron has). Defaults to 3. 3 Returns: Type Description Callable local energy function which computes -0.5 nabla^2 psi / psi + (Z / r), where psi is the wavefn, and Z is the charge of the nucleus. Has the signature (params, x) -> local energy array of shape (x.shape[0],) Source code in vmcnet/examples/hydrogen_like_atom.py def make_hydrogen_like_local_energy ( log_psi_apply : Callable [[ P , Array ], Union [ jnp . float32 , Array ]], charge : jnp . float32 , d : int = 3 , ) -> ModelApply [ P ]: \"\"\"Local energy calculation for the hydrogen-like atom in general dimension d. Args: log_psi_apply (Callable): a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| charge (jnp.float32): charge of the nucleus d (int, optional): Dimension of the system (number of coordinates that each ion and electron has). Defaults to 3. Returns: Callable: local energy function which computes -0.5 nabla^2 psi / psi + (Z / r), where psi is the wavefn, and Z is the charge of the nucleus. Has the signature (params, x) -> local energy array of shape (x.shape[0],) \"\"\" ion_location = jnp . zeros (( 1 , d )) ion_charge = jnp . array ([ charge ]) kinetic_fn = physics . kinetic . create_continuous_kinetic_energy ( log_psi_apply ) potential_fn = physics . potential . create_electron_ion_coulomb_potential ( ion_location , ion_charge ) return physics . core . combine_local_energy_terms ([ kinetic_fn , potential_fn ])","title":"hydrogen_like_atom"},{"location":"api/examples/hydrogen_like_atom/#vmcnet.examples.hydrogen_like_atom.HydrogenLikeWavefunction","text":"Model which computes -decay_rate * r, with trainable decay_rate. This model returns log|psi(x)|, where psi is the 1-s orbital exp(-decay_rate * r). This psi is an eigenfunction of the hydrogen-like Hamiltonian (one ion and one electron) when the decay rate is equal to 2 * nuclear_charge / (d - 1), with d > 1, where d is the dimension of the system. Thus when the system is 3-d, then this model computes the ground-state wavefunction precisely when decay_rate = nuclear_charge. Attributes: Name Type Description decay_rate jnp.float32 initial decay rate in the model","title":"HydrogenLikeWavefunction"},{"location":"api/examples/hydrogen_like_atom/#vmcnet.examples.hydrogen_like_atom.HydrogenLikeWavefunction.__call__","text":"Log of isotropic exponential decay. Computes -decay_rate * ||x||. Parameters: Name Type Description Default x Array single electron positions, of shape (..., 1, d), where d is the dimension of the system required Returns: Type Description Array log of exponential decay wavefunction, with shape x.shape[:-2] Source code in vmcnet/examples/hydrogen_like_atom.py @flax . linen . compact def __call__ ( self , x : Array ) -> Array : # type: ignore[override] \"\"\"Log of isotropic exponential decay. Computes -decay_rate * ||x||. Args: x (Array): single electron positions, of shape (..., 1, d), where d is the dimension of the system Returns: Array: log of exponential decay wavefunction, with shape x.shape[:-2] \"\"\" r = jnp . linalg . norm ( x , axis =- 1 ) scaled_r = models . core . Dense ( 1 , kernel_init = lambda key , shape , ** kwargs : jnp . array ( [[ self . init_decay_rate ]] ), use_bias = False , )( r ) return - jnp . squeeze ( scaled_r , axis =- 1 )","title":"__call__()"},{"location":"api/examples/hydrogen_like_atom/#vmcnet.examples.hydrogen_like_atom.make_hydrogen_like_local_energy","text":"Local energy calculation for the hydrogen-like atom in general dimension d. Parameters: Name Type Description Default log_psi_apply Callable a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| required charge jnp.float32 charge of the nucleus required d int Dimension of the system (number of coordinates that each ion and electron has). Defaults to 3. 3 Returns: Type Description Callable local energy function which computes -0.5 nabla^2 psi / psi + (Z / r), where psi is the wavefn, and Z is the charge of the nucleus. Has the signature (params, x) -> local energy array of shape (x.shape[0],) Source code in vmcnet/examples/hydrogen_like_atom.py def make_hydrogen_like_local_energy ( log_psi_apply : Callable [[ P , Array ], Union [ jnp . float32 , Array ]], charge : jnp . float32 , d : int = 3 , ) -> ModelApply [ P ]: \"\"\"Local energy calculation for the hydrogen-like atom in general dimension d. Args: log_psi_apply (Callable): a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| charge (jnp.float32): charge of the nucleus d (int, optional): Dimension of the system (number of coordinates that each ion and electron has). Defaults to 3. Returns: Callable: local energy function which computes -0.5 nabla^2 psi / psi + (Z / r), where psi is the wavefn, and Z is the charge of the nucleus. Has the signature (params, x) -> local energy array of shape (x.shape[0],) \"\"\" ion_location = jnp . zeros (( 1 , d )) ion_charge = jnp . array ([ charge ]) kinetic_fn = physics . kinetic . create_continuous_kinetic_energy ( log_psi_apply ) potential_fn = physics . potential . create_electron_ion_coulomb_potential ( ion_location , ion_charge ) return physics . core . combine_local_energy_terms ([ kinetic_fn , potential_fn ])","title":"make_hydrogen_like_local_energy()"},{"location":"api/mcmc/dynamic_width_position_amplitude/","text":"Metropolis routines for position amplitude data with dynamically sized steps. DWPAData ( dict ) TypedDict holding positions and wavefunction amplitudes, plus MoveMetadata. DynamicWidthPositionAmplitudeData ( dict ) TypedDict holding positions and wavefunction amplitudes, plus MoveMetadata. MoveMetadata ( dict ) Metadata for metropolis algorithm with dynamically sized gaussian steps. Attributes: Name Type Description std_move jnp.float32 the standard deviation of the gaussian step move_acceptance_sum jnp.float32 the sum of the move acceptance ratios of each step taken since the last std_move update. At update time, this sum will be divided by moves_since_update to get the overall average, and std_move will be adjusted in order to attempt to keep this value near some target. moves_since_update jnp.int32 Number of moves since the last std_move update. This is tracked so that the metropolis algorithm can make updates to std_move at fixed intervals rather than with every step. make_dynamic_width_position_amplitude_data ( position , amplitude , std_move , move_acceptance_sum = 0.0 , moves_since_update = 0 ) Create instance of DynamicWidthPositionAmplitudeData. Parameters: Name Type Description Default position Array the particle positions required amplitude Array the wavefunction amplitudes required std_move jnp.float32 std for gaussian moves required move_acceptance_sum jnp.float32 sum of the acceptance ratios of each step since the last update. Default of 0 should not be altered if using this function for initial data. 0.0 moves_since_update jnp.float32 the number of moves since the std_move was last updated. Default of 0 should not be altered if using this function for initial data. 0 Returns: Type Description DynamicWidthPositionAmplitudeData DWPAData Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_dynamic_width_position_amplitude_data ( position : Array , amplitude : Array , std_move : jnp . float32 , move_acceptance_sum : jnp . float32 = 0.0 , moves_since_update : jnp . int32 = 0 , ) -> DWPAData : \"\"\"Create instance of DynamicWidthPositionAmplitudeData. Args: position (Array): the particle positions amplitude (Array): the wavefunction amplitudes std_move (jnp.float32): std for gaussian moves move_acceptance_sum (jnp.float32): sum of the acceptance ratios of each step since the last update. Default of 0 should not be altered if using this function for initial data. moves_since_update (jnp.float32): the number of moves since the std_move was last updated. Default of 0 should not be altered if using this function for initial data. Returns: DWPAData \"\"\" return make_position_amplitude_data ( position , amplitude , MoveMetadata ( std_move = std_move , move_acceptance_sum = move_acceptance_sum , moves_since_update = moves_since_update , ), ) make_threshold_adjust_std_move ( target_acceptance_prob = 0.5 , threshold_delta = 0.1 , adjustment_delta = 0.1 ) Create a step size adjustment fn which aims to maintain a 50% acceptance rating. Works by increasing the step size when the acceptance is at least some delta above a target, and decreasing it when the acceptance is the some delta below the target. Parameters: Name Type Description Default target_acceptance_prob jnp.float32 target value for the average acceptance ratio. Defaults to 0.5. 0.5 threshold_delta jnp.float32 how far away from the target the acceptance ratio must be to trigger a compensating update. Defaults to 0.1. 0.1 adjustment_delta jnp.float32 how big of an adjustment to make to the step width. Adjustments will multiply by either (1.0 + adjustment_delta) or (1.0 - adjustment_delta). Defaults to 0.1. 0.1 Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_threshold_adjust_std_move ( target_acceptance_prob : jnp . float32 = 0.5 , threshold_delta : jnp . float32 = 0.1 , adjustment_delta : jnp . float32 = 0.1 , ) -> Callable [[ jnp . float32 , jnp . float32 ], jnp . float32 ]: \"\"\"Create a step size adjustment fn which aims to maintain a 50% acceptance rating. Works by increasing the step size when the acceptance is at least some delta above a target, and decreasing it when the acceptance is the some delta below the target. Args: target_acceptance_prob (jnp.float32): target value for the average acceptance ratio. Defaults to 0.5. threshold_delta (jnp.float32): how far away from the target the acceptance ratio must be to trigger a compensating update. Defaults to 0.1. adjustment_delta (jnp.float32): how big of an adjustment to make to the step width. Adjustments will multiply by either (1.0 + adjustment_delta) or (1.0 - adjustment_delta). Defaults to 0.1. \"\"\" def adjust_std_move ( old_std_move : jnp . float32 , avg_move_acceptance : jnp . float32 ) -> jnp . float32 : # Use jax.lax.cond since the predicates are data dependent. std_move = jax . lax . cond ( avg_move_acceptance > target_acceptance_prob + threshold_delta , lambda old_val : old_val * ( 1 + adjustment_delta ), lambda old_val : old_val , old_std_move , ) std_move = jax . lax . cond ( avg_move_acceptance < target_acceptance_prob - threshold_delta , lambda old_val : old_val * ( 1 - adjustment_delta ), lambda old_val : old_val , std_move , ) return std_move return adjust_std_move make_update_move_metadata_fn ( nmoves_per_update , adjust_std_move_fn ) Create a function that updates the move_metadata periodically. Periodicity is controlled by the nmoves_per_update parameter and the logic for updating the std of the gaussian step is handled by adjust_std_move_fn. Parameters: Name Type Description Default nmoves_per_update jnp.int32 std_move will be updated every time this many steps are taken. required adjust_std_move_fn Callable handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move required Returns: Type Description Callable function with signature (old_move_metadata, move_mask) -> new_move_metadata Result can be fed into the factory for a metropolis step to handle the updating of the MoveMetadata. Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_update_move_metadata_fn ( nmoves_per_update : jnp . int32 , adjust_std_move_fn : Callable [[ jnp . float32 , jnp . float32 ], jnp . float32 ], ) -> Callable [[ MoveMetadata , Array ], MoveMetadata ]: \"\"\"Create a function that updates the move_metadata periodically. Periodicity is controlled by the nmoves_per_update parameter and the logic for updating the std of the gaussian step is handled by adjust_std_move_fn. Args: nmoves_per_update (jnp.int32): std_move will be updated every time this many steps are taken. adjust_std_move_fn (Callable): handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move Returns: Callable: function with signature (old_move_metadata, move_mask) -> new_move_metadata Result can be fed into the factory for a metropolis step to handle the updating of the MoveMetadata. \"\"\" def update_move_metadata ( move_metadata : MoveMetadata , current_move_mask : Array ) -> MoveMetadata : std_move = move_metadata [ \"std_move\" ] move_acceptance_sum = move_metadata [ \"move_acceptance_sum\" ] moves_since_update = move_metadata [ \"moves_since_update\" ] current_avg_acceptance = mean_all_local_devices ( current_move_mask ) move_acceptance_sum = move_acceptance_sum + current_avg_acceptance moves_since_update = moves_since_update + 1 def update_std_move ( _ ): move_acceptance_avg = move_acceptance_sum / moves_since_update return ( adjust_std_move_fn ( std_move , move_acceptance_avg ), 0 , 0.0 ) def skip_update_std_move ( _ ): return ( std_move , moves_since_update , move_acceptance_sum ) ( std_move , moves_since_update , move_acceptance_sum ) = jax . lax . cond ( moves_since_update >= nmoves_per_update , update_std_move , skip_update_std_move , operand = None , ) return MoveMetadata ( std_move = std_move , move_acceptance_sum = move_acceptance_sum , moves_since_update = moves_since_update , ) return update_move_metadata make_dynamic_pos_amp_gaussian_step ( model_apply , nmoves_per_update = 10 , adjust_std_move_fn =< function make_threshold_adjust_std_move .< locals >. adjust_std_move at 0x7ff17294be50 > , logabs = True ) Create a metropolis step with dynamic gaussian step width. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required nmoves_per_update jnp.int32 number of metropolis steps to take between each update to std_move 10 adjust_std_move_fn Callable handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move <function make_threshold_adjust_std_move.<locals>.adjust_std_move at 0x7ff17294be50> logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable function which runs a metropolis step. Has the signature (params, DWPAData, key) -> (mean acceptance probability, DWPAData, new_key) Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_dynamic_pos_amp_gaussian_step ( model_apply : ModelApply [ P ], nmoves_per_update : jnp . int32 = 10 , adjust_std_move_fn : Callable [ [ jnp . float32 , jnp . float32 ], jnp . float32 ] = make_threshold_adjust_std_move (), logabs : bool = True , ) -> MetropolisStep : \"\"\"Create a metropolis step with dynamic gaussian step width. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude nmoves_per_update (jnp.int32): number of metropolis steps to take between each update to std_move adjust_std_move_fn (Callable): handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: function which runs a metropolis step. Has the signature (params, DWPAData, key) -> (mean acceptance probability, DWPAData, new_key) \"\"\" update_move_metadata_fn = make_update_move_metadata_fn ( nmoves_per_update , adjust_std_move_fn ) return make_position_amplitude_gaussian_metropolis_step ( model_apply , lambda data : data [ \"move_metadata\" ][ \"std_move\" ], update_move_metadata_fn , logabs , )","title":"dynamic_width_position_amplitude"},{"location":"api/mcmc/dynamic_width_position_amplitude/#vmcnet.mcmc.dynamic_width_position_amplitude.DWPAData","text":"TypedDict holding positions and wavefunction amplitudes, plus MoveMetadata.","title":"DWPAData"},{"location":"api/mcmc/dynamic_width_position_amplitude/#vmcnet.mcmc.dynamic_width_position_amplitude.DynamicWidthPositionAmplitudeData","text":"TypedDict holding positions and wavefunction amplitudes, plus MoveMetadata.","title":"DynamicWidthPositionAmplitudeData"},{"location":"api/mcmc/dynamic_width_position_amplitude/#vmcnet.mcmc.dynamic_width_position_amplitude.MoveMetadata","text":"Metadata for metropolis algorithm with dynamically sized gaussian steps. Attributes: Name Type Description std_move jnp.float32 the standard deviation of the gaussian step move_acceptance_sum jnp.float32 the sum of the move acceptance ratios of each step taken since the last std_move update. At update time, this sum will be divided by moves_since_update to get the overall average, and std_move will be adjusted in order to attempt to keep this value near some target. moves_since_update jnp.int32 Number of moves since the last std_move update. This is tracked so that the metropolis algorithm can make updates to std_move at fixed intervals rather than with every step.","title":"MoveMetadata"},{"location":"api/mcmc/dynamic_width_position_amplitude/#vmcnet.mcmc.dynamic_width_position_amplitude.make_dynamic_width_position_amplitude_data","text":"Create instance of DynamicWidthPositionAmplitudeData. Parameters: Name Type Description Default position Array the particle positions required amplitude Array the wavefunction amplitudes required std_move jnp.float32 std for gaussian moves required move_acceptance_sum jnp.float32 sum of the acceptance ratios of each step since the last update. Default of 0 should not be altered if using this function for initial data. 0.0 moves_since_update jnp.float32 the number of moves since the std_move was last updated. Default of 0 should not be altered if using this function for initial data. 0 Returns: Type Description DynamicWidthPositionAmplitudeData DWPAData Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_dynamic_width_position_amplitude_data ( position : Array , amplitude : Array , std_move : jnp . float32 , move_acceptance_sum : jnp . float32 = 0.0 , moves_since_update : jnp . int32 = 0 , ) -> DWPAData : \"\"\"Create instance of DynamicWidthPositionAmplitudeData. Args: position (Array): the particle positions amplitude (Array): the wavefunction amplitudes std_move (jnp.float32): std for gaussian moves move_acceptance_sum (jnp.float32): sum of the acceptance ratios of each step since the last update. Default of 0 should not be altered if using this function for initial data. moves_since_update (jnp.float32): the number of moves since the std_move was last updated. Default of 0 should not be altered if using this function for initial data. Returns: DWPAData \"\"\" return make_position_amplitude_data ( position , amplitude , MoveMetadata ( std_move = std_move , move_acceptance_sum = move_acceptance_sum , moves_since_update = moves_since_update , ), )","title":"make_dynamic_width_position_amplitude_data()"},{"location":"api/mcmc/dynamic_width_position_amplitude/#vmcnet.mcmc.dynamic_width_position_amplitude.make_threshold_adjust_std_move","text":"Create a step size adjustment fn which aims to maintain a 50% acceptance rating. Works by increasing the step size when the acceptance is at least some delta above a target, and decreasing it when the acceptance is the some delta below the target. Parameters: Name Type Description Default target_acceptance_prob jnp.float32 target value for the average acceptance ratio. Defaults to 0.5. 0.5 threshold_delta jnp.float32 how far away from the target the acceptance ratio must be to trigger a compensating update. Defaults to 0.1. 0.1 adjustment_delta jnp.float32 how big of an adjustment to make to the step width. Adjustments will multiply by either (1.0 + adjustment_delta) or (1.0 - adjustment_delta). Defaults to 0.1. 0.1 Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_threshold_adjust_std_move ( target_acceptance_prob : jnp . float32 = 0.5 , threshold_delta : jnp . float32 = 0.1 , adjustment_delta : jnp . float32 = 0.1 , ) -> Callable [[ jnp . float32 , jnp . float32 ], jnp . float32 ]: \"\"\"Create a step size adjustment fn which aims to maintain a 50% acceptance rating. Works by increasing the step size when the acceptance is at least some delta above a target, and decreasing it when the acceptance is the some delta below the target. Args: target_acceptance_prob (jnp.float32): target value for the average acceptance ratio. Defaults to 0.5. threshold_delta (jnp.float32): how far away from the target the acceptance ratio must be to trigger a compensating update. Defaults to 0.1. adjustment_delta (jnp.float32): how big of an adjustment to make to the step width. Adjustments will multiply by either (1.0 + adjustment_delta) or (1.0 - adjustment_delta). Defaults to 0.1. \"\"\" def adjust_std_move ( old_std_move : jnp . float32 , avg_move_acceptance : jnp . float32 ) -> jnp . float32 : # Use jax.lax.cond since the predicates are data dependent. std_move = jax . lax . cond ( avg_move_acceptance > target_acceptance_prob + threshold_delta , lambda old_val : old_val * ( 1 + adjustment_delta ), lambda old_val : old_val , old_std_move , ) std_move = jax . lax . cond ( avg_move_acceptance < target_acceptance_prob - threshold_delta , lambda old_val : old_val * ( 1 - adjustment_delta ), lambda old_val : old_val , std_move , ) return std_move return adjust_std_move","title":"make_threshold_adjust_std_move()"},{"location":"api/mcmc/dynamic_width_position_amplitude/#vmcnet.mcmc.dynamic_width_position_amplitude.make_update_move_metadata_fn","text":"Create a function that updates the move_metadata periodically. Periodicity is controlled by the nmoves_per_update parameter and the logic for updating the std of the gaussian step is handled by adjust_std_move_fn. Parameters: Name Type Description Default nmoves_per_update jnp.int32 std_move will be updated every time this many steps are taken. required adjust_std_move_fn Callable handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move required Returns: Type Description Callable function with signature (old_move_metadata, move_mask) -> new_move_metadata Result can be fed into the factory for a metropolis step to handle the updating of the MoveMetadata. Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_update_move_metadata_fn ( nmoves_per_update : jnp . int32 , adjust_std_move_fn : Callable [[ jnp . float32 , jnp . float32 ], jnp . float32 ], ) -> Callable [[ MoveMetadata , Array ], MoveMetadata ]: \"\"\"Create a function that updates the move_metadata periodically. Periodicity is controlled by the nmoves_per_update parameter and the logic for updating the std of the gaussian step is handled by adjust_std_move_fn. Args: nmoves_per_update (jnp.int32): std_move will be updated every time this many steps are taken. adjust_std_move_fn (Callable): handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move Returns: Callable: function with signature (old_move_metadata, move_mask) -> new_move_metadata Result can be fed into the factory for a metropolis step to handle the updating of the MoveMetadata. \"\"\" def update_move_metadata ( move_metadata : MoveMetadata , current_move_mask : Array ) -> MoveMetadata : std_move = move_metadata [ \"std_move\" ] move_acceptance_sum = move_metadata [ \"move_acceptance_sum\" ] moves_since_update = move_metadata [ \"moves_since_update\" ] current_avg_acceptance = mean_all_local_devices ( current_move_mask ) move_acceptance_sum = move_acceptance_sum + current_avg_acceptance moves_since_update = moves_since_update + 1 def update_std_move ( _ ): move_acceptance_avg = move_acceptance_sum / moves_since_update return ( adjust_std_move_fn ( std_move , move_acceptance_avg ), 0 , 0.0 ) def skip_update_std_move ( _ ): return ( std_move , moves_since_update , move_acceptance_sum ) ( std_move , moves_since_update , move_acceptance_sum ) = jax . lax . cond ( moves_since_update >= nmoves_per_update , update_std_move , skip_update_std_move , operand = None , ) return MoveMetadata ( std_move = std_move , move_acceptance_sum = move_acceptance_sum , moves_since_update = moves_since_update , ) return update_move_metadata","title":"make_update_move_metadata_fn()"},{"location":"api/mcmc/dynamic_width_position_amplitude/#vmcnet.mcmc.dynamic_width_position_amplitude.make_dynamic_pos_amp_gaussian_step","text":"Create a metropolis step with dynamic gaussian step width. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required nmoves_per_update jnp.int32 number of metropolis steps to take between each update to std_move 10 adjust_std_move_fn Callable handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move <function make_threshold_adjust_std_move.<locals>.adjust_std_move at 0x7ff17294be50> logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable function which runs a metropolis step. Has the signature (params, DWPAData, key) -> (mean acceptance probability, DWPAData, new_key) Source code in vmcnet/mcmc/dynamic_width_position_amplitude.py def make_dynamic_pos_amp_gaussian_step ( model_apply : ModelApply [ P ], nmoves_per_update : jnp . int32 = 10 , adjust_std_move_fn : Callable [ [ jnp . float32 , jnp . float32 ], jnp . float32 ] = make_threshold_adjust_std_move (), logabs : bool = True , ) -> MetropolisStep : \"\"\"Create a metropolis step with dynamic gaussian step width. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude nmoves_per_update (jnp.int32): number of metropolis steps to take between each update to std_move adjust_std_move_fn (Callable): handles the logic for updating std_move. Has signature (old_std_move, avg_move_acceptance) -> new_std_move logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: function which runs a metropolis step. Has the signature (params, DWPAData, key) -> (mean acceptance probability, DWPAData, new_key) \"\"\" update_move_metadata_fn = make_update_move_metadata_fn ( nmoves_per_update , adjust_std_move_fn ) return make_position_amplitude_gaussian_metropolis_step ( model_apply , lambda data : data [ \"move_metadata\" ][ \"std_move\" ], update_move_metadata_fn , logabs , )","title":"make_dynamic_pos_amp_gaussian_step()"},{"location":"api/mcmc/metropolis/","text":"Proposal and acceptance fns for Metropolis-Hastings Markov-Chain Monte Carlo. make_metropolis_step ( proposal_fn , acceptance_fn , update_data_fn ) Factory to create a function which takes a single metropolis step. Following Metropolis-Hastings Markov Chain Monte Carlo, a transition from one data state to another is split into proposal and acceptance. When used in a Metropolis routine to approximate a stationary distribution P, the proposal and acceptance functions should satisfy detailed balance, i.e., proposal_prob_ij * acceptance_ij * P_i = proposal_prob_ji * acceptance_ji * P_j, where proposal_prob_ij is the likelihood of proposing the transition from state i to state j, acceptance_ij is the likelihood of accepting a transition from state i to state j, and P_i is the probability of being in state i. Parameters: Name Type Description Default proposal_fn Callable proposal function which produces new proposed data. Has the signature (params, data, key) -> proposed_data, key required acceptance_fn Callable acceptance function which produces a vector of numbers used to create a mask for accepting the proposals. Has the signature (params, data, proposed_data) -> Array: acceptance probabilities required update_data_fn Callable function used to update the data given the original data, the proposed data, and the array mask identifying which proposals to accept. Has the signature (data, proposed_data, mask) -> new_data required Returns: Type Description Callable function which takes in (data, params, key) and outputs (mean acceptance probability, new data, new jax PRNG key split from previous one) Source code in vmcnet/mcmc/metropolis.py def make_metropolis_step ( proposal_fn : Callable [[ P , D , PRNGKey ], Tuple [ D , PRNGKey ]], acceptance_fn : Callable [[ P , D , D ], Array ], update_data_fn : Callable [[ D , D , Array ], D ], ) -> MetropolisStep [ P , D ]: \"\"\"Factory to create a function which takes a single metropolis step. Following Metropolis-Hastings Markov Chain Monte Carlo, a transition from one data state to another is split into proposal and acceptance. When used in a Metropolis routine to approximate a stationary distribution P, the proposal and acceptance functions should satisfy detailed balance, i.e., proposal_prob_ij * acceptance_ij * P_i = proposal_prob_ji * acceptance_ji * P_j, where proposal_prob_ij is the likelihood of proposing the transition from state i to state j, acceptance_ij is the likelihood of accepting a transition from state i to state j, and P_i is the probability of being in state i. Args: proposal_fn (Callable): proposal function which produces new proposed data. Has the signature (params, data, key) -> proposed_data, key acceptance_fn (Callable): acceptance function which produces a vector of numbers used to create a mask for accepting the proposals. Has the signature (params, data, proposed_data) -> Array: acceptance probabilities update_data_fn (Callable): function used to update the data given the original data, the proposed data, and the array mask identifying which proposals to accept. Has the signature (data, proposed_data, mask) -> new_data Returns: Callable: function which takes in (data, params, key) and outputs (mean acceptance probability, new data, new jax PRNG key split from previous one) \"\"\" def metrop_step_fn ( params : P , data : D , key : PRNGKey ) -> Tuple [ jnp . float32 , D , PRNGKey ]: \"\"\"Take a single metropolis step.\"\"\" key , subkey = jax . random . split ( key ) proposed_data , key = proposal_fn ( params , data , key ) accept_prob = acceptance_fn ( params , data , proposed_data ) move_mask = cast ( Array , jax . random . uniform ( subkey , shape = accept_prob . shape ) < accept_prob , ) new_data = update_data_fn ( data , proposed_data , move_mask ) return jnp . mean ( accept_prob ), new_data , key return metrop_step_fn walk_data ( nsteps , params , data , key , metrop_step_fn ) Take multiple Metropolis-Hastings steps. This function is roughly equivalent to: accept_sum = 0.0 for _ in range(nsteps): accept_prob, data, key = metropolis_step_fn(data, params, key) accept_sum += accept_prob return accept_sum / nsteps, data, key but has better tracing/pmap behavior due to the use of jax.lax.scan instead of a python for loop. See :func: ~vmcnet.train.vmc.take_metropolis_step . Parameters: Name Type Description Default nsteps int number of steps to take required data pytree-like data to walk (update) with each step required params pytree-like parameters passed to proposal_fn and acceptance_fn, e.g. model params required key PRNGKey an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn required metrop_step_fn Callable function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) required Returns: Type Description (jnp.float32, pytree-like, PRNGKey) acceptance probability, new data, new jax PRNG key split (possibly multiple times) from previous one Source code in vmcnet/mcmc/metropolis.py def walk_data ( nsteps : int , params : P , data : D , key : PRNGKey , metrop_step_fn : MetropolisStep [ P , D ], ) -> Tuple [ jnp . float32 , D , PRNGKey ]: \"\"\"Take multiple Metropolis-Hastings steps. This function is roughly equivalent to: ``` accept_sum = 0.0 for _ in range(nsteps): accept_prob, data, key = metropolis_step_fn(data, params, key) accept_sum += accept_prob return accept_sum / nsteps, data, key ``` but has better tracing/pmap behavior due to the use of jax.lax.scan instead of a python for loop. See :func:`~vmcnet.train.vmc.take_metropolis_step`. Args: nsteps (int): number of steps to take data (pytree-like): data to walk (update) with each step params (pytree-like): parameters passed to proposal_fn and acceptance_fn, e.g. model params key (PRNGKey): an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn metrop_step_fn (Callable): function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) Returns: (jnp.float32, pytree-like, PRNGKey): acceptance probability, new data, new jax PRNG key split (possibly multiple times) from previous one \"\"\" def step_fn ( carry , x ): del x accept_prob , data , key = metrop_step_fn ( params , carry [ 1 ], carry [ 2 ]) return ( carry [ 0 ] + accept_prob , data , key ), None out = jax . lax . scan ( step_fn , ( 0.0 , data , key ), xs = None , length = nsteps ) accept_sum , data , key = out [ 0 ] return accept_sum / nsteps , data , key make_jitted_burning_step ( metrop_step_fn , apply_pmap = True ) Factory to create a burning step, which is an optionally pmapped Metropolis step. This provides the functionality to optionally apply jax.pmap to a single Metropolis step. Only one step is traced so that the first burning step is traced but subsequent steps are properly jit-compiled. The acceptance probabilities (which typically don't mean much during burning) are thrown away. For more about the Metropolis step itself, see :func: ~vmcnet.mcmc.metropolis.make_metropolis_step . Parameters: Name Type Description Default metrop_step_fn Callable function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) required apply_pmap bool whether to apply jax.pmap to the burning step. If False, applies jax.jit. Defaults to True. True Returns: Type Description Callable function with signature (data, params, key) -> (data, key), with jax.pmap optionally applied if apply_pmap is True. Source code in vmcnet/mcmc/metropolis.py def make_jitted_burning_step ( metrop_step_fn : MetropolisStep [ P , D ], apply_pmap : bool = True , ) -> BurningStep [ P , D ]: \"\"\"Factory to create a burning step, which is an optionally pmapped Metropolis step. This provides the functionality to optionally apply jax.pmap to a single Metropolis step. Only one step is traced so that the first burning step is traced but subsequent steps are properly jit-compiled. The acceptance probabilities (which typically don't mean much during burning) are thrown away. For more about the Metropolis step itself, see :func:`~vmcnet.mcmc.metropolis.make_metropolis_step`. Args: metrop_step_fn (Callable): function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) apply_pmap (bool, optional): whether to apply jax.pmap to the burning step. If False, applies jax.jit. Defaults to True. Returns: Callable: function with signature (data, params, key) -> (data, key), with jax.pmap optionally applied if apply_pmap is True. \"\"\" def burning_step ( params : P , data : D , key : PRNGKey ) -> Tuple [ D , PRNGKey ]: _ , data , key = metrop_step_fn ( params , data , key ) return data , key if not apply_pmap : return jax . jit ( burning_step ) return utils . distribute . pmap ( burning_step ) make_jitted_walker_fn ( nsteps , metrop_step_fn , apply_pmap = True ) Factory to create a function which takes multiple Metropolis steps. This provides the functionality to optionally apply jax.pmap to a jax.lax.scan loop of multiple metropolis steps. A typical use case would be to run this function between parameter updates in a VMC loop. An accumulated mean acceptance probability statistic is returned from this walker function. See :func: ~vmcnet.train.vmc.vmc_loop for usage. Parameters: Name Type Description Default nsteps int number of metropolis steps to take in each call required metrop_step_fn Callable function which does a metropolis step. Has the signature (data, params, key) -> (mean accept probl, new data, new key) required apply_pmap bool whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. True Returns: Type Description Callable funciton with signature (params, data, key) -> (mean accept prob, new data, new key) with jax.pmap optionally applied if pmapped is True, and jax.jit applied if apply_pmap is False. Source code in vmcnet/mcmc/metropolis.py def make_jitted_walker_fn ( nsteps : int , metrop_step_fn : MetropolisStep [ P , D ], apply_pmap : bool = True , ) -> WalkerFn [ P , D ]: \"\"\"Factory to create a function which takes multiple Metropolis steps. This provides the functionality to optionally apply jax.pmap to a jax.lax.scan loop of multiple metropolis steps. A typical use case would be to run this function between parameter updates in a VMC loop. An accumulated mean acceptance probability statistic is returned from this walker function. See :func:`~vmcnet.train.vmc.vmc_loop` for usage. Args: nsteps (int): number of metropolis steps to take in each call metrop_step_fn (Callable): function which does a metropolis step. Has the signature (data, params, key) -> (mean accept probl, new data, new key) apply_pmap (bool, optional): whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. Returns: Callable: funciton with signature (params, data, key) -> (mean accept prob, new data, new key) with jax.pmap optionally applied if pmapped is True, and jax.jit applied if apply_pmap is False. \"\"\" def walker_fn ( params : P , data : D , key : PRNGKey ) -> Tuple [ jnp . float32 , D , PRNGKey ]: accept_ratio , data , key = walk_data ( nsteps , params , data , key , metrop_step_fn ) accept_ratio = utils . distribute . pmean_if_pmap ( accept_ratio ) return accept_ratio , data , key if not apply_pmap : return jax . jit ( walker_fn ) pmapped_walker_fn = utils . distribute . pmap ( walker_fn ) def pmapped_walker_fn_with_single_accept_ratio ( params : P , data : D , key : PRNGKey ) -> Tuple [ jnp . float32 , D , PRNGKey ]: accept_ratio , data , key = pmapped_walker_fn ( params , data , key ) accept_ratio = utils . distribute . get_first ( accept_ratio ) return accept_ratio , data , key return pmapped_walker_fn_with_single_accept_ratio burn_data ( burning_step , nsteps_to_burn , params , data , key ) Repeatedly apply a burning step. Parameters: Name Type Description Default burning_step BurningStep function which does a burning step. Has the signature (data, params, key) -> (new data, new key) required nsteps_to_burn int number of times to call burning_step required data pytree-like initial data required params pytree-like parameters passed to the burning step required key PRNGKey an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn required Returns: Type Description (pytree-like, PRNGKey) new data, new key Source code in vmcnet/mcmc/metropolis.py def burn_data ( burning_step : BurningStep [ P , D ], nsteps_to_burn : int , params : P , data : D , key : PRNGKey , ) -> Tuple [ D , PRNGKey ]: \"\"\"Repeatedly apply a burning step. Args: burning_step (BurningStep): function which does a burning step. Has the signature (data, params, key) -> (new data, new key) nsteps_to_burn (int): number of times to call burning_step data (pytree-like): initial data params (pytree-like): parameters passed to the burning step key (PRNGKey): an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn Returns: (pytree-like, PRNGKey): new data, new key \"\"\" logging . info ( \"Burning data for %d steps\" , nsteps_to_burn ) for _ in range ( nsteps_to_burn ): data , key = burning_step ( params , data , key ) return data , key gaussian_proposal ( positions , std_move , key ) Simple symmetric gaussian proposal in all positions at once. Parameters: Name Type Description Default positions Array original positions required std_move jnp.float32 standard deviation of the moves required key PRNGKey an array with shape (2,) representing a jax PRNG key required Returns: Type Description (Array, Array) (new positions, new key split from previous) Source code in vmcnet/mcmc/metropolis.py def gaussian_proposal ( positions : Array , std_move : jnp . float32 , key : PRNGKey ) -> Tuple [ Array , PRNGKey ]: \"\"\"Simple symmetric gaussian proposal in all positions at once. Args: positions (Array): original positions std_move (jnp.float32): standard deviation of the moves key (PRNGKey): an array with shape (2,) representing a jax PRNG key Returns: (Array, Array): (new positions, new key split from previous) \"\"\" key , subkey = jax . random . split ( key ) return positions + std_move * jax . random . normal ( subkey , shape = positions . shape ), key metropolis_symmetric_acceptance ( amplitude , proposed_amplitude , logabs = True ) Standard Metropolis acceptance ratio for a symmetric proposal function. The general Metropolis-Hastings choice of acceptance ratio for moves from state i to state j is given by accept_ij = min(1, (P_j * proposal_prob_ji) / (P_i * proposal_prob_ij)). When proposal_prob is symmetric (assumed in this function), this simply reduces to accept_ij = min(1, P_j / P_i). Some care is taken to avoid numerical overflow and division by zero. The inputs are wavefunction amplitudes psi or log(|psi|), so the probability P_i refers to |psi(i)|^2. Parameters: Name Type Description Default amplitude Array one-dimensional array of wavefunction amplitudes for the current state, or log wavefunction amplitudes if logabs is True required proposed_amplitude Array one-dimensional array of wavefunction amplitudes for the proposed state, or log wavefunction amplitudes if logabs is True required logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Array one-dimensional array of acceptance ratios for the Metropolis algorithm Source code in vmcnet/mcmc/metropolis.py def metropolis_symmetric_acceptance ( amplitude : Array , proposed_amplitude : Array , logabs : bool = True ) -> Array : \"\"\"Standard Metropolis acceptance ratio for a symmetric proposal function. The general Metropolis-Hastings choice of acceptance ratio for moves from state i to state j is given by accept_ij = min(1, (P_j * proposal_prob_ji) / (P_i * proposal_prob_ij)). When proposal_prob is symmetric (assumed in this function), this simply reduces to accept_ij = min(1, P_j / P_i). Some care is taken to avoid numerical overflow and division by zero. The inputs are wavefunction amplitudes psi or log(|psi|), so the probability P_i refers to |psi(i)|^2. Args: amplitude (Array): one-dimensional array of wavefunction amplitudes for the current state, or log wavefunction amplitudes if logabs is True proposed_amplitude (Array): one-dimensional array of wavefunction amplitudes for the proposed state, or log wavefunction amplitudes if logabs is True logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Array: one-dimensional array of acceptance ratios for the Metropolis algorithm \"\"\" if not logabs : prob_old = jnp . square ( amplitude ) prob_new = jnp . square ( proposed_amplitude ) ratio = prob_new / prob_old # safe division by zero ratio = jnp . where ( jnp . logical_or ( prob_old < prob_new , prob_old == 0.0 ), jnp . ones_like ( ratio ), ratio , ) return ratio log_prob_old = 2.0 * amplitude log_prob_new = 2.0 * proposed_amplitude # avoid overflow if log_prob_new - log_prob_old is large return jnp . where ( log_prob_new > log_prob_old , jnp . ones_like ( log_prob_new ), jnp . exp ( log_prob_new - log_prob_old ), )","title":"metropolis"},{"location":"api/mcmc/metropolis/#vmcnet.mcmc.metropolis.make_metropolis_step","text":"Factory to create a function which takes a single metropolis step. Following Metropolis-Hastings Markov Chain Monte Carlo, a transition from one data state to another is split into proposal and acceptance. When used in a Metropolis routine to approximate a stationary distribution P, the proposal and acceptance functions should satisfy detailed balance, i.e., proposal_prob_ij * acceptance_ij * P_i = proposal_prob_ji * acceptance_ji * P_j, where proposal_prob_ij is the likelihood of proposing the transition from state i to state j, acceptance_ij is the likelihood of accepting a transition from state i to state j, and P_i is the probability of being in state i. Parameters: Name Type Description Default proposal_fn Callable proposal function which produces new proposed data. Has the signature (params, data, key) -> proposed_data, key required acceptance_fn Callable acceptance function which produces a vector of numbers used to create a mask for accepting the proposals. Has the signature (params, data, proposed_data) -> Array: acceptance probabilities required update_data_fn Callable function used to update the data given the original data, the proposed data, and the array mask identifying which proposals to accept. Has the signature (data, proposed_data, mask) -> new_data required Returns: Type Description Callable function which takes in (data, params, key) and outputs (mean acceptance probability, new data, new jax PRNG key split from previous one) Source code in vmcnet/mcmc/metropolis.py def make_metropolis_step ( proposal_fn : Callable [[ P , D , PRNGKey ], Tuple [ D , PRNGKey ]], acceptance_fn : Callable [[ P , D , D ], Array ], update_data_fn : Callable [[ D , D , Array ], D ], ) -> MetropolisStep [ P , D ]: \"\"\"Factory to create a function which takes a single metropolis step. Following Metropolis-Hastings Markov Chain Monte Carlo, a transition from one data state to another is split into proposal and acceptance. When used in a Metropolis routine to approximate a stationary distribution P, the proposal and acceptance functions should satisfy detailed balance, i.e., proposal_prob_ij * acceptance_ij * P_i = proposal_prob_ji * acceptance_ji * P_j, where proposal_prob_ij is the likelihood of proposing the transition from state i to state j, acceptance_ij is the likelihood of accepting a transition from state i to state j, and P_i is the probability of being in state i. Args: proposal_fn (Callable): proposal function which produces new proposed data. Has the signature (params, data, key) -> proposed_data, key acceptance_fn (Callable): acceptance function which produces a vector of numbers used to create a mask for accepting the proposals. Has the signature (params, data, proposed_data) -> Array: acceptance probabilities update_data_fn (Callable): function used to update the data given the original data, the proposed data, and the array mask identifying which proposals to accept. Has the signature (data, proposed_data, mask) -> new_data Returns: Callable: function which takes in (data, params, key) and outputs (mean acceptance probability, new data, new jax PRNG key split from previous one) \"\"\" def metrop_step_fn ( params : P , data : D , key : PRNGKey ) -> Tuple [ jnp . float32 , D , PRNGKey ]: \"\"\"Take a single metropolis step.\"\"\" key , subkey = jax . random . split ( key ) proposed_data , key = proposal_fn ( params , data , key ) accept_prob = acceptance_fn ( params , data , proposed_data ) move_mask = cast ( Array , jax . random . uniform ( subkey , shape = accept_prob . shape ) < accept_prob , ) new_data = update_data_fn ( data , proposed_data , move_mask ) return jnp . mean ( accept_prob ), new_data , key return metrop_step_fn","title":"make_metropolis_step()"},{"location":"api/mcmc/metropolis/#vmcnet.mcmc.metropolis.walk_data","text":"Take multiple Metropolis-Hastings steps. This function is roughly equivalent to: accept_sum = 0.0 for _ in range(nsteps): accept_prob, data, key = metropolis_step_fn(data, params, key) accept_sum += accept_prob return accept_sum / nsteps, data, key but has better tracing/pmap behavior due to the use of jax.lax.scan instead of a python for loop. See :func: ~vmcnet.train.vmc.take_metropolis_step . Parameters: Name Type Description Default nsteps int number of steps to take required data pytree-like data to walk (update) with each step required params pytree-like parameters passed to proposal_fn and acceptance_fn, e.g. model params required key PRNGKey an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn required metrop_step_fn Callable function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) required Returns: Type Description (jnp.float32, pytree-like, PRNGKey) acceptance probability, new data, new jax PRNG key split (possibly multiple times) from previous one Source code in vmcnet/mcmc/metropolis.py def walk_data ( nsteps : int , params : P , data : D , key : PRNGKey , metrop_step_fn : MetropolisStep [ P , D ], ) -> Tuple [ jnp . float32 , D , PRNGKey ]: \"\"\"Take multiple Metropolis-Hastings steps. This function is roughly equivalent to: ``` accept_sum = 0.0 for _ in range(nsteps): accept_prob, data, key = metropolis_step_fn(data, params, key) accept_sum += accept_prob return accept_sum / nsteps, data, key ``` but has better tracing/pmap behavior due to the use of jax.lax.scan instead of a python for loop. See :func:`~vmcnet.train.vmc.take_metropolis_step`. Args: nsteps (int): number of steps to take data (pytree-like): data to walk (update) with each step params (pytree-like): parameters passed to proposal_fn and acceptance_fn, e.g. model params key (PRNGKey): an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn metrop_step_fn (Callable): function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) Returns: (jnp.float32, pytree-like, PRNGKey): acceptance probability, new data, new jax PRNG key split (possibly multiple times) from previous one \"\"\" def step_fn ( carry , x ): del x accept_prob , data , key = metrop_step_fn ( params , carry [ 1 ], carry [ 2 ]) return ( carry [ 0 ] + accept_prob , data , key ), None out = jax . lax . scan ( step_fn , ( 0.0 , data , key ), xs = None , length = nsteps ) accept_sum , data , key = out [ 0 ] return accept_sum / nsteps , data , key","title":"walk_data()"},{"location":"api/mcmc/metropolis/#vmcnet.mcmc.metropolis.make_jitted_burning_step","text":"Factory to create a burning step, which is an optionally pmapped Metropolis step. This provides the functionality to optionally apply jax.pmap to a single Metropolis step. Only one step is traced so that the first burning step is traced but subsequent steps are properly jit-compiled. The acceptance probabilities (which typically don't mean much during burning) are thrown away. For more about the Metropolis step itself, see :func: ~vmcnet.mcmc.metropolis.make_metropolis_step . Parameters: Name Type Description Default metrop_step_fn Callable function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) required apply_pmap bool whether to apply jax.pmap to the burning step. If False, applies jax.jit. Defaults to True. True Returns: Type Description Callable function with signature (data, params, key) -> (data, key), with jax.pmap optionally applied if apply_pmap is True. Source code in vmcnet/mcmc/metropolis.py def make_jitted_burning_step ( metrop_step_fn : MetropolisStep [ P , D ], apply_pmap : bool = True , ) -> BurningStep [ P , D ]: \"\"\"Factory to create a burning step, which is an optionally pmapped Metropolis step. This provides the functionality to optionally apply jax.pmap to a single Metropolis step. Only one step is traced so that the first burning step is traced but subsequent steps are properly jit-compiled. The acceptance probabilities (which typically don't mean much during burning) are thrown away. For more about the Metropolis step itself, see :func:`~vmcnet.mcmc.metropolis.make_metropolis_step`. Args: metrop_step_fn (Callable): function which does a metropolis step. Has the signature (data, params, key) -> (mean accept prob, new data, new key) apply_pmap (bool, optional): whether to apply jax.pmap to the burning step. If False, applies jax.jit. Defaults to True. Returns: Callable: function with signature (data, params, key) -> (data, key), with jax.pmap optionally applied if apply_pmap is True. \"\"\" def burning_step ( params : P , data : D , key : PRNGKey ) -> Tuple [ D , PRNGKey ]: _ , data , key = metrop_step_fn ( params , data , key ) return data , key if not apply_pmap : return jax . jit ( burning_step ) return utils . distribute . pmap ( burning_step )","title":"make_jitted_burning_step()"},{"location":"api/mcmc/metropolis/#vmcnet.mcmc.metropolis.make_jitted_walker_fn","text":"Factory to create a function which takes multiple Metropolis steps. This provides the functionality to optionally apply jax.pmap to a jax.lax.scan loop of multiple metropolis steps. A typical use case would be to run this function between parameter updates in a VMC loop. An accumulated mean acceptance probability statistic is returned from this walker function. See :func: ~vmcnet.train.vmc.vmc_loop for usage. Parameters: Name Type Description Default nsteps int number of metropolis steps to take in each call required metrop_step_fn Callable function which does a metropolis step. Has the signature (data, params, key) -> (mean accept probl, new data, new key) required apply_pmap bool whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. True Returns: Type Description Callable funciton with signature (params, data, key) -> (mean accept prob, new data, new key) with jax.pmap optionally applied if pmapped is True, and jax.jit applied if apply_pmap is False. Source code in vmcnet/mcmc/metropolis.py def make_jitted_walker_fn ( nsteps : int , metrop_step_fn : MetropolisStep [ P , D ], apply_pmap : bool = True , ) -> WalkerFn [ P , D ]: \"\"\"Factory to create a function which takes multiple Metropolis steps. This provides the functionality to optionally apply jax.pmap to a jax.lax.scan loop of multiple metropolis steps. A typical use case would be to run this function between parameter updates in a VMC loop. An accumulated mean acceptance probability statistic is returned from this walker function. See :func:`~vmcnet.train.vmc.vmc_loop` for usage. Args: nsteps (int): number of metropolis steps to take in each call metrop_step_fn (Callable): function which does a metropolis step. Has the signature (data, params, key) -> (mean accept probl, new data, new key) apply_pmap (bool, optional): whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. Returns: Callable: funciton with signature (params, data, key) -> (mean accept prob, new data, new key) with jax.pmap optionally applied if pmapped is True, and jax.jit applied if apply_pmap is False. \"\"\" def walker_fn ( params : P , data : D , key : PRNGKey ) -> Tuple [ jnp . float32 , D , PRNGKey ]: accept_ratio , data , key = walk_data ( nsteps , params , data , key , metrop_step_fn ) accept_ratio = utils . distribute . pmean_if_pmap ( accept_ratio ) return accept_ratio , data , key if not apply_pmap : return jax . jit ( walker_fn ) pmapped_walker_fn = utils . distribute . pmap ( walker_fn ) def pmapped_walker_fn_with_single_accept_ratio ( params : P , data : D , key : PRNGKey ) -> Tuple [ jnp . float32 , D , PRNGKey ]: accept_ratio , data , key = pmapped_walker_fn ( params , data , key ) accept_ratio = utils . distribute . get_first ( accept_ratio ) return accept_ratio , data , key return pmapped_walker_fn_with_single_accept_ratio","title":"make_jitted_walker_fn()"},{"location":"api/mcmc/metropolis/#vmcnet.mcmc.metropolis.burn_data","text":"Repeatedly apply a burning step. Parameters: Name Type Description Default burning_step BurningStep function which does a burning step. Has the signature (data, params, key) -> (new data, new key) required nsteps_to_burn int number of times to call burning_step required data pytree-like initial data required params pytree-like parameters passed to the burning step required key PRNGKey an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn required Returns: Type Description (pytree-like, PRNGKey) new data, new key Source code in vmcnet/mcmc/metropolis.py def burn_data ( burning_step : BurningStep [ P , D ], nsteps_to_burn : int , params : P , data : D , key : PRNGKey , ) -> Tuple [ D , PRNGKey ]: \"\"\"Repeatedly apply a burning step. Args: burning_step (BurningStep): function which does a burning step. Has the signature (data, params, key) -> (new data, new key) nsteps_to_burn (int): number of times to call burning_step data (pytree-like): initial data params (pytree-like): parameters passed to the burning step key (PRNGKey): an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn Returns: (pytree-like, PRNGKey): new data, new key \"\"\" logging . info ( \"Burning data for %d steps\" , nsteps_to_burn ) for _ in range ( nsteps_to_burn ): data , key = burning_step ( params , data , key ) return data , key","title":"burn_data()"},{"location":"api/mcmc/metropolis/#vmcnet.mcmc.metropolis.gaussian_proposal","text":"Simple symmetric gaussian proposal in all positions at once. Parameters: Name Type Description Default positions Array original positions required std_move jnp.float32 standard deviation of the moves required key PRNGKey an array with shape (2,) representing a jax PRNG key required Returns: Type Description (Array, Array) (new positions, new key split from previous) Source code in vmcnet/mcmc/metropolis.py def gaussian_proposal ( positions : Array , std_move : jnp . float32 , key : PRNGKey ) -> Tuple [ Array , PRNGKey ]: \"\"\"Simple symmetric gaussian proposal in all positions at once. Args: positions (Array): original positions std_move (jnp.float32): standard deviation of the moves key (PRNGKey): an array with shape (2,) representing a jax PRNG key Returns: (Array, Array): (new positions, new key split from previous) \"\"\" key , subkey = jax . random . split ( key ) return positions + std_move * jax . random . normal ( subkey , shape = positions . shape ), key","title":"gaussian_proposal()"},{"location":"api/mcmc/metropolis/#vmcnet.mcmc.metropolis.metropolis_symmetric_acceptance","text":"Standard Metropolis acceptance ratio for a symmetric proposal function. The general Metropolis-Hastings choice of acceptance ratio for moves from state i to state j is given by accept_ij = min(1, (P_j * proposal_prob_ji) / (P_i * proposal_prob_ij)). When proposal_prob is symmetric (assumed in this function), this simply reduces to accept_ij = min(1, P_j / P_i). Some care is taken to avoid numerical overflow and division by zero. The inputs are wavefunction amplitudes psi or log(|psi|), so the probability P_i refers to |psi(i)|^2. Parameters: Name Type Description Default amplitude Array one-dimensional array of wavefunction amplitudes for the current state, or log wavefunction amplitudes if logabs is True required proposed_amplitude Array one-dimensional array of wavefunction amplitudes for the proposed state, or log wavefunction amplitudes if logabs is True required logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Array one-dimensional array of acceptance ratios for the Metropolis algorithm Source code in vmcnet/mcmc/metropolis.py def metropolis_symmetric_acceptance ( amplitude : Array , proposed_amplitude : Array , logabs : bool = True ) -> Array : \"\"\"Standard Metropolis acceptance ratio for a symmetric proposal function. The general Metropolis-Hastings choice of acceptance ratio for moves from state i to state j is given by accept_ij = min(1, (P_j * proposal_prob_ji) / (P_i * proposal_prob_ij)). When proposal_prob is symmetric (assumed in this function), this simply reduces to accept_ij = min(1, P_j / P_i). Some care is taken to avoid numerical overflow and division by zero. The inputs are wavefunction amplitudes psi or log(|psi|), so the probability P_i refers to |psi(i)|^2. Args: amplitude (Array): one-dimensional array of wavefunction amplitudes for the current state, or log wavefunction amplitudes if logabs is True proposed_amplitude (Array): one-dimensional array of wavefunction amplitudes for the proposed state, or log wavefunction amplitudes if logabs is True logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Array: one-dimensional array of acceptance ratios for the Metropolis algorithm \"\"\" if not logabs : prob_old = jnp . square ( amplitude ) prob_new = jnp . square ( proposed_amplitude ) ratio = prob_new / prob_old # safe division by zero ratio = jnp . where ( jnp . logical_or ( prob_old < prob_new , prob_old == 0.0 ), jnp . ones_like ( ratio ), ratio , ) return ratio log_prob_old = 2.0 * amplitude log_prob_new = 2.0 * proposed_amplitude # avoid overflow if log_prob_new - log_prob_old is large return jnp . where ( log_prob_new > log_prob_old , jnp . ones_like ( log_prob_new ), jnp . exp ( log_prob_new - log_prob_old ), )","title":"metropolis_symmetric_acceptance()"},{"location":"api/mcmc/position_amplitude_core/","text":"Shared routines for position amplitude metropolis data. PositionAmplitudeData ( dict ) TypedDict of data holding positions, amplitudes, and optional metadata. Holding both particle position and wavefn amplitude in the data can be advantageous to avoid recalculating amplitudes in some routines, e.g. acceptance probabilities. Furthermore, holding additional metadata can enable more sophisticated metropolis algorithms such as dynamically adjusted gaussian step sizes. Attributes: Name Type Description walker_data PositionAmplitudeWalkerData the positions and amplitudes move_metadata any any metadata needed for the metropolis algorithm PositionAmplitudeWalkerData ( dict ) TypedDict of walker data holding just positions and amplitudes. Holding both particle position and wavefn amplitude in the same named tuple allows us to simultaneously mask over both in the acceptance function. The first dimension of position and amplitude should match, but position can have more dimensions. Attributes: Name Type Description position Array array of shape (n, ...) amplitude Array array of shape (n,) make_position_amplitude_data ( position , amplitude , move_metadata ) Create PositionAmplitudeData from position, amplitude, and move_metadata. Parameters: Name Type Description Default position Array the particle positions required amplitude Array the wavefunction amplitudes required move_metadata Any other required metadata for the metropolis algorithm required Returns: Type Description PositionAmplitudeData data containing positions, wavefn amplitudes, and move metadata Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_data ( position : Array , amplitude : Array , move_metadata : Any ): \"\"\"Create PositionAmplitudeData from position, amplitude, and move_metadata. Args: position (Array): the particle positions amplitude (Array): the wavefunction amplitudes move_metadata (Any): other required metadata for the metropolis algorithm Returns: PositionAmplitudeData: data containing positions, wavefn amplitudes, and move metadata \"\"\" return PositionAmplitudeData ( walker_data = PositionAmplitudeWalkerData ( position = position , amplitude = amplitude ), move_metadata = move_metadata , ) get_position_from_data ( data ) Get the position data from PositionAmplitudeData. Parameters: Name Type Description Default data PositionAmplitudeData the data required Returns: Type Description Array the particle positions from the data Source code in vmcnet/mcmc/position_amplitude_core.py def get_position_from_data ( data : PositionAmplitudeData ) -> Array : \"\"\"Get the position data from PositionAmplitudeData. Args: data (PositionAmplitudeData): the data Returns: Array: the particle positions from the data \"\"\" return data [ \"walker_data\" ][ \"position\" ] get_amplitude_from_data ( data ) Get the amplitude data from PositionAmplitudeData. Parameters: Name Type Description Default data PositionAmplitudeData the data required Returns: Type Description Array the wave function amplitudes from the data Source code in vmcnet/mcmc/position_amplitude_core.py def get_amplitude_from_data ( data : PositionAmplitudeData ) -> Array : \"\"\"Get the amplitude data from PositionAmplitudeData. Args: data (PositionAmplitudeData): the data Returns: Array: the wave function amplitudes from the data \"\"\" return data [ \"walker_data\" ][ \"amplitude\" ] to_pam_tuple ( data ) Returns data as a (position, amplitude, move_metadata) tuple. Useful for quickly assigning all three pieces to local variables for further use. Source code in vmcnet/mcmc/position_amplitude_core.py def to_pam_tuple ( data : PositionAmplitudeData ) -> Tuple [ Array , Array , Any ]: \"\"\"Returns data as a (position, amplitude, move_metadata) tuple. Useful for quickly assigning all three pieces to local variables for further use. \"\"\" return ( data [ \"walker_data\" ][ \"position\" ], data [ \"walker_data\" ][ \"amplitude\" ], data [ \"move_metadata\" ], ) distribute_position_amplitude_data ( data ) Distribute PositionAmplitudeData across devices. Parameters: Name Type Description Default data PositionAmplitudeData the data to distribute required Returns: Type Description PositionAmplitudeData the distributed data. Source code in vmcnet/mcmc/position_amplitude_core.py def distribute_position_amplitude_data ( data : PositionAmplitudeData , ) -> PositionAmplitudeData : \"\"\"Distribute PositionAmplitudeData across devices. Args: data (PositionAmplitudeData): the data to distribute Returns: PositionAmplitudeData: the distributed data. \"\"\" walker_data = data [ \"walker_data\" ] move_metadata = data [ \"move_metadata\" ] walker_data = default_distribute_data ( walker_data ) move_metadata = replicate_all_local_devices ( move_metadata ) return PositionAmplitudeData ( walker_data = walker_data , move_metadata = move_metadata ) make_position_amplitude_gaussian_proposal ( model_apply , get_std_move ) Create a gaussian proposal fn on PositionAmplitudeData. Positions are perturbed by a guassian; amplitudes are evaluated using the supplied model; move_metadata is not modified. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required get_std_move Callable function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move required Returns: Type Description Callable proposal function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, key) -> (PositionAmplitudeData, key). Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_gaussian_proposal ( model_apply : ModelApply [ P ], get_std_move : Callable [[ PositionAmplitudeData ], jnp . float32 ], ) -> Callable [ [ P , PositionAmplitudeData , PRNGKey ], Tuple [ PositionAmplitudeData , PRNGKey ] ]: \"\"\"Create a gaussian proposal fn on PositionAmplitudeData. Positions are perturbed by a guassian; amplitudes are evaluated using the supplied model; move_metadata is not modified. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude get_std_move (Callable): function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move Returns: Callable: proposal function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, key) -> (PositionAmplitudeData, key). \"\"\" def proposal_fn ( params : P , data : PositionAmplitudeData , key : PRNGKey ): std_move = get_std_move ( data ) proposed_position , key = metropolis . gaussian_proposal ( data [ \"walker_data\" ][ \"position\" ], std_move , key ) proposed_amplitude = model_apply ( params , proposed_position ) return ( make_position_amplitude_data ( proposed_position , proposed_amplitude , data [ \"move_metadata\" ] ), key , ) return proposal_fn make_position_amplitude_metropolis_symmetric_acceptance ( logabs = True ) Create a Metropolis acceptance function on PositionAmplitudeData. Parameters: Name Type Description Default logabs bool whether amplitudes provided to acceptance_fn represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable acceptance function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, PositionAmplitudeData) -> accept_ratio Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_metropolis_symmetric_acceptance ( logabs : bool = True , ) -> Callable [[ P , PositionAmplitudeData , PositionAmplitudeData ], Array ]: \"\"\"Create a Metropolis acceptance function on PositionAmplitudeData. Args: logabs (bool, optional): whether amplitudes provided to `acceptance_fn` represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: acceptance function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, PositionAmplitudeData) -> accept_ratio \"\"\" def acceptance_fn ( params : P , data : PositionAmplitudeData , proposed_data : PositionAmplitudeData ): del params return metropolis . metropolis_symmetric_acceptance ( data [ \"walker_data\" ][ \"amplitude\" ], proposed_data [ \"walker_data\" ][ \"amplitude\" ], logabs = logabs , ) return acceptance_fn make_position_amplitude_update ( update_move_metadata_fn = None ) Factory for an update to PositionAmplitudeData. The returned update takes a mask of approved MCMC walker moves move_mask and accepts those proposed moves from proposed_data , for both positions and amplitudes. The std_move gaussian step width can also be modified by an optional adjust_std_move_fn . The moves in move_mask are applied along the first axis of the position data, and should be the same shape as the amplitude data (one-dimensional Array). Parameters: Name Type Description Default update_move_metadata_fn Callable function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata None Returns: Type Description Callable function with signature (PositionAmplitudeData, PositionAmplitudeData, Array) -> (PositionAmplitudeData), which takes in the original PositionAmplitudeData, the proposed PositionAmplitudeData, and a move mask. Uses the move mask to decide which proposed data to accept. Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_update ( update_move_metadata_fn : Optional [ Callable [[ M , Array ], M ]] = None ) -> Callable [ [ PositionAmplitudeData , PositionAmplitudeData , Array , ], PositionAmplitudeData , ]: \"\"\"Factory for an update to PositionAmplitudeData. The returned update takes a mask of approved MCMC walker moves `move_mask` and accepts those proposed moves from `proposed_data`, for both positions and amplitudes. The `std_move` gaussian step width can also be modified by an optional `adjust_std_move_fn`. The moves in `move_mask` are applied along the first axis of the position data, and should be the same shape as the amplitude data (one-dimensional Array). Args: update_move_metadata_fn (Callable): function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata Returns: Callable: function with signature (PositionAmplitudeData, PositionAmplitudeData, Array) -> (PositionAmplitudeData), which takes in the original PositionAmplitudeData, the proposed PositionAmplitudeData, and a move mask. Uses the move mask to decide which proposed data to accept. \"\"\" def update_position_amplitude ( data : PositionAmplitudeData , proposed_data : PositionAmplitudeData , move_mask : Array , ) -> PositionAmplitudeData : def mask_on_first_dimension ( old_data : Array , proposal : Array ): shaped_mask = jnp . reshape ( move_mask , ( - 1 , * (( 1 ,) * ( old_data . ndim - 1 )))) return jnp . where ( shaped_mask , proposal , old_data ) new_walker_data = jax . tree_map ( mask_on_first_dimension , data [ \"walker_data\" ], proposed_data [ \"walker_data\" ] ) new_move_metadata = proposed_data [ \"move_metadata\" ] if update_move_metadata_fn is not None : new_move_metadata = update_move_metadata_fn ( data [ \"move_metadata\" ], move_mask ) return PositionAmplitudeData ( walker_data = new_walker_data , move_metadata = new_move_metadata ) return update_position_amplitude make_position_amplitude_gaussian_metropolis_step ( model_apply , get_std_move , update_move_metadata_fn = None , logabs = True ) Make a gaussian proposal with Metropolis acceptance for PositionAmplitudeData. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required get_std_move Callable function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move required update_move_metadata_fn Callable function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata. None logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_gaussian_metropolis_step ( model_apply : ModelApply [ P ], get_std_move : Callable [[ PositionAmplitudeData ], jnp . float32 ], update_move_metadata_fn : Optional [ Callable [[ M , Array ], M ]] = None , logabs : bool = True , ) -> metropolis . MetropolisStep [ P , PositionAmplitudeData ]: \"\"\"Make a gaussian proposal with Metropolis acceptance for PositionAmplitudeData. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude get_std_move (Callable): function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move update_move_metadata_fn (Callable, optional): function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata. logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) \"\"\" proposal_fn = make_position_amplitude_gaussian_proposal ( model_apply , get_std_move ) accept_fn = make_position_amplitude_metropolis_symmetric_acceptance ( logabs = logabs ) metrop_step_fn = metropolis . make_metropolis_step ( proposal_fn , accept_fn , make_position_amplitude_update ( update_move_metadata_fn ), ) return metrop_step_fn","title":"position_amplitude_core"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.PositionAmplitudeData","text":"TypedDict of data holding positions, amplitudes, and optional metadata. Holding both particle position and wavefn amplitude in the data can be advantageous to avoid recalculating amplitudes in some routines, e.g. acceptance probabilities. Furthermore, holding additional metadata can enable more sophisticated metropolis algorithms such as dynamically adjusted gaussian step sizes. Attributes: Name Type Description walker_data PositionAmplitudeWalkerData the positions and amplitudes move_metadata any any metadata needed for the metropolis algorithm","title":"PositionAmplitudeData"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.PositionAmplitudeWalkerData","text":"TypedDict of walker data holding just positions and amplitudes. Holding both particle position and wavefn amplitude in the same named tuple allows us to simultaneously mask over both in the acceptance function. The first dimension of position and amplitude should match, but position can have more dimensions. Attributes: Name Type Description position Array array of shape (n, ...) amplitude Array array of shape (n,)","title":"PositionAmplitudeWalkerData"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.make_position_amplitude_data","text":"Create PositionAmplitudeData from position, amplitude, and move_metadata. Parameters: Name Type Description Default position Array the particle positions required amplitude Array the wavefunction amplitudes required move_metadata Any other required metadata for the metropolis algorithm required Returns: Type Description PositionAmplitudeData data containing positions, wavefn amplitudes, and move metadata Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_data ( position : Array , amplitude : Array , move_metadata : Any ): \"\"\"Create PositionAmplitudeData from position, amplitude, and move_metadata. Args: position (Array): the particle positions amplitude (Array): the wavefunction amplitudes move_metadata (Any): other required metadata for the metropolis algorithm Returns: PositionAmplitudeData: data containing positions, wavefn amplitudes, and move metadata \"\"\" return PositionAmplitudeData ( walker_data = PositionAmplitudeWalkerData ( position = position , amplitude = amplitude ), move_metadata = move_metadata , )","title":"make_position_amplitude_data()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.get_position_from_data","text":"Get the position data from PositionAmplitudeData. Parameters: Name Type Description Default data PositionAmplitudeData the data required Returns: Type Description Array the particle positions from the data Source code in vmcnet/mcmc/position_amplitude_core.py def get_position_from_data ( data : PositionAmplitudeData ) -> Array : \"\"\"Get the position data from PositionAmplitudeData. Args: data (PositionAmplitudeData): the data Returns: Array: the particle positions from the data \"\"\" return data [ \"walker_data\" ][ \"position\" ]","title":"get_position_from_data()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.get_amplitude_from_data","text":"Get the amplitude data from PositionAmplitudeData. Parameters: Name Type Description Default data PositionAmplitudeData the data required Returns: Type Description Array the wave function amplitudes from the data Source code in vmcnet/mcmc/position_amplitude_core.py def get_amplitude_from_data ( data : PositionAmplitudeData ) -> Array : \"\"\"Get the amplitude data from PositionAmplitudeData. Args: data (PositionAmplitudeData): the data Returns: Array: the wave function amplitudes from the data \"\"\" return data [ \"walker_data\" ][ \"amplitude\" ]","title":"get_amplitude_from_data()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.to_pam_tuple","text":"Returns data as a (position, amplitude, move_metadata) tuple. Useful for quickly assigning all three pieces to local variables for further use. Source code in vmcnet/mcmc/position_amplitude_core.py def to_pam_tuple ( data : PositionAmplitudeData ) -> Tuple [ Array , Array , Any ]: \"\"\"Returns data as a (position, amplitude, move_metadata) tuple. Useful for quickly assigning all three pieces to local variables for further use. \"\"\" return ( data [ \"walker_data\" ][ \"position\" ], data [ \"walker_data\" ][ \"amplitude\" ], data [ \"move_metadata\" ], )","title":"to_pam_tuple()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.distribute_position_amplitude_data","text":"Distribute PositionAmplitudeData across devices. Parameters: Name Type Description Default data PositionAmplitudeData the data to distribute required Returns: Type Description PositionAmplitudeData the distributed data. Source code in vmcnet/mcmc/position_amplitude_core.py def distribute_position_amplitude_data ( data : PositionAmplitudeData , ) -> PositionAmplitudeData : \"\"\"Distribute PositionAmplitudeData across devices. Args: data (PositionAmplitudeData): the data to distribute Returns: PositionAmplitudeData: the distributed data. \"\"\" walker_data = data [ \"walker_data\" ] move_metadata = data [ \"move_metadata\" ] walker_data = default_distribute_data ( walker_data ) move_metadata = replicate_all_local_devices ( move_metadata ) return PositionAmplitudeData ( walker_data = walker_data , move_metadata = move_metadata )","title":"distribute_position_amplitude_data()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.make_position_amplitude_gaussian_proposal","text":"Create a gaussian proposal fn on PositionAmplitudeData. Positions are perturbed by a guassian; amplitudes are evaluated using the supplied model; move_metadata is not modified. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required get_std_move Callable function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move required Returns: Type Description Callable proposal function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, key) -> (PositionAmplitudeData, key). Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_gaussian_proposal ( model_apply : ModelApply [ P ], get_std_move : Callable [[ PositionAmplitudeData ], jnp . float32 ], ) -> Callable [ [ P , PositionAmplitudeData , PRNGKey ], Tuple [ PositionAmplitudeData , PRNGKey ] ]: \"\"\"Create a gaussian proposal fn on PositionAmplitudeData. Positions are perturbed by a guassian; amplitudes are evaluated using the supplied model; move_metadata is not modified. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude get_std_move (Callable): function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move Returns: Callable: proposal function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, key) -> (PositionAmplitudeData, key). \"\"\" def proposal_fn ( params : P , data : PositionAmplitudeData , key : PRNGKey ): std_move = get_std_move ( data ) proposed_position , key = metropolis . gaussian_proposal ( data [ \"walker_data\" ][ \"position\" ], std_move , key ) proposed_amplitude = model_apply ( params , proposed_position ) return ( make_position_amplitude_data ( proposed_position , proposed_amplitude , data [ \"move_metadata\" ] ), key , ) return proposal_fn","title":"make_position_amplitude_gaussian_proposal()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.make_position_amplitude_metropolis_symmetric_acceptance","text":"Create a Metropolis acceptance function on PositionAmplitudeData. Parameters: Name Type Description Default logabs bool whether amplitudes provided to acceptance_fn represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable acceptance function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, PositionAmplitudeData) -> accept_ratio Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_metropolis_symmetric_acceptance ( logabs : bool = True , ) -> Callable [[ P , PositionAmplitudeData , PositionAmplitudeData ], Array ]: \"\"\"Create a Metropolis acceptance function on PositionAmplitudeData. Args: logabs (bool, optional): whether amplitudes provided to `acceptance_fn` represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: acceptance function which can be passed to the main VMC routine. Has signature (params, PositionAmplitudeData, PositionAmplitudeData) -> accept_ratio \"\"\" def acceptance_fn ( params : P , data : PositionAmplitudeData , proposed_data : PositionAmplitudeData ): del params return metropolis . metropolis_symmetric_acceptance ( data [ \"walker_data\" ][ \"amplitude\" ], proposed_data [ \"walker_data\" ][ \"amplitude\" ], logabs = logabs , ) return acceptance_fn","title":"make_position_amplitude_metropolis_symmetric_acceptance()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.make_position_amplitude_update","text":"Factory for an update to PositionAmplitudeData. The returned update takes a mask of approved MCMC walker moves move_mask and accepts those proposed moves from proposed_data , for both positions and amplitudes. The std_move gaussian step width can also be modified by an optional adjust_std_move_fn . The moves in move_mask are applied along the first axis of the position data, and should be the same shape as the amplitude data (one-dimensional Array). Parameters: Name Type Description Default update_move_metadata_fn Callable function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata None Returns: Type Description Callable function with signature (PositionAmplitudeData, PositionAmplitudeData, Array) -> (PositionAmplitudeData), which takes in the original PositionAmplitudeData, the proposed PositionAmplitudeData, and a move mask. Uses the move mask to decide which proposed data to accept. Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_update ( update_move_metadata_fn : Optional [ Callable [[ M , Array ], M ]] = None ) -> Callable [ [ PositionAmplitudeData , PositionAmplitudeData , Array , ], PositionAmplitudeData , ]: \"\"\"Factory for an update to PositionAmplitudeData. The returned update takes a mask of approved MCMC walker moves `move_mask` and accepts those proposed moves from `proposed_data`, for both positions and amplitudes. The `std_move` gaussian step width can also be modified by an optional `adjust_std_move_fn`. The moves in `move_mask` are applied along the first axis of the position data, and should be the same shape as the amplitude data (one-dimensional Array). Args: update_move_metadata_fn (Callable): function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata Returns: Callable: function with signature (PositionAmplitudeData, PositionAmplitudeData, Array) -> (PositionAmplitudeData), which takes in the original PositionAmplitudeData, the proposed PositionAmplitudeData, and a move mask. Uses the move mask to decide which proposed data to accept. \"\"\" def update_position_amplitude ( data : PositionAmplitudeData , proposed_data : PositionAmplitudeData , move_mask : Array , ) -> PositionAmplitudeData : def mask_on_first_dimension ( old_data : Array , proposal : Array ): shaped_mask = jnp . reshape ( move_mask , ( - 1 , * (( 1 ,) * ( old_data . ndim - 1 )))) return jnp . where ( shaped_mask , proposal , old_data ) new_walker_data = jax . tree_map ( mask_on_first_dimension , data [ \"walker_data\" ], proposed_data [ \"walker_data\" ] ) new_move_metadata = proposed_data [ \"move_metadata\" ] if update_move_metadata_fn is not None : new_move_metadata = update_move_metadata_fn ( data [ \"move_metadata\" ], move_mask ) return PositionAmplitudeData ( walker_data = new_walker_data , move_metadata = new_move_metadata ) return update_position_amplitude","title":"make_position_amplitude_update()"},{"location":"api/mcmc/position_amplitude_core/#vmcnet.mcmc.position_amplitude_core.make_position_amplitude_gaussian_metropolis_step","text":"Make a gaussian proposal with Metropolis acceptance for PositionAmplitudeData. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required get_std_move Callable function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move required update_move_metadata_fn Callable function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata. None logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) Source code in vmcnet/mcmc/position_amplitude_core.py def make_position_amplitude_gaussian_metropolis_step ( model_apply : ModelApply [ P ], get_std_move : Callable [[ PositionAmplitudeData ], jnp . float32 ], update_move_metadata_fn : Optional [ Callable [[ M , Array ], M ]] = None , logabs : bool = True , ) -> metropolis . MetropolisStep [ P , PositionAmplitudeData ]: \"\"\"Make a gaussian proposal with Metropolis acceptance for PositionAmplitudeData. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude get_std_move (Callable): function which gets the standard deviation of the gaussian move, which can optionally depend on the data. Has signature (PositionAmplitudeData) -> std_move update_move_metadata_fn (Callable, optional): function which calculates the new move_metadata. Has signature (old_move_metadata, move_mask) -> new_move_metadata. logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) \"\"\" proposal_fn = make_position_amplitude_gaussian_proposal ( model_apply , get_std_move ) accept_fn = make_position_amplitude_metropolis_symmetric_acceptance ( logabs = logabs ) metrop_step_fn = metropolis . make_metropolis_step ( proposal_fn , accept_fn , make_position_amplitude_update ( update_move_metadata_fn ), ) return metrop_step_fn","title":"make_position_amplitude_gaussian_metropolis_step()"},{"location":"api/mcmc/simple_position_amplitude/","text":"Helper functions for position amplitude data with fixed-width gaussian steps. SPAData ( dict ) TypedDict of positions, amplitudes, and nothing else. SimplePositionAmplitudeData ( dict ) TypedDict of positions, amplitudes, and nothing else. make_simple_position_amplitude_data ( position , amplitude ) Create SimplePositionAmplitudeData from position and amplitude. Parameters: Name Type Description Default position Array the particle positions required amplitude Array the wavefunction amplitudes required Returns: Type Description SimplePositionAmplitudeData SPAData Source code in vmcnet/mcmc/simple_position_amplitude.py def make_simple_position_amplitude_data ( position : Array , amplitude : Array ) -> SPAData : \"\"\"Create SimplePositionAmplitudeData from position and amplitude. Args: position (Array): the particle positions amplitude (Array): the wavefunction amplitudes Returns: SPAData \"\"\" return make_position_amplitude_data ( position , amplitude , None ) make_simple_pos_amp_gaussian_step ( model_apply , std_move , logabs = True ) Create metropolis step for PositionAmplitudeData with fixed gaussian step width. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required std_move float32 the standard deviation of the gaussian step required logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) Source code in vmcnet/mcmc/simple_position_amplitude.py def make_simple_pos_amp_gaussian_step ( model_apply : ModelApply [ P ], std_move : jnp . float32 , logabs : bool = True , ) -> MetropolisStep [ P , SPAData ]: \"\"\"Create metropolis step for PositionAmplitudeData with fixed gaussian step width. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude std_move: the standard deviation of the gaussian step logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) \"\"\" return make_position_amplitude_gaussian_metropolis_step ( model_apply , lambda _ : std_move , None , logabs )","title":"simple_position_amplitude"},{"location":"api/mcmc/simple_position_amplitude/#vmcnet.mcmc.simple_position_amplitude.SPAData","text":"TypedDict of positions, amplitudes, and nothing else.","title":"SPAData"},{"location":"api/mcmc/simple_position_amplitude/#vmcnet.mcmc.simple_position_amplitude.SimplePositionAmplitudeData","text":"TypedDict of positions, amplitudes, and nothing else.","title":"SimplePositionAmplitudeData"},{"location":"api/mcmc/simple_position_amplitude/#vmcnet.mcmc.simple_position_amplitude.make_simple_position_amplitude_data","text":"Create SimplePositionAmplitudeData from position and amplitude. Parameters: Name Type Description Default position Array the particle positions required amplitude Array the wavefunction amplitudes required Returns: Type Description SimplePositionAmplitudeData SPAData Source code in vmcnet/mcmc/simple_position_amplitude.py def make_simple_position_amplitude_data ( position : Array , amplitude : Array ) -> SPAData : \"\"\"Create SimplePositionAmplitudeData from position and amplitude. Args: position (Array): the particle positions amplitude (Array): the wavefunction amplitudes Returns: SPAData \"\"\" return make_position_amplitude_data ( position , amplitude , None )","title":"make_simple_position_amplitude_data()"},{"location":"api/mcmc/simple_position_amplitude/#vmcnet.mcmc.simple_position_amplitude.make_simple_pos_amp_gaussian_step","text":"Create metropolis step for PositionAmplitudeData with fixed gaussian step width. Parameters: Name Type Description Default model_apply Callable function which evaluates a model. Has signature (params, position) -> amplitude required std_move float32 the standard deviation of the gaussian step required logabs bool whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. True Returns: Type Description Callable function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) Source code in vmcnet/mcmc/simple_position_amplitude.py def make_simple_pos_amp_gaussian_step ( model_apply : ModelApply [ P ], std_move : jnp . float32 , logabs : bool = True , ) -> MetropolisStep [ P , SPAData ]: \"\"\"Create metropolis step for PositionAmplitudeData with fixed gaussian step width. Args: model_apply (Callable): function which evaluates a model. Has signature (params, position) -> amplitude std_move: the standard deviation of the gaussian step logabs (bool, optional): whether the provided amplitudes represent psi (logabs = False) or log|psi| (logabs = True). Defaults to True. Returns: Callable: function which does a metropolis step. Has the signature (params, PositionAmplitudeData, key) -> (mean acceptance probability, PositionAmplitudeData, new_key) \"\"\" return make_position_amplitude_gaussian_metropolis_step ( model_apply , lambda _ : std_move , None , logabs )","title":"make_simple_pos_amp_gaussian_step()"},{"location":"api/mcmc/statistics/","text":"Routines for computing statistics related to MCMC time series. per_chain_autocorr_fast ( samples , cutoff = None ) Calculate autocorrelation curve per chain using FFT. See Sokal, Alan D., \"Monte Carlo Methods in Statistical Mechanics: Foundations and New Algorithms,\" pp. 13-16, September 1996. http://staff.ustc.edu.cn/~yjdeng/lecturenotes/cargese_1996.pdf Parameters: Name Type Description Default samples np.ndarray samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. required cutoff int hard cut-off for the length of returned curve. None Returns: Type Description np.ndarray autcorrelation curve per chain, of shape (C, M), where C = min(N, cutoff) Source code in vmcnet/mcmc/statistics.py def per_chain_autocorr_fast ( samples : np . ndarray , cutoff : Optional [ int ] = None ) -> np . ndarray : \"\"\"Calculate autocorrelation curve per chain using FFT. See Sokal, Alan D., \"Monte Carlo Methods in Statistical Mechanics: Foundations and New Algorithms,\" pp. 13-16, September 1996. http://staff.ustc.edu.cn/~yjdeng/lecturenotes/cargese_1996.pdf Args: samples (np.ndarray): samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. cutoff (int): hard cut-off for the length of returned curve. Returns: np.ndarray: autcorrelation curve per chain, of shape (C, M), where C = min(N, cutoff) \"\"\" N = len ( samples ) if cutoff is None : cutoff = N else : cutoff = min ( cutoff , N ) centered_samples = samples - np . mean ( samples , axis = 0 , keepdims = True ) # Calculate autocorrelation curve efficiently as the inverse Fourier transform # of the power spectral density of the series. fvi = np . fft . fft ( centered_samples , n = ( 2 * N ), axis = 0 ) G = np . real ( np . fft . ifft ( fvi * np . conjugate ( fvi ), axis = 0 ))[: cutoff ] # Divide (i)th term by (n-i) to account for the number of available samples. normalization_factors = N - np . arange ( cutoff ) G /= np . expand_dims ( normalization_factors , - 1 ) # Divide by C(0) to get the normalized autocorrelation curve. G /= G [: 1 ] return G multi_chain_autocorr_and_variance ( samples , cutoff = None ) Calculate multi-chain autocorrelation curve with cutoff and multi-chain variance. The variance estimate here is the population variance, sum_i (x_i - mu)^2 / N, and not the sample variance, sum_i (x_i - mu)^2 / (N - 1). See Stan Reference Manual, Version 2.27, Section 16.3-16-4 https://mc-stan.org/docs/2_27/reference-manual Parameters: Name Type Description Default samples np.ndarray samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. required cutoff int hard cut-off for the length of returned curve. None Returns: Type Description (np.ndarray, np.float32) combined autcorrelation curve using data from all chains, of length C, where C = min(N, cutoff); overall variance estimate Source code in vmcnet/mcmc/statistics.py def multi_chain_autocorr_and_variance ( samples : np . ndarray , cutoff : Optional [ int ] = None ) -> Tuple [ np . ndarray , np . float32 ]: \"\"\"Calculate multi-chain autocorrelation curve with cutoff and multi-chain variance. The variance estimate here is the population variance, sum_i (x_i - mu)^2 / N, and *not* the sample variance, sum_i (x_i - mu)^2 / (N - 1). See Stan Reference Manual, Version 2.27, Section 16.3-16-4 https://mc-stan.org/docs/2_27/reference-manual Args: samples (np.ndarray): samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. cutoff (int): hard cut-off for the length of returned curve. Returns: (np.ndarray, np.float32): combined autcorrelation curve using data from all chains, of length C, where C = min(N, cutoff); overall variance estimate \"\"\" N = len ( samples ) per_chain_autocorr = per_chain_autocorr_fast ( samples , cutoff ) per_chain_var = np . var ( samples , axis = 0 , ddof = 1 ) autocorrelation_term = np . mean ( per_chain_var * per_chain_autocorr , axis = 1 ) between_chain_term = np . var ( np . mean ( samples , axis = 0 ), ddof = 1 ) within_chain_term = np . mean ( per_chain_var ) overall_var_estimate = within_chain_term * ( N - 1 ) / N + between_chain_term return ( 1 - ( within_chain_term - autocorrelation_term ) / overall_var_estimate , overall_var_estimate , ) tau ( autocorr_curve ) Integrated autocorrelation, with automatic truncation. Uses Geyer's initial minimum sequence for estimating the integrated autocorrelation. This is a consistent overestimate of the asymptotic IAC. See Geyer, Charles J. 2011. \u201cIntroduction to Markov Chain Monte Carlo.\u201d In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 3\u201348. Chapman; Hall/CRC. Also see Charles J. Geyer. \"Practical Markov Chain Monte Carlo.\" Statist. Sci. 7 (4) 473 - 483, November, 1992. https://doi.org/10.1214/ss/1177011137 Parameters: Name Type Description Default autocorr_curve np.ndarray 1D array containing the series of autocorrelation values over which to calculate the autocorrelation time. required Returns: Type Description (np.ndarray) a single estimate of the autocorrelation time, packaged as an array. Source code in vmcnet/mcmc/statistics.py def tau ( autocorr_curve : np . ndarray ) -> np . ndarray : \"\"\"Integrated autocorrelation, with automatic truncation. Uses Geyer's initial minimum sequence for estimating the integrated autocorrelation. This is a consistent overestimate of the asymptotic IAC. See Geyer, Charles J. 2011. \u201cIntroduction to Markov Chain Monte Carlo.\u201d In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 3\u201348. Chapman; Hall/CRC. Also see Charles J. Geyer. \"Practical Markov Chain Monte Carlo.\" Statist. Sci. 7 (4) 473 - 483, November, 1992. https://doi.org/10.1214/ss/1177011137 Args: autocorr_curve (np.ndarray): 1D array containing the series of autocorrelation values over which to calculate the autocorrelation time. Returns: (np.ndarray): a single estimate of the autocorrelation time, packaged as an array. \"\"\" # Cut off the last sample if necessary to get an even number of samples. nsamples = autocorr_curve . shape [ 0 ] even_nsamples = int ( 2 * np . floor ( nsamples / 2 )) even_length_curve = autocorr_curve [: even_nsamples ] # Create new curve containing sums of adjacent pairs from the initial curve. paired_curve = even_length_curve . reshape ( ( even_nsamples // 2 , 2 ) + tuple ( autocorr_curve . shape [ 1 :]) ) sums_of_pairs = np . sum ( paired_curve , axis = 1 ) # The new curve is always positive in theory. In practice, the first negative term # can be used as a sentinel to decide when to cut off the calculation. negative_indices = np . nonzero ( sums_of_pairs < 0 )[ 0 ] if len ( negative_indices ) > 0 : first_negative_idx = negative_indices [ 0 ] else : first_negative_idx = len ( sums_of_pairs ) positive_sums_curve = sums_of_pairs [: first_negative_idx ] # Final estimate is based on the sum of the monotonically decreasing curve created # by taking the running minimum of the positive_sums_curve. monotonic_min_curve = np . minimum . accumulate ( positive_sums_curve ) return - 1.0 + 2.0 * np . sum ( monotonic_min_curve , axis = 0 ) get_stats_summary ( samples ) Get a summary of the stats (mean, var, std_err, iac) for a collection of samples. Parameters: Name Type Description Default samples np.ndarray samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. required Returns: Type Description dictionary a summary of the statistics, with keys \"average\", \"variance\", \"std_err\", and \"integrated_autocorrelation\" Source code in vmcnet/mcmc/statistics.py def get_stats_summary ( samples : np . ndarray ) -> Dict [ str , np . float32 ]: \"\"\"Get a summary of the stats (mean, var, std_err, iac) for a collection of samples. Args: samples (np.ndarray): samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. Returns: dictionary: a summary of the statistics, with keys \"average\", \"variance\", \"std_err\", and \"integrated_autocorrelation\" \"\"\" # Nested mean may be more numerically stable than single mean average = np . mean ( np . mean ( samples , axis =- 1 ), axis =- 1 ) autocorr_curve , variance = multi_chain_autocorr_and_variance ( samples ) iac = tau ( autocorr_curve ) std_err = np . sqrt ( iac * variance / np . size ( samples )) eval_statistics = { \"average\" : average , \"variance\" : variance , \"std_err\" : std_err , \"integrated_autocorrelation\" : iac , } return eval_statistics","title":"statistics"},{"location":"api/mcmc/statistics/#vmcnet.mcmc.statistics.per_chain_autocorr_fast","text":"Calculate autocorrelation curve per chain using FFT. See Sokal, Alan D., \"Monte Carlo Methods in Statistical Mechanics: Foundations and New Algorithms,\" pp. 13-16, September 1996. http://staff.ustc.edu.cn/~yjdeng/lecturenotes/cargese_1996.pdf Parameters: Name Type Description Default samples np.ndarray samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. required cutoff int hard cut-off for the length of returned curve. None Returns: Type Description np.ndarray autcorrelation curve per chain, of shape (C, M), where C = min(N, cutoff) Source code in vmcnet/mcmc/statistics.py def per_chain_autocorr_fast ( samples : np . ndarray , cutoff : Optional [ int ] = None ) -> np . ndarray : \"\"\"Calculate autocorrelation curve per chain using FFT. See Sokal, Alan D., \"Monte Carlo Methods in Statistical Mechanics: Foundations and New Algorithms,\" pp. 13-16, September 1996. http://staff.ustc.edu.cn/~yjdeng/lecturenotes/cargese_1996.pdf Args: samples (np.ndarray): samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. cutoff (int): hard cut-off for the length of returned curve. Returns: np.ndarray: autcorrelation curve per chain, of shape (C, M), where C = min(N, cutoff) \"\"\" N = len ( samples ) if cutoff is None : cutoff = N else : cutoff = min ( cutoff , N ) centered_samples = samples - np . mean ( samples , axis = 0 , keepdims = True ) # Calculate autocorrelation curve efficiently as the inverse Fourier transform # of the power spectral density of the series. fvi = np . fft . fft ( centered_samples , n = ( 2 * N ), axis = 0 ) G = np . real ( np . fft . ifft ( fvi * np . conjugate ( fvi ), axis = 0 ))[: cutoff ] # Divide (i)th term by (n-i) to account for the number of available samples. normalization_factors = N - np . arange ( cutoff ) G /= np . expand_dims ( normalization_factors , - 1 ) # Divide by C(0) to get the normalized autocorrelation curve. G /= G [: 1 ] return G","title":"per_chain_autocorr_fast()"},{"location":"api/mcmc/statistics/#vmcnet.mcmc.statistics.multi_chain_autocorr_and_variance","text":"Calculate multi-chain autocorrelation curve with cutoff and multi-chain variance. The variance estimate here is the population variance, sum_i (x_i - mu)^2 / N, and not the sample variance, sum_i (x_i - mu)^2 / (N - 1). See Stan Reference Manual, Version 2.27, Section 16.3-16-4 https://mc-stan.org/docs/2_27/reference-manual Parameters: Name Type Description Default samples np.ndarray samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. required cutoff int hard cut-off for the length of returned curve. None Returns: Type Description (np.ndarray, np.float32) combined autcorrelation curve using data from all chains, of length C, where C = min(N, cutoff); overall variance estimate Source code in vmcnet/mcmc/statistics.py def multi_chain_autocorr_and_variance ( samples : np . ndarray , cutoff : Optional [ int ] = None ) -> Tuple [ np . ndarray , np . float32 ]: \"\"\"Calculate multi-chain autocorrelation curve with cutoff and multi-chain variance. The variance estimate here is the population variance, sum_i (x_i - mu)^2 / N, and *not* the sample variance, sum_i (x_i - mu)^2 / (N - 1). See Stan Reference Manual, Version 2.27, Section 16.3-16-4 https://mc-stan.org/docs/2_27/reference-manual Args: samples (np.ndarray): samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. cutoff (int): hard cut-off for the length of returned curve. Returns: (np.ndarray, np.float32): combined autcorrelation curve using data from all chains, of length C, where C = min(N, cutoff); overall variance estimate \"\"\" N = len ( samples ) per_chain_autocorr = per_chain_autocorr_fast ( samples , cutoff ) per_chain_var = np . var ( samples , axis = 0 , ddof = 1 ) autocorrelation_term = np . mean ( per_chain_var * per_chain_autocorr , axis = 1 ) between_chain_term = np . var ( np . mean ( samples , axis = 0 ), ddof = 1 ) within_chain_term = np . mean ( per_chain_var ) overall_var_estimate = within_chain_term * ( N - 1 ) / N + between_chain_term return ( 1 - ( within_chain_term - autocorrelation_term ) / overall_var_estimate , overall_var_estimate , )","title":"multi_chain_autocorr_and_variance()"},{"location":"api/mcmc/statistics/#vmcnet.mcmc.statistics.tau","text":"Integrated autocorrelation, with automatic truncation. Uses Geyer's initial minimum sequence for estimating the integrated autocorrelation. This is a consistent overestimate of the asymptotic IAC. See Geyer, Charles J. 2011. \u201cIntroduction to Markov Chain Monte Carlo.\u201d In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 3\u201348. Chapman; Hall/CRC. Also see Charles J. Geyer. \"Practical Markov Chain Monte Carlo.\" Statist. Sci. 7 (4) 473 - 483, November, 1992. https://doi.org/10.1214/ss/1177011137 Parameters: Name Type Description Default autocorr_curve np.ndarray 1D array containing the series of autocorrelation values over which to calculate the autocorrelation time. required Returns: Type Description (np.ndarray) a single estimate of the autocorrelation time, packaged as an array. Source code in vmcnet/mcmc/statistics.py def tau ( autocorr_curve : np . ndarray ) -> np . ndarray : \"\"\"Integrated autocorrelation, with automatic truncation. Uses Geyer's initial minimum sequence for estimating the integrated autocorrelation. This is a consistent overestimate of the asymptotic IAC. See Geyer, Charles J. 2011. \u201cIntroduction to Markov Chain Monte Carlo.\u201d In Handbook of Markov Chain Monte Carlo, edited by Steve Brooks, Andrew Gelman, Galin L. Jones, and Xiao-Li Meng, 3\u201348. Chapman; Hall/CRC. Also see Charles J. Geyer. \"Practical Markov Chain Monte Carlo.\" Statist. Sci. 7 (4) 473 - 483, November, 1992. https://doi.org/10.1214/ss/1177011137 Args: autocorr_curve (np.ndarray): 1D array containing the series of autocorrelation values over which to calculate the autocorrelation time. Returns: (np.ndarray): a single estimate of the autocorrelation time, packaged as an array. \"\"\" # Cut off the last sample if necessary to get an even number of samples. nsamples = autocorr_curve . shape [ 0 ] even_nsamples = int ( 2 * np . floor ( nsamples / 2 )) even_length_curve = autocorr_curve [: even_nsamples ] # Create new curve containing sums of adjacent pairs from the initial curve. paired_curve = even_length_curve . reshape ( ( even_nsamples // 2 , 2 ) + tuple ( autocorr_curve . shape [ 1 :]) ) sums_of_pairs = np . sum ( paired_curve , axis = 1 ) # The new curve is always positive in theory. In practice, the first negative term # can be used as a sentinel to decide when to cut off the calculation. negative_indices = np . nonzero ( sums_of_pairs < 0 )[ 0 ] if len ( negative_indices ) > 0 : first_negative_idx = negative_indices [ 0 ] else : first_negative_idx = len ( sums_of_pairs ) positive_sums_curve = sums_of_pairs [: first_negative_idx ] # Final estimate is based on the sum of the monotonically decreasing curve created # by taking the running minimum of the positive_sums_curve. monotonic_min_curve = np . minimum . accumulate ( positive_sums_curve ) return - 1.0 + 2.0 * np . sum ( monotonic_min_curve , axis = 0 )","title":"tau()"},{"location":"api/mcmc/statistics/#vmcnet.mcmc.statistics.get_stats_summary","text":"Get a summary of the stats (mean, var, std_err, iac) for a collection of samples. Parameters: Name Type Description Default samples np.ndarray samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. required Returns: Type Description dictionary a summary of the statistics, with keys \"average\", \"variance\", \"std_err\", and \"integrated_autocorrelation\" Source code in vmcnet/mcmc/statistics.py def get_stats_summary ( samples : np . ndarray ) -> Dict [ str , np . float32 ]: \"\"\"Get a summary of the stats (mean, var, std_err, iac) for a collection of samples. Args: samples (np.ndarray): samples of shape (N, M) where N is num-samples-per-chain and M is num-chains. Returns: dictionary: a summary of the statistics, with keys \"average\", \"variance\", \"std_err\", and \"integrated_autocorrelation\" \"\"\" # Nested mean may be more numerically stable than single mean average = np . mean ( np . mean ( samples , axis =- 1 ), axis =- 1 ) autocorr_curve , variance = multi_chain_autocorr_and_variance ( samples ) iac = tau ( autocorr_curve ) std_err = np . sqrt ( iac * variance / np . size ( samples )) eval_statistics = { \"average\" : average , \"variance\" : variance , \"std_err\" : std_err , \"integrated_autocorrelation\" : iac , } return eval_statistics","title":"get_stats_summary()"},{"location":"api/models/antiequivariance/","text":"Antiequivariant parts to compose into a model. OrbitalCofactorAntiequivarianceLayer ( Module ) dataclass Apply a cofactor antiequivariance multiplicatively to equivariant inputs. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. __call__ ( self , eq_inputs , r_ei = None ) special Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (ArrayList) per-spin list where each list entry is an array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (ArrayList): per-spin list where each list entry is an array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) ferminet_orbital_layer = FermiNetOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # Calculate orbital matrices as list of shape [(..., nelec[i], nelec[i])] orbital_matrix_list = ferminet_orbital_layer ( eq_inputs , r_ei ) # Calculate cofactors as list of shape [(..., nelec[i])] cofactors = jax . tree_map ( cofactor_antieq , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), cofactors ) PerParticleDeterminantAntiequivarianceLayer ( Module ) dataclass Antieq. layer based on determinants of per-particle orbital matrices, slog out. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. __call__ ( self , eq_inputs , r_ei = None ) special Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (ArrayList) per-spin list where each list entry is an array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (ArrayList): per-spin list where each list entry is an array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) equivariant_orbital_layer = DoublyEquivariantOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # ArrayList of shape (..., nelec[i], nelec[i], nelec[i]) orbital_matrix_list = equivariant_orbital_layer ( eq_inputs , r_ei ) # ArrayList of nspins arrays of shape (..., nelec[i]) dets = jax . tree_map ( jnp . linalg . det , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), dets ) SLogOrbitalCofactorAntiequivarianceLayer ( Module ) dataclass Apply a cofactor antieq. multiplicatively to equivariant inputs with slog out. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. __call__ ( self , eq_inputs , r_ei = None ) special Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (SLArrayList) per-spin list where each list entry is an slog array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> SLArrayList : \"\"\"Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (SLArrayList): per-spin list where each list entry is an slog array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) ferminet_orbital_layer = FermiNetOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # Calculate orbital matrices as list of shape [(..., nelec[i], nelec[i])] orbital_matrix_list = ferminet_orbital_layer ( eq_inputs , r_ei ) # Calculate slog cofactors as list of shape [((..., nelec[i]), (..., nelec[i]))] slog_cofactors = jax . tree_map ( slog_cofactor_antieq , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), slog_cofactors ) SLogPerParticleDeterminantAntiequivarianceLayer ( Module ) dataclass Antieq. layer based on determinants of per-particle orbital matrices, slog out. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. __call__ ( self , eq_inputs , r_ei = None ) special Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (SLArrayList) per-spin list where each list entry is an slog array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> SLArrayList : \"\"\"Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (SLArrayList): per-spin list where each list entry is an slog array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) equivariant_orbital_layer = DoublyEquivariantOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # ArrayList of shape (..., nelec[i], nelec[i], nelec[i]) orbital_matrix_list = equivariant_orbital_layer ( eq_inputs , r_ei ) # SLArrayList of nspins slog arrays of shape (..., nelec[i]) slog_dets = jax . tree_map ( jnp . linalg . slogdet , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), slog_dets ) get_submatrices_along_first_col ( x ) Get the submatrices of x by deleting row i and col 0, for all rows of x. Parameters: Name Type Description Default x Array a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. required Returns: Type Description (int, Array) n, submatrices of shape (..., n, n-1, n-1), obtained by deleting row (..., i, :) and deleted column is (..., :, 0), for 0 <= i <= n - 1. Source code in vmcnet/models/antiequivariance.py def get_submatrices_along_first_col ( x : Array ) -> Tuple [ int , Array ]: \"\"\"Get the submatrices of x by deleting row i and col 0, for all rows of x. Args: x (Array): a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. Returns: (int, Array): n, submatrices of shape (..., n, n-1, n-1), obtained by deleting row (..., i, :) and deleted column is (..., :, 0), for 0 <= i <= n - 1. \"\"\" if len ( x . shape ) < 2 or x . shape [ - 1 ] != x . shape [ - 2 ]: msg = \"Calculating cofactors requires shape (..., n, n), got {} \" raise ValueError ( msg . format ( x . shape )) n = x . shape [ - 1 ] # Calculate minor_(0,i) by deleting the first orbital and ith particle indices submats = [ jnp . delete ( jnp . delete ( x , i , axis =- 2 ), 0 , axis =- 1 ) for i in range ( n )] # Stack on axis -3 to ensure shape (..., n) once det removes the last two axes stacked_submats = jnp . stack ( submats , axis =- 3 ) return n , stacked_submats cofactor_antieq ( x ) Compute a cofactor-based antiequivariance. Input must be square in the last two dimensions, of shape (..., n, n). The second last dimension is assumed to be the particle dimension, and the last is assumed to be the orbital dimension. The output will be of shape (..., n), preserving the particle dimension but getting rid of the orbital dimension. The transformation applies to each square matrix separately. Given an nxn matrix M, with cofactor matrix C, the output for that matrix will be a single vector of length n, whose ith component is M_(i,0) C_(i,0) (-1)**i. This is a single term in the cofactor expansion of the determinant of M. Thus, the sum of the returned vector values will always be equal to the determinant of M. This function implements an antiequivariant transformation, meaning that permuting two particle indices in the input will result in the output having 1) the same two particle indices permuted, and 2) ALL values multiplied by -1. Parameters: Name Type Description Default x Array a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. required Returns: Type Description (Array) array of shape (..., n), with the ith output (along the last axis) given by the ith term in the cofactor expansion of det(x) along the first entry of the last axis Source code in vmcnet/models/antiequivariance.py def cofactor_antieq ( x : Array ) -> Array : \"\"\"Compute a cofactor-based antiequivariance. Input must be square in the last two dimensions, of shape (..., n, n). The second last dimension is assumed to be the particle dimension, and the last is assumed to be the orbital dimension. The output will be of shape (..., n), preserving the particle dimension but getting rid of the orbital dimension. The transformation applies to each square matrix separately. Given an nxn matrix M, with cofactor matrix C, the output for that matrix will be a single vector of length n, whose ith component is M_(i,0)*C_(i,0)*(-1)**i. This is a single term in the cofactor expansion of the determinant of M. Thus, the sum of the returned vector values will always be equal to the determinant of M. This function implements an antiequivariant transformation, meaning that permuting two particle indices in the input will result in the output having 1) the same two particle indices permuted, and 2) ALL values multiplied by -1. Args: x (Array): a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. Returns: (Array): array of shape (..., n), with the ith output (along the last axis) given by the ith term in the cofactor expansion of det(x) along the first entry of the last axis \"\"\" first_orbital_vals = x [ ... , 0 ] n , stacked_submatrices = get_submatrices_along_first_col ( x ) cofactors = get_alternating_signs ( n ) * jnp . linalg . det ( stacked_submatrices ) return first_orbital_vals * cofactors slog_cofactor_antieq ( x ) Compute a cofactor-based antiequivariance, returning results in slogabs form. See :func: ~vmcnet.models.antiequivariance.cofactor_antieq . This function performs the same operations, but gives a result in the (sign, log) domain, going through a jnp.linalg.slogdet call instead of a jnp.linalg.det call. Parameters: Name Type Description Default x Array a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. required Returns: Type Description (Array, Array) tuple of arrays, each of shape (..., n). The first is sign(result), and the second is log(abs(result)). Source code in vmcnet/models/antiequivariance.py def slog_cofactor_antieq ( x : Array ) -> SLArray : \"\"\"Compute a cofactor-based antiequivariance, returning results in slogabs form. See :func:`~vmcnet.models.antiequivariance.cofactor_antieq`. This function performs the same operations, but gives a result in the (sign, log) domain, going through a jnp.linalg.slogdet call instead of a jnp.linalg.det call. Args: x (Array): a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. Returns: (Array, Array): tuple of arrays, each of shape (..., n). The first is sign(result), and the second is log(abs(result)). \"\"\" # Calculate x_(i, 0) by selecting orbital index 0 first_orbital_vals = x [ ... , 0 ] orbital_signs , orbital_logs = array_to_slog ( first_orbital_vals ) n , stacked_submatrices = get_submatrices_along_first_col ( x ) # TODO(ggoldsh): find a faster way to calculate these overlapping determinants. ( cofactor_signs , cofactor_logs ) = jnp . linalg . slogdet ( stacked_submatrices ) signs_and_logs = ( orbital_signs * cofactor_signs * get_alternating_signs ( n ), orbital_logs + cofactor_logs , ) return signs_and_logs multiply_antieq_by_eq_features ( split_antieq , eq_features , spin_split ) Multiply equivariant input array with a spin-split antiequivariance. Parameters: Name Type Description Default split_antieq ArrayList list of arrays containing nspins arrays of shape broadcastable to (..., nelec[i], 1) required eq_features Array array of shape (..., nelec, d) required spin_split ParticleSplit the spin split. required Returns: Type Description (ArrayList) list of per-spin arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. Source code in vmcnet/models/antiequivariance.py def multiply_antieq_by_eq_features ( split_antieq : ArrayList , eq_features : Array , spin_split : ParticleSplit , ) -> ArrayList : \"\"\"Multiply equivariant input array with a spin-split antiequivariance. Args: split_antieq (ArrayList): list of arrays containing nspins arrays of shape broadcastable to (..., nelec[i], 1) eq_features (Array): array of shape (..., nelec, d) spin_split (ParticleSplit): the spin split. Returns: (ArrayList): list of per-spin arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. \"\"\" split_inputs = jnp . split ( eq_features , spin_split , axis =- 2 ) return tree_prod ( split_inputs , split_antieq ) multiply_slog_antieq_by_eq_features ( split_slog_antieq , eq_features , spin_split ) Multiply equivariant input array with a spin-split slog-form antiequivariance. Parameters: Name Type Description Default split_slog_antieq SLArrayList SLArrayList containing nspins arrays of shape broadcastable to (..., nelec[i], 1) required eq_features Array array of shape (..., nelec, d) required spin_split ParticleSplit the spin split. required Returns: Type Description (SLArrayList) list of per-spin slog arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. Source code in vmcnet/models/antiequivariance.py def multiply_slog_antieq_by_eq_features ( split_slog_antieq : SLArrayList , eq_features : Array , spin_split : ParticleSplit , ) -> SLArrayList : \"\"\"Multiply equivariant input array with a spin-split slog-form antiequivariance. Args: split_slog_antieq (SLArrayList): SLArrayList containing nspins arrays of shape broadcastable to (..., nelec[i], 1) eq_features (Array): array of shape (..., nelec, d) spin_split (ParticleSplit): the spin split. Returns: (SLArrayList): list of per-spin slog arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. \"\"\" # Expand antiequivariance to shape (..., nelec[i], 1)] to broadcast with inputs split_slog_inputs = array_list_to_slog ( jnp . split ( eq_features , spin_split , axis =- 2 )) return jax . tree_map ( slog_multiply , split_slog_inputs , split_slog_antieq , is_leaf = is_tuple_of_arrays , )","title":"antiequivariance"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.OrbitalCofactorAntiequivarianceLayer","text":"Apply a cofactor antiequivariance multiplicatively to equivariant inputs. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a.","title":"OrbitalCofactorAntiequivarianceLayer"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.OrbitalCofactorAntiequivarianceLayer.__call__","text":"Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (ArrayList) per-spin list where each list entry is an array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (ArrayList): per-spin list where each list entry is an array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) ferminet_orbital_layer = FermiNetOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # Calculate orbital matrices as list of shape [(..., nelec[i], nelec[i])] orbital_matrix_list = ferminet_orbital_layer ( eq_inputs , r_ei ) # Calculate cofactors as list of shape [(..., nelec[i])] cofactors = jax . tree_map ( cofactor_antieq , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), cofactors )","title":"__call__()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.PerParticleDeterminantAntiequivarianceLayer","text":"Antieq. layer based on determinants of per-particle orbital matrices, slog out. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a.","title":"PerParticleDeterminantAntiequivarianceLayer"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.PerParticleDeterminantAntiequivarianceLayer.__call__","text":"Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (ArrayList) per-spin list where each list entry is an array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (ArrayList): per-spin list where each list entry is an array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) equivariant_orbital_layer = DoublyEquivariantOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # ArrayList of shape (..., nelec[i], nelec[i], nelec[i]) orbital_matrix_list = equivariant_orbital_layer ( eq_inputs , r_ei ) # ArrayList of nspins arrays of shape (..., nelec[i]) dets = jax . tree_map ( jnp . linalg . det , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), dets )","title":"__call__()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.SLogOrbitalCofactorAntiequivarianceLayer","text":"Apply a cofactor antieq. multiplicatively to equivariant inputs with slog out. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a.","title":"SLogOrbitalCofactorAntiequivarianceLayer"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.SLogOrbitalCofactorAntiequivarianceLayer.__call__","text":"Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (SLArrayList) per-spin list where each list entry is an slog array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> SLArrayList : \"\"\"Calculate the orbitals and the cofactor-based antiequivariance. For a single spin, if the orbital matrix is M, and the cofactor matrix of the orbital matrix is C, the ith output will be equal to M_(i,0) * C_(i,0) * (-1)**i. For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (SLArrayList): per-spin list where each list entry is an slog array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) ferminet_orbital_layer = FermiNetOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # Calculate orbital matrices as list of shape [(..., nelec[i], nelec[i])] orbital_matrix_list = ferminet_orbital_layer ( eq_inputs , r_ei ) # Calculate slog cofactors as list of shape [((..., nelec[i]), (..., nelec[i]))] slog_cofactors = jax . tree_map ( slog_cofactor_antieq , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), slog_cofactors )","title":"__call__()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.SLogPerParticleDeterminantAntiequivarianceLayer","text":"Antieq. layer based on determinants of per-particle orbital matrices, slog out. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a.","title":"SLogPerParticleDeterminantAntiequivarianceLayer"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.SLogPerParticleDeterminantAntiequivarianceLayer.__call__","text":"Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Parameters: Name Type Description Default eq_inputs ndarray (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. required r_ei Array array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. None Returns: Type Description (SLArrayList) per-spin list where each list entry is an slog array of shape (..., nelec, 1). Source code in vmcnet/models/antiequivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , eq_inputs : Array , r_ei : Array = None ) -> SLArrayList : \"\"\"Calculate the per-particle orbitals and the antiequivariant determinants. For a single spin, if the the orbital matrix for particle p is M_p, the output at index p will be equal to det(M_p). For multiple spins, each spin is handled separately in this same way. Args: eq_inputs: (Array): array of shape (..., nelec, d), which should contain values that are equivariant with respect to the particle positions. r_ei (Array, optional): array of shape (..., nelec, nion, d) representing electron-ion displacements, which if present will be used as an extra input to the orbital layer. Returns: (SLArrayList): per-spin list where each list entry is an slog array of shape (..., nelec, 1). \"\"\" nelec_total = eq_inputs . shape [ - 2 ] nelec_per_spin = get_nelec_per_split ( self . spin_split , nelec_total ) equivariant_orbital_layer = DoublyEquivariantOrbitalLayer ( self . spin_split , nelec_per_spin , self . kernel_initializer_orbital_linear , self . kernel_initializer_envelope_dim , self . kernel_initializer_envelope_ion , self . bias_initializer_orbital_linear , self . orbitals_use_bias , self . isotropic_decay , ) # ArrayList of shape (..., nelec[i], nelec[i], nelec[i]) orbital_matrix_list = equivariant_orbital_layer ( eq_inputs , r_ei ) # SLArrayList of nspins slog arrays of shape (..., nelec[i]) slog_dets = jax . tree_map ( jnp . linalg . slogdet , orbital_matrix_list ) return jax . tree_map ( lambda x : jnp . expand_dims ( x , - 1 ), slog_dets )","title":"__call__()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.get_submatrices_along_first_col","text":"Get the submatrices of x by deleting row i and col 0, for all rows of x. Parameters: Name Type Description Default x Array a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. required Returns: Type Description (int, Array) n, submatrices of shape (..., n, n-1, n-1), obtained by deleting row (..., i, :) and deleted column is (..., :, 0), for 0 <= i <= n - 1. Source code in vmcnet/models/antiequivariance.py def get_submatrices_along_first_col ( x : Array ) -> Tuple [ int , Array ]: \"\"\"Get the submatrices of x by deleting row i and col 0, for all rows of x. Args: x (Array): a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. Returns: (int, Array): n, submatrices of shape (..., n, n-1, n-1), obtained by deleting row (..., i, :) and deleted column is (..., :, 0), for 0 <= i <= n - 1. \"\"\" if len ( x . shape ) < 2 or x . shape [ - 1 ] != x . shape [ - 2 ]: msg = \"Calculating cofactors requires shape (..., n, n), got {} \" raise ValueError ( msg . format ( x . shape )) n = x . shape [ - 1 ] # Calculate minor_(0,i) by deleting the first orbital and ith particle indices submats = [ jnp . delete ( jnp . delete ( x , i , axis =- 2 ), 0 , axis =- 1 ) for i in range ( n )] # Stack on axis -3 to ensure shape (..., n) once det removes the last two axes stacked_submats = jnp . stack ( submats , axis =- 3 ) return n , stacked_submats","title":"get_submatrices_along_first_col()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.cofactor_antieq","text":"Compute a cofactor-based antiequivariance. Input must be square in the last two dimensions, of shape (..., n, n). The second last dimension is assumed to be the particle dimension, and the last is assumed to be the orbital dimension. The output will be of shape (..., n), preserving the particle dimension but getting rid of the orbital dimension. The transformation applies to each square matrix separately. Given an nxn matrix M, with cofactor matrix C, the output for that matrix will be a single vector of length n, whose ith component is M_(i,0) C_(i,0) (-1)**i. This is a single term in the cofactor expansion of the determinant of M. Thus, the sum of the returned vector values will always be equal to the determinant of M. This function implements an antiequivariant transformation, meaning that permuting two particle indices in the input will result in the output having 1) the same two particle indices permuted, and 2) ALL values multiplied by -1. Parameters: Name Type Description Default x Array a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. required Returns: Type Description (Array) array of shape (..., n), with the ith output (along the last axis) given by the ith term in the cofactor expansion of det(x) along the first entry of the last axis Source code in vmcnet/models/antiequivariance.py def cofactor_antieq ( x : Array ) -> Array : \"\"\"Compute a cofactor-based antiequivariance. Input must be square in the last two dimensions, of shape (..., n, n). The second last dimension is assumed to be the particle dimension, and the last is assumed to be the orbital dimension. The output will be of shape (..., n), preserving the particle dimension but getting rid of the orbital dimension. The transformation applies to each square matrix separately. Given an nxn matrix M, with cofactor matrix C, the output for that matrix will be a single vector of length n, whose ith component is M_(i,0)*C_(i,0)*(-1)**i. This is a single term in the cofactor expansion of the determinant of M. Thus, the sum of the returned vector values will always be equal to the determinant of M. This function implements an antiequivariant transformation, meaning that permuting two particle indices in the input will result in the output having 1) the same two particle indices permuted, and 2) ALL values multiplied by -1. Args: x (Array): a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. Returns: (Array): array of shape (..., n), with the ith output (along the last axis) given by the ith term in the cofactor expansion of det(x) along the first entry of the last axis \"\"\" first_orbital_vals = x [ ... , 0 ] n , stacked_submatrices = get_submatrices_along_first_col ( x ) cofactors = get_alternating_signs ( n ) * jnp . linalg . det ( stacked_submatrices ) return first_orbital_vals * cofactors","title":"cofactor_antieq()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.slog_cofactor_antieq","text":"Compute a cofactor-based antiequivariance, returning results in slogabs form. See :func: ~vmcnet.models.antiequivariance.cofactor_antieq . This function performs the same operations, but gives a result in the (sign, log) domain, going through a jnp.linalg.slogdet call instead of a jnp.linalg.det call. Parameters: Name Type Description Default x Array a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. required Returns: Type Description (Array, Array) tuple of arrays, each of shape (..., n). The first is sign(result), and the second is log(abs(result)). Source code in vmcnet/models/antiequivariance.py def slog_cofactor_antieq ( x : Array ) -> SLArray : \"\"\"Compute a cofactor-based antiequivariance, returning results in slogabs form. See :func:`~vmcnet.models.antiequivariance.cofactor_antieq`. This function performs the same operations, but gives a result in the (sign, log) domain, going through a jnp.linalg.slogdet call instead of a jnp.linalg.det call. Args: x (Array): a tensor of orbital matrices which is square in the last two dimensions, thus of shape (..., n, n). The second last dimension is the particle dimension, and the last is the orbital dimension. Returns: (Array, Array): tuple of arrays, each of shape (..., n). The first is sign(result), and the second is log(abs(result)). \"\"\" # Calculate x_(i, 0) by selecting orbital index 0 first_orbital_vals = x [ ... , 0 ] orbital_signs , orbital_logs = array_to_slog ( first_orbital_vals ) n , stacked_submatrices = get_submatrices_along_first_col ( x ) # TODO(ggoldsh): find a faster way to calculate these overlapping determinants. ( cofactor_signs , cofactor_logs ) = jnp . linalg . slogdet ( stacked_submatrices ) signs_and_logs = ( orbital_signs * cofactor_signs * get_alternating_signs ( n ), orbital_logs + cofactor_logs , ) return signs_and_logs","title":"slog_cofactor_antieq()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.multiply_antieq_by_eq_features","text":"Multiply equivariant input array with a spin-split antiequivariance. Parameters: Name Type Description Default split_antieq ArrayList list of arrays containing nspins arrays of shape broadcastable to (..., nelec[i], 1) required eq_features Array array of shape (..., nelec, d) required spin_split ParticleSplit the spin split. required Returns: Type Description (ArrayList) list of per-spin arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. Source code in vmcnet/models/antiequivariance.py def multiply_antieq_by_eq_features ( split_antieq : ArrayList , eq_features : Array , spin_split : ParticleSplit , ) -> ArrayList : \"\"\"Multiply equivariant input array with a spin-split antiequivariance. Args: split_antieq (ArrayList): list of arrays containing nspins arrays of shape broadcastable to (..., nelec[i], 1) eq_features (Array): array of shape (..., nelec, d) spin_split (ParticleSplit): the spin split. Returns: (ArrayList): list of per-spin arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. \"\"\" split_inputs = jnp . split ( eq_features , spin_split , axis =- 2 ) return tree_prod ( split_inputs , split_antieq )","title":"multiply_antieq_by_eq_features()"},{"location":"api/models/antiequivariance/#vmcnet.models.antiequivariance.multiply_slog_antieq_by_eq_features","text":"Multiply equivariant input array with a spin-split slog-form antiequivariance. Parameters: Name Type Description Default split_slog_antieq SLArrayList SLArrayList containing nspins arrays of shape broadcastable to (..., nelec[i], 1) required eq_features Array array of shape (..., nelec, d) required spin_split ParticleSplit the spin split. required Returns: Type Description (SLArrayList) list of per-spin slog arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. Source code in vmcnet/models/antiequivariance.py def multiply_slog_antieq_by_eq_features ( split_slog_antieq : SLArrayList , eq_features : Array , spin_split : ParticleSplit , ) -> SLArrayList : \"\"\"Multiply equivariant input array with a spin-split slog-form antiequivariance. Args: split_slog_antieq (SLArrayList): SLArrayList containing nspins arrays of shape broadcastable to (..., nelec[i], 1) eq_features (Array): array of shape (..., nelec, d) spin_split (ParticleSplit): the spin split. Returns: (SLArrayList): list of per-spin slog arrays of shape (..., nelec[i], d) which represent the product of the equivariant inputs with the antiequivariance. \"\"\" # Expand antiequivariance to shape (..., nelec[i], 1)] to broadcast with inputs split_slog_inputs = array_list_to_slog ( jnp . split ( eq_features , spin_split , axis =- 2 )) return jax . tree_map ( slog_multiply , split_slog_inputs , split_slog_antieq , is_leaf = is_tuple_of_arrays , )","title":"multiply_slog_antieq_by_eq_features()"},{"location":"api/models/antisymmetry/","text":"Antisymmetry parts to compose into a model. FactorizedAntisymmetrize ( Module ) dataclass Separately antisymmetrize fns over leaves of a pytree and return the product. See https://arxiv.org/abs/2112.03491 for a description of the factorized antisymmetric layer. Attributes: Name Type Description fns_to_antisymmetrize pytree pytree of functions with the same tree structure as the input pytree, each of which is a Callable with signature Array of shape (..., ninput_dim) -> Array of shape (..., dout). On the ith leaf, ninput_dim = n[i] * din[i], where n[i] is the size of the second-to-last axis and din[i] is the size of the last axis of the input xs. logabs bool whether to compute sum_i log(abs(psi_i)) if logabs is True, or prod_i psi_i if logabs is False, where psi_i is the output from antisymmetrizing the ith function on the ith input. Defaults to True. __call__ ( self , xs ) special Antisymmetrize the leaves of self.fns_to_antisymmetrize on the leaves of xs. Parameters: Name Type Description Default xs pytree pytree of inputs with the same tree structure as that of self.fns_to_antisymmetrize. The ith leaf has shape (..., n[i], d[i]), and the ith antisymmetrization happens with respect to n[i]. required Returns: Type Description Array or SLArray prod_i psi_i if self.logabs is False, or prod_i sign(psi_i), sum_i log(abs(psi_i)) if self.logabs is True, where psi_i is the output from antisymmetrizing the ith function on the ith input. Source code in vmcnet/models/antisymmetry.py @flax . linen . compact def __call__ ( self , xs : PyTree ) -> Union [ Array , SLArray ]: # type: ignore[override] \"\"\"Antisymmetrize the leaves of self.fns_to_antisymmetrize on the leaves of xs. Args: xs (pytree): pytree of inputs with the same tree structure as that of self.fns_to_antisymmetrize. The ith leaf has shape (..., n[i], d[i]), and the ith antisymmetrization happens with respect to n[i]. Returns: Array or SLArray: prod_i psi_i if self.logabs is False, or prod_i sign(psi_i), sum_i log(abs(psi_i)) if self.logabs is True, where psi_i is the output from antisymmetrizing the ith function on the ith input. \"\"\" # Flatten the trees for fns_to_antisymmetrize and xs, because Module # freezes all instances of lists to tuples, so this can cause treedef # compatibility problems antisyms = jax . tree_map ( self . _single_leaf_call , jax . tree_leaves ( self . fns_to_antisymmetrize ), jax . tree_leaves ( xs ), ) if not self . logabs : return _reduce_prod_over_leaves ( antisyms ) slog_antisyms = array_list_to_slog ( jax . tree_leaves ( antisyms )) return functools . reduce ( slog_multiply , slog_antisyms ) GenericAntisymmetrize ( Module ) dataclass Antisymmetrize a single function over the leaves of a pytree. See https://arxiv.org/abs/2112.03491 for a description of the generic antisymmetric layer. For each leaf of a pytree, a given function of all the leaves is antisymmetrized over the second-to-last axis of each leaf. These explicit antisymmetrization operations are composed with each other (they commute, so the order does not matter), giving an output which is antisymmetric with respect to particle exchange within each leaf but not with respect to particle exchange between leaves. Attributes: Name Type Description fn_to_antisymmetrize Callable Callable with signature Array with shape (..., ninput_dim) -> (..., 1). This is the function to be antisymmetrized. ninput_dim is equal to n[1] * d[1] + ... + n[k] * d[k], where n[i] is the size of the second-to-last axis and d[i] is the size of the last axis of the ith leaf of the input xs. logabs bool whether to compute log(abs(psi)) if logabs is True, or psi if logabs is False, where psi is the output from antisymmetrizing self.fn_to_antisymmetrize. Defaults to True. setup ( self ) Setup the function to antisymmetrize. Source code in vmcnet/models/antisymmetry.py def setup ( self ): \"\"\"Setup the function to antisymmetrize.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _fn_to_antisymmetrize = self . fn_to_antisymmetrize __call__ ( self , xs ) special Antisymmetrize self.fn_to_antisymmetrize over the leaves of xs. Parameters: Name Type Description Default xs pytree a pytree of inputs, each which corresponds to a different set of particles to antisymmetrize with respect to. The ith leaf has shape (..., n[i], d[i]), and the antisymmetrization happens with respect to n[i]. required Returns: Type Description Array psi if logabs is False, or sign(psi), log(abs(psi)) if self.logabs is True, where psi is the output from antisymmetrizing self.fn_to_antisymmetrize on all leaves of xs. Source code in vmcnet/models/antisymmetry.py @flax . linen . compact def __call__ ( self , xs : PyTree ) -> Union [ Array , SLArray ]: # type: ignore[override] \"\"\"Antisymmetrize self.fn_to_antisymmetrize over the leaves of xs. Args: xs (pytree): a pytree of inputs, each which corresponds to a different set of particles to antisymmetrize with respect to. The ith leaf has shape (..., n[i], d[i]), and the antisymmetrization happens with respect to n[i]. Returns: Array: psi if logabs is False, or sign(psi), log(abs(psi)) if self.logabs is True, where psi is the output from antisymmetrizing self.fn_to_antisymmetrize on all leaves of xs. \"\"\" perms_and_signs = jax . tree_map ( self . _get_single_leaf_perm , xs ) perms_and_signs_leaves , _ = jax . tree_flatten ( perms_and_signs , is_tuple_of_arrays ) nleaves = len ( perms_and_signs_leaves ) nperms_per_leaf = [ leaf [ 0 ] . shape [ - 3 ] for leaf in perms_and_signs_leaves ] broadcasted_perms = [] reshaped_signs = [] for i , ( leaf_perms , leaf_signs ) in enumerate ( perms_and_signs_leaves ): ith_factorial = ( 1 ,) * i + leaf_signs . shape [ 0 : 1 ] + ( 1 ,) * ( nleaves - i - 1 ) # desired sign[i] shape is (1, ..., n_i!, ..., 1, 1), with nspins + 1 dims sign_shape = ith_factorial + ( 1 ,) leaf_signs = jnp . reshape ( leaf_signs , sign_shape ) reshaped_signs . append ( leaf_signs ) # desired broadcasted x_i shape is [i: (..., n_1!, ..., n_k!, n_i * d_i)], # where k = nleaves, and x_i = (..., n_i, d_i). This is achieved by: # 1) reshape to (..., 1, ..., n_i!,... 1, n_i, d_i), then # 2) broadcast to (..., n_1!, ..., n_k!, n_i, d_i) # 3) flatten last axis to (..., n_1!, ..., n_k!, n_i * d_i) reshape_x_shape = ( leaf_perms . shape [: - 3 ] + ith_factorial + leaf_perms . shape [ - 2 :] ) broadcast_x_shape = ( leaf_perms . shape [: - 3 ] + tuple ( nperms_per_leaf ) + leaf_perms . shape [ - 2 :] ) leaf_perms = jnp . reshape ( leaf_perms , reshape_x_shape ) leaf_perms = jnp . broadcast_to ( leaf_perms , broadcast_x_shape ) flat_leaf_perms = jnp . reshape ( leaf_perms , leaf_perms . shape [: - 2 ] + ( - 1 ,)) broadcasted_perms . append ( flat_leaf_perms ) # make input shape (..., n_1!, ..., n_k!, n_1 * d_1 + ... + n_k * d_k) concat_perms = jnp . concatenate ( broadcasted_perms , axis =- 1 ) all_perms_out = self . _fn_to_antisymmetrize ( concat_perms ) # all_perms_out has shape (..., n_1!, ..., n_k!, 1) # Each leaf of reshaped_signs has k+1 axes, but all except the ith axis has size # 1. The ith axis has size n_i!. Thus when the leaves of reshaped_signs are # multiplied with all_perms_out, the product will broadcast each leaf and apply # the signs along the correct (ith) axis of the output. signed_perms_out = _reduce_prod_over_leaves ([ all_perms_out , reshaped_signs ]) antisymmetrized_out = jnp . sum ( signed_perms_out , axis = tuple ( - i for i in range ( 1 , nleaves + 2 )) ) if not self . logabs : return antisymmetrized_out return array_to_slog ( antisymmetrized_out ) ParallelPermutations ( Module ) dataclass Get all perms along the 2nd-to-last axis, w/ perms stored as a constant. If inputs are shape (..., n, d), then the outputs are shape (..., n!, n, d). The signs of the permutations are also returned. This layer is here so that the permutations and their signs are stored as a constant in the computational graph, instead of being recomputed at each iteration. This makes sense to do if there is enough memory to store all permutations of the input and any downstream computations, so that downstream computations can be done in parallel on all permutations. If there is not enough memory for this, then it is better to compute the permutations on the fly. Attributes: Name Type Description n int size of the second-to-last axis of the inputs. Should be >= 1. setup ( self ) Store the list of permutations and signs for the symmetric group. Source code in vmcnet/models/antisymmetry.py def setup ( self ): \"\"\"Store the list of permutations and signs for the symmetric group.\"\"\" self . permutation_list = jnp . array ( list ( itertools . permutations ( range ( self . n )))) self . signs = _get_lexicographic_signs ( self . n ) __call__ ( self , x ) special Collect all permutations of x and the signs of these permutations. Parameters: Name Type Description Default x Array an array of particles with shape (..., n, d) required Returns: Type Description (Array, Array) all permutations of x along the second axis, with shape (..., n!, n, d), and the signs of the permutations in the same order as the third-to-last axis (lexicographic order) Source code in vmcnet/models/antisymmetry.py def __call__ ( self , x : Array ) -> Tuple [ Array , Array ]: # type: ignore[override] \"\"\"Collect all permutations of x and the signs of these permutations. Args: x (Array): an array of particles with shape (..., n, d) Returns: (Array, Array): all permutations of x along the second axis, with shape (..., n!, n, d), and the signs of the permutations in the same order as the third-to-last axis (lexicographic order) \"\"\" return jnp . take ( x , self . permutation_list , axis =- 2 ), self . signs slogdet_product ( xs ) Compute the (sign, log) of the product of determinants of the leaves of a pytree. Parameters: Name Type Description Default xs pytree pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. required Returns: Type Description (Array, Array) the product of the sign_dets and the sum of the log_dets over all leaves of the pytree xs Source code in vmcnet/models/antisymmetry.py def slogdet_product ( xs : PyTree ) -> SLArray : \"\"\"Compute the (sign, log) of the product of determinants of the leaves of a pytree. Args: xs (pytree): pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. Returns: (Array, Array): the product of the sign_dets and the sum of the log_dets over all leaves of the pytree xs \"\"\" slogdets = jax . tree_map ( jnp . linalg . slogdet , xs ) slogdet_leaves , _ = jax . tree_flatten ( slogdets , is_tuple_of_arrays ) sign_prod , log_prod = functools . reduce ( lambda a , b : ( a [ 0 ] * b [ 0 ], a [ 1 ] + b [ 1 ]), slogdet_leaves ) return sign_prod , log_prod logdet_product ( xs ) Compute the log|prod_x det(x)| of the leaves x of a pytree (throwing away sign). Because we don't need to carry sign, the logic can be made slightly simpler and we can avoid a few computations. Parameters: Name Type Description Default xs pytree pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. required Returns: Type Description Array the sum of the log_dets over all leaves of the pytree xs, which is equal to the log of the product of the dets over all leaves of xs Source code in vmcnet/models/antisymmetry.py def logdet_product ( xs : PyTree ) -> Array : \"\"\"Compute the log|prod_x det(x)| of the leaves x of a pytree (throwing away sign). Because we don't need to carry sign, the logic can be made slightly simpler and we can avoid a few computations. Args: xs (pytree): pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. Returns: Array: the sum of the log_dets over all leaves of the pytree xs, which is equal to the log of the product of the dets over all leaves of xs \"\"\" logdets = jax . tree_map ( lambda x : jnp . linalg . slogdet ( x )[ 1 ], xs ) log_prod = _reduce_sum_over_leaves ( logdets ) return log_prod","title":"antisymmetry"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.FactorizedAntisymmetrize","text":"Separately antisymmetrize fns over leaves of a pytree and return the product. See https://arxiv.org/abs/2112.03491 for a description of the factorized antisymmetric layer. Attributes: Name Type Description fns_to_antisymmetrize pytree pytree of functions with the same tree structure as the input pytree, each of which is a Callable with signature Array of shape (..., ninput_dim) -> Array of shape (..., dout). On the ith leaf, ninput_dim = n[i] * din[i], where n[i] is the size of the second-to-last axis and din[i] is the size of the last axis of the input xs. logabs bool whether to compute sum_i log(abs(psi_i)) if logabs is True, or prod_i psi_i if logabs is False, where psi_i is the output from antisymmetrizing the ith function on the ith input. Defaults to True.","title":"FactorizedAntisymmetrize"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.FactorizedAntisymmetrize.__call__","text":"Antisymmetrize the leaves of self.fns_to_antisymmetrize on the leaves of xs. Parameters: Name Type Description Default xs pytree pytree of inputs with the same tree structure as that of self.fns_to_antisymmetrize. The ith leaf has shape (..., n[i], d[i]), and the ith antisymmetrization happens with respect to n[i]. required Returns: Type Description Array or SLArray prod_i psi_i if self.logabs is False, or prod_i sign(psi_i), sum_i log(abs(psi_i)) if self.logabs is True, where psi_i is the output from antisymmetrizing the ith function on the ith input. Source code in vmcnet/models/antisymmetry.py @flax . linen . compact def __call__ ( self , xs : PyTree ) -> Union [ Array , SLArray ]: # type: ignore[override] \"\"\"Antisymmetrize the leaves of self.fns_to_antisymmetrize on the leaves of xs. Args: xs (pytree): pytree of inputs with the same tree structure as that of self.fns_to_antisymmetrize. The ith leaf has shape (..., n[i], d[i]), and the ith antisymmetrization happens with respect to n[i]. Returns: Array or SLArray: prod_i psi_i if self.logabs is False, or prod_i sign(psi_i), sum_i log(abs(psi_i)) if self.logabs is True, where psi_i is the output from antisymmetrizing the ith function on the ith input. \"\"\" # Flatten the trees for fns_to_antisymmetrize and xs, because Module # freezes all instances of lists to tuples, so this can cause treedef # compatibility problems antisyms = jax . tree_map ( self . _single_leaf_call , jax . tree_leaves ( self . fns_to_antisymmetrize ), jax . tree_leaves ( xs ), ) if not self . logabs : return _reduce_prod_over_leaves ( antisyms ) slog_antisyms = array_list_to_slog ( jax . tree_leaves ( antisyms )) return functools . reduce ( slog_multiply , slog_antisyms )","title":"__call__()"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.GenericAntisymmetrize","text":"Antisymmetrize a single function over the leaves of a pytree. See https://arxiv.org/abs/2112.03491 for a description of the generic antisymmetric layer. For each leaf of a pytree, a given function of all the leaves is antisymmetrized over the second-to-last axis of each leaf. These explicit antisymmetrization operations are composed with each other (they commute, so the order does not matter), giving an output which is antisymmetric with respect to particle exchange within each leaf but not with respect to particle exchange between leaves. Attributes: Name Type Description fn_to_antisymmetrize Callable Callable with signature Array with shape (..., ninput_dim) -> (..., 1). This is the function to be antisymmetrized. ninput_dim is equal to n[1] * d[1] + ... + n[k] * d[k], where n[i] is the size of the second-to-last axis and d[i] is the size of the last axis of the ith leaf of the input xs. logabs bool whether to compute log(abs(psi)) if logabs is True, or psi if logabs is False, where psi is the output from antisymmetrizing self.fn_to_antisymmetrize. Defaults to True.","title":"GenericAntisymmetrize"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.GenericAntisymmetrize.setup","text":"Setup the function to antisymmetrize. Source code in vmcnet/models/antisymmetry.py def setup ( self ): \"\"\"Setup the function to antisymmetrize.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _fn_to_antisymmetrize = self . fn_to_antisymmetrize","title":"setup()"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.GenericAntisymmetrize.__call__","text":"Antisymmetrize self.fn_to_antisymmetrize over the leaves of xs. Parameters: Name Type Description Default xs pytree a pytree of inputs, each which corresponds to a different set of particles to antisymmetrize with respect to. The ith leaf has shape (..., n[i], d[i]), and the antisymmetrization happens with respect to n[i]. required Returns: Type Description Array psi if logabs is False, or sign(psi), log(abs(psi)) if self.logabs is True, where psi is the output from antisymmetrizing self.fn_to_antisymmetrize on all leaves of xs. Source code in vmcnet/models/antisymmetry.py @flax . linen . compact def __call__ ( self , xs : PyTree ) -> Union [ Array , SLArray ]: # type: ignore[override] \"\"\"Antisymmetrize self.fn_to_antisymmetrize over the leaves of xs. Args: xs (pytree): a pytree of inputs, each which corresponds to a different set of particles to antisymmetrize with respect to. The ith leaf has shape (..., n[i], d[i]), and the antisymmetrization happens with respect to n[i]. Returns: Array: psi if logabs is False, or sign(psi), log(abs(psi)) if self.logabs is True, where psi is the output from antisymmetrizing self.fn_to_antisymmetrize on all leaves of xs. \"\"\" perms_and_signs = jax . tree_map ( self . _get_single_leaf_perm , xs ) perms_and_signs_leaves , _ = jax . tree_flatten ( perms_and_signs , is_tuple_of_arrays ) nleaves = len ( perms_and_signs_leaves ) nperms_per_leaf = [ leaf [ 0 ] . shape [ - 3 ] for leaf in perms_and_signs_leaves ] broadcasted_perms = [] reshaped_signs = [] for i , ( leaf_perms , leaf_signs ) in enumerate ( perms_and_signs_leaves ): ith_factorial = ( 1 ,) * i + leaf_signs . shape [ 0 : 1 ] + ( 1 ,) * ( nleaves - i - 1 ) # desired sign[i] shape is (1, ..., n_i!, ..., 1, 1), with nspins + 1 dims sign_shape = ith_factorial + ( 1 ,) leaf_signs = jnp . reshape ( leaf_signs , sign_shape ) reshaped_signs . append ( leaf_signs ) # desired broadcasted x_i shape is [i: (..., n_1!, ..., n_k!, n_i * d_i)], # where k = nleaves, and x_i = (..., n_i, d_i). This is achieved by: # 1) reshape to (..., 1, ..., n_i!,... 1, n_i, d_i), then # 2) broadcast to (..., n_1!, ..., n_k!, n_i, d_i) # 3) flatten last axis to (..., n_1!, ..., n_k!, n_i * d_i) reshape_x_shape = ( leaf_perms . shape [: - 3 ] + ith_factorial + leaf_perms . shape [ - 2 :] ) broadcast_x_shape = ( leaf_perms . shape [: - 3 ] + tuple ( nperms_per_leaf ) + leaf_perms . shape [ - 2 :] ) leaf_perms = jnp . reshape ( leaf_perms , reshape_x_shape ) leaf_perms = jnp . broadcast_to ( leaf_perms , broadcast_x_shape ) flat_leaf_perms = jnp . reshape ( leaf_perms , leaf_perms . shape [: - 2 ] + ( - 1 ,)) broadcasted_perms . append ( flat_leaf_perms ) # make input shape (..., n_1!, ..., n_k!, n_1 * d_1 + ... + n_k * d_k) concat_perms = jnp . concatenate ( broadcasted_perms , axis =- 1 ) all_perms_out = self . _fn_to_antisymmetrize ( concat_perms ) # all_perms_out has shape (..., n_1!, ..., n_k!, 1) # Each leaf of reshaped_signs has k+1 axes, but all except the ith axis has size # 1. The ith axis has size n_i!. Thus when the leaves of reshaped_signs are # multiplied with all_perms_out, the product will broadcast each leaf and apply # the signs along the correct (ith) axis of the output. signed_perms_out = _reduce_prod_over_leaves ([ all_perms_out , reshaped_signs ]) antisymmetrized_out = jnp . sum ( signed_perms_out , axis = tuple ( - i for i in range ( 1 , nleaves + 2 )) ) if not self . logabs : return antisymmetrized_out return array_to_slog ( antisymmetrized_out )","title":"__call__()"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.ParallelPermutations","text":"Get all perms along the 2nd-to-last axis, w/ perms stored as a constant. If inputs are shape (..., n, d), then the outputs are shape (..., n!, n, d). The signs of the permutations are also returned. This layer is here so that the permutations and their signs are stored as a constant in the computational graph, instead of being recomputed at each iteration. This makes sense to do if there is enough memory to store all permutations of the input and any downstream computations, so that downstream computations can be done in parallel on all permutations. If there is not enough memory for this, then it is better to compute the permutations on the fly. Attributes: Name Type Description n int size of the second-to-last axis of the inputs. Should be >= 1.","title":"ParallelPermutations"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.ParallelPermutations.setup","text":"Store the list of permutations and signs for the symmetric group. Source code in vmcnet/models/antisymmetry.py def setup ( self ): \"\"\"Store the list of permutations and signs for the symmetric group.\"\"\" self . permutation_list = jnp . array ( list ( itertools . permutations ( range ( self . n )))) self . signs = _get_lexicographic_signs ( self . n )","title":"setup()"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.ParallelPermutations.__call__","text":"Collect all permutations of x and the signs of these permutations. Parameters: Name Type Description Default x Array an array of particles with shape (..., n, d) required Returns: Type Description (Array, Array) all permutations of x along the second axis, with shape (..., n!, n, d), and the signs of the permutations in the same order as the third-to-last axis (lexicographic order) Source code in vmcnet/models/antisymmetry.py def __call__ ( self , x : Array ) -> Tuple [ Array , Array ]: # type: ignore[override] \"\"\"Collect all permutations of x and the signs of these permutations. Args: x (Array): an array of particles with shape (..., n, d) Returns: (Array, Array): all permutations of x along the second axis, with shape (..., n!, n, d), and the signs of the permutations in the same order as the third-to-last axis (lexicographic order) \"\"\" return jnp . take ( x , self . permutation_list , axis =- 2 ), self . signs","title":"__call__()"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.slogdet_product","text":"Compute the (sign, log) of the product of determinants of the leaves of a pytree. Parameters: Name Type Description Default xs pytree pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. required Returns: Type Description (Array, Array) the product of the sign_dets and the sum of the log_dets over all leaves of the pytree xs Source code in vmcnet/models/antisymmetry.py def slogdet_product ( xs : PyTree ) -> SLArray : \"\"\"Compute the (sign, log) of the product of determinants of the leaves of a pytree. Args: xs (pytree): pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. Returns: (Array, Array): the product of the sign_dets and the sum of the log_dets over all leaves of the pytree xs \"\"\" slogdets = jax . tree_map ( jnp . linalg . slogdet , xs ) slogdet_leaves , _ = jax . tree_flatten ( slogdets , is_tuple_of_arrays ) sign_prod , log_prod = functools . reduce ( lambda a , b : ( a [ 0 ] * b [ 0 ], a [ 1 ] + b [ 1 ]), slogdet_leaves ) return sign_prod , log_prod","title":"slogdet_product()"},{"location":"api/models/antisymmetry/#vmcnet.models.antisymmetry.logdet_product","text":"Compute the log|prod_x det(x)| of the leaves x of a pytree (throwing away sign). Because we don't need to carry sign, the logic can be made slightly simpler and we can avoid a few computations. Parameters: Name Type Description Default xs pytree pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. required Returns: Type Description Array the sum of the log_dets over all leaves of the pytree xs, which is equal to the log of the product of the dets over all leaves of xs Source code in vmcnet/models/antisymmetry.py def logdet_product ( xs : PyTree ) -> Array : \"\"\"Compute the log|prod_x det(x)| of the leaves x of a pytree (throwing away sign). Because we don't need to carry sign, the logic can be made slightly simpler and we can avoid a few computations. Args: xs (pytree): pytree of tensors which are square in the last two dimensions, i.e. all leaves must have shape (..., n_leaf, n_leaf); the last two dims can be different from leaf to leaf, but the batch dimensions must be the same for all leaves. Returns: Array: the sum of the log_dets over all leaves of the pytree xs, which is equal to the log of the product of the dets over all leaves of xs \"\"\" logdets = jax . tree_map ( lambda x : jnp . linalg . slogdet ( x )[ 1 ], xs ) log_prod = _reduce_sum_over_leaves ( logdets ) return log_prod","title":"logdet_product()"},{"location":"api/models/construct/","text":"Combine pieces to form full models. AntiequivarianceNet ( Module ) dataclass Antisymmetry from anti-equivariance, backflow -> antieq -> odd invariance. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') antiequivariant_layer Callable function which computes antiequivariances-per- spin. Has the signature (stream_1e of shape (..., n, d_backflow), r_ei of shape (..., n, nion, d)) -> (antieqs of shapes [spin: (..., n[spin], d_antieq)]) array_list_sign_covariance Callable function which is sign- covariant with respect to each spin. Has the signature [(..., nelec[spin], d_antieq)] -> (..., d_antisym). Since this function is sign covariant, its outputs are antisymmetric, so Psi can be calculated by summing over the final axis of the result. multiply_by_eq_features bool If True, the antiequivariance from the antiequivariant_layer is multiplied by the equivariant features from the backflow before being fed into the sign covariant function. If False, the antiequivariance is processed directly by the sign covariant function. Defaults to False. setup ( self ) Setup backflow. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _antiequivariant_layer = self . antiequivariant_layer self . _array_list_sign_covariance = self . array_list_sign_covariance __call__ ( self , elec_pos ) special Compose backflow -> antiequivariance -> sign covariant equivariance -> sum. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array log(abs(psi)), where psi is a general odd invariance of an anti-equivariant backflow. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose backflow -> antiequivariance -> sign covariant equivariance -> sum. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: log(abs(psi)), where psi is a general odd invariance of an anti-equivariant backflow. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" stream_1e , stream_2e , r_ei , _ = self . _compute_input_streams ( elec_pos ) backflow_out = self . _backflow ( stream_1e , stream_2e ) antiequivariant_out = self . _antiequivariant_layer ( backflow_out , r_ei ) if self . multiply_by_eq_features : antiequivariant_out = antiequivariance . multiply_antieq_by_eq_features ( antiequivariant_out , backflow_out , self . spin_split ) antisym_vector = self . _array_list_sign_covariance ( antiequivariant_out ) return array_to_slog ( jnp . sum ( antisym_vector , axis =- 1 )) DeterminantFnMode ( Enum ) Enum specifying how to use determinant resnet in FermiNet model. EmbeddedParticleFermiNet ( FermiNet ) dataclass Model that expands its inputs with extra hidden particles, then applies FermiNet. Note: the backflow argument supplied for the construction of this model should use the spin_split for the TOTAL number of particles, visible and hidden, for each spin, not the standard spin_split for the visible particles only. Attributes: Name Type Description nhidden_fermions_per_spin Sequence[int] number of hidden fermions to generate for each spin. Must have length nspins. invariance_compute_input_streams ComputeInputStreams function to compute input streams from electron positions, for the invariance that is used to generate the hidden particle positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) invariance_backflow Callable backflow function to be used for the invariance which generates the hidden fermion positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') invariance_kernel_initializer WeightInitializer kernel initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array invariance_bias_initializer WeightInitializer bias initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array invariance_use_bias (bool, optional): whether to add a bias term in the dense layer of the invariance. Defaults to True. invariance_register_kfac bool whether to register the dense layer of the invariance with KFAC. Defaults to True. setup ( self ) Setup EmbeddedParticleFermiNet. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup EmbeddedParticleFermiNet.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 super () . setup () self . _invariance_compute_input_streams = self . invariance_compute_input_streams self . _invariance_backflow = self . invariance_backflow ExtendedOrbitalMatrixFermiNet ( FermiNet ) dataclass FermiNet-based model with larger orbital matrices via padding with invariance. Attributes: Name Type Description nhidden_fermions_per_spin Sequence[int] sequence of integers specifying how many extra hidden particle dimensions and corresponding virtual orbitals to add to the orbital matrices. If not None, must have length nspins. Defaults to None (no extra dims added, equivalent to FermiNet). invariance_kernel_initializer WeightInitializer kernel initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array. Defaults to an orthogonal initializer. invariance_bias_initializer WeightInitializer bias initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array. Defaults to a scaled random normal initializer. invariance_use_bias (bool, optional): whether to add a bias term in the dense layer of the invariance. Defaults to True. invariance_register_kfac bool whether to register the dense layer of the invariance with KFAC. Defaults to True. invariance_backflow Callable backflow function to be used for the invariance which generates the hidden fermion positions. If None, the outputs of the regular FermiNet backflow are used instead to form an invariance. Defaults to None. FactorizedAntisymmetry ( Module ) dataclass A sum of products of explicitly antisymmetrized ResNets, composed with backflow. This connects the computational graph between a backflow, a factorized antisymmetrized ResNet, and a jastrow. See https://arxiv.org/abs/2112.03491 for a description of the factorized antisymmetric layer. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') jastrow Callable function which computes a Jastrow factor from displacements. Has the signature ( r_ei of shape (batch_dims, n, nion, d), r_ee of shape (batch_dims, n, n, d), ) -> log jastrow of shape (batch_dims,) rank int The rank of the explicit antisymmetry. In practical terms, the number of resnets to antisymmetrize for each spin. This is analogous to ndeterminants for regular FermiNet. ndense_resnet int number of dense nodes in each layer of each antisymmetrized ResNet nlayers_resnet int number of layers in each antisymmetrized ResNet kernel_initializer_resnet WeightInitializer kernel initializer for the dense layers in the antisymmetrized ResNets. Has signature (key, shape, dtype) -> Array bias_initializer_resnet WeightInitializer bias initializer for the dense layers in the antisymmetrized ResNets. Has signature (key, shape, dtype) -> Array activation_fn_resnet Activation activation function in the antisymmetrized ResNets. Has the signature Array -> Array (shape is preserved) resnet_use_bias bool whether to add a bias term in the dense layers of the antisymmetrized ResNets. Defaults to True. setup ( self ) Setup backflow. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _jastrow = self . jastrow __call__ ( self , elec_pos ) special Compose FermiNet backflow -> antisymmetrized ResNets -> logabs product. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose FermiNet backflow -> antisymmetrized ResNets -> logabs product. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" input_stream_1e , input_stream_2e , r_ei , r_ee = self . _compute_input_streams ( elec_pos ) stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) split_spins = jnp . split ( stream_1e , self . spin_split , axis =- 2 ) def fn_to_antisymmetrize ( x_one_spin ): resnet_outputs = [ SimpleResNet ( self . ndense_resnet , 1 , self . nlayers_resnet , self . activation_fn_resnet , self . kernel_initializer_resnet , self . bias_initializer_resnet , use_bias = self . resnet_use_bias , )( x_one_spin ) for _ in range ( self . rank ) ] return jnp . concatenate ( resnet_outputs , axis =- 1 ) # TODO (ggoldsh/jeffminlin): better typing for the Array vs SLArray version of # this model to avoid having to cast the return type. slog_antisyms = cast ( SLArray , FactorizedAntisymmetrize ([ fn_to_antisymmetrize for _ in split_spins ])( split_spins ), ) sign_psi , log_antisyms = slog_sum_over_axis ( slog_antisyms , axis =- 1 ) jastrow_part = self . _jastrow ( input_stream_1e , input_stream_2e , stream_1e , r_ei , r_ee ) return sign_psi , log_antisyms + jastrow_part FermiNet ( Module ) dataclass FermiNet/generalized Slater determinant model. This model was first introduced in the following papers: https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.033429 https://arxiv.org/abs/2011.07125 Their repository can be found at https://github.com/deepmind/ferminet, which includes a JAX branch. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') ndeterminants int number of determinants in the FermiNet model, i.e. the number of distinct orbital layers applied kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term in the linear part of the orbitals. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. determinant_fn DeterminantFn or None A optional function with signature dout, [nspins: (..., ndeterminants)] -> (..., dout). If not None, the function will be used to calculate Psi based on the outputs of the orbital matrix determinants. Depending on the determinant_fn_mode selected, this function can be used in one of several ways. If the mode is SIGN_COVARIANCE, the function will use d=1 and will be explicitly symmetrized over the sign group, on a per-spin basis, to be sign-covariant (odd). If PARALLEL_EVEN or PAIRWISE_EVEN are selected, the function will be symmetrized to be spin-wise sign invariant (even). For PARALLEL_EVEN, the function will use d=ndeterminants, and each output will be multiplied by the product of corresponding determinants. That is, for 2 spins, with up determinants u_i and down determinants d_i, the ansatz will be sum_{i}(u_i * d_i * f_i(u,d)), where f_i(u,d) is the symmetrized determinant function. For PAIRWISE_EVEN, the function will use d=ndeterminants**nspins, and each output will again be multiplied by a product of determinants, but this time the determinants will range over all pairs. That is, for 2 spins, the ansatz will be sum_{i, j}(u_i * d_j * f_{i,j}(u,d)). Currently, PAIRWISE_EVEN mode only supports nspins = 2. If None, the equivalent of PARALLEL_EVEN mode (overriding any set determinant_fn_mode) is used without a symmetrized resnet (so the output, before any log-transformations, is a sum of products of determinants). determinant_fn_mode DeterminantFnMode One of SIGN_COVARIANCE, PARALLEL_EVEN, or PAIRWISE_EVEN. Used to decide how exactly to use the provided determinant_fn to calculate an ansatz for Psi; irrelevant if determinant_fn is set to None. full_det bool If True, the model will use a single, \"full\" determinant with orbitals from particles of all spins. For example, for a spin_split of (2,2), the original FermiNet with ndeterminants=1 would calculate two separate 2x2 orbital matrices and multiply their determinants together. A full determinant model would instead calculate a single 4x4 matrix, with the first two particle indices corresponding to the up-spin particles and the last two particle indices corresponding to the down-spin particles. The output of the model would then be the determinant of that single matrix, if ndeterminants=1, or the sum of multiple such determinants if ndeterminants>1. setup ( self ) Setup backflow and symmetrized determinant function. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow and symmetrized determinant function.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _symmetrized_det_fn = None if self . determinant_fn is not None : if self . determinant_fn_mode == DeterminantFnMode . SIGN_COVARIANCE : self . _symmetrized_det_fn = make_array_list_fn_sign_covariant ( functools . partial ( self . determinant_fn , 1 ) ) elif self . determinant_fn_mode == DeterminantFnMode . PARALLEL_EVEN : self . _symmetrized_det_fn = make_array_list_fn_sign_invariant ( functools . partial ( self . determinant_fn , self . ndeterminants ) ) elif self . determinant_fn_mode == DeterminantFnMode . PAIRWISE_EVEN : # TODO (ggoldsh): build support for PAIRWISE_EVEN for nspins != 2 self . _symmetrized_det_fn = make_array_list_fn_sign_invariant ( functools . partial ( self . determinant_fn , self . ndeterminants ** 2 ) ) else : raise self . _get_bad_determinant_fn_mode_error () __call__ ( self , elec_pos ) special Compose FermiNet backflow -> orbitals -> logabs determinant product. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array FermiNet output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose FermiNet backflow -> orbitals -> logabs determinant product. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: FermiNet output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" elec_pos , orbitals_split = self . _get_elec_pos_and_orbitals_split ( elec_pos ) input_stream_1e , input_stream_2e , r_ei , _ = self . _compute_input_streams ( elec_pos ) stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) norbitals_per_split = self . _get_norbitals_per_split ( elec_pos , orbitals_split ) # orbitals is [norb_splits: (ndeterminants, ..., nelec[i], norbitals[i])] orbitals = self . _eval_orbitals ( orbitals_split , norbitals_per_split , input_stream_1e , input_stream_2e , stream_1e , r_ei , ) if self . full_det : orbitals = [ jnp . concatenate ( orbitals , axis =- 2 )] if self . _symmetrized_det_fn is not None : # dets is ArrayList of shape [norb_splits: (ndeterminants, ...)] dets = jax . tree_map ( jnp . linalg . det , orbitals ) # Move axis to get shape [norb_splits: (..., ndeterminants)] fn_inputs = jax . tree_map ( lambda x : jnp . moveaxis ( x , 0 , - 1 ), dets ) if self . determinant_fn_mode == DeterminantFnMode . SIGN_COVARIANCE : psi = jnp . squeeze ( self . _symmetrized_det_fn ( fn_inputs ), - 1 ) elif self . determinant_fn_mode == DeterminantFnMode . PARALLEL_EVEN : psi = self . _calculate_psi_parallel_even ( fn_inputs ) elif self . determinant_fn_mode == DeterminantFnMode . PAIRWISE_EVEN : psi = self . _calculate_psi_pairwise_even ( fn_inputs ) else : raise self . _get_bad_determinant_fn_mode_error () return array_to_slog ( psi ) # slog_det_prods is SLArray of shape (ndeterminants, ...) slog_det_prods = slogdet_product ( orbitals ) return slog_sum_over_axis ( slog_det_prods ) GenericAntisymmetry ( Module ) dataclass A single ResNet antisymmetrized over all input leaves, composed with backflow. The ResNet is antisymmetrized with respect to each spin split separately (i.e. the antisymmetrization operators for each spin are composed and applied). This connects the computational graph between a backflow, a generic antisymmetrized ResNet, and a jastrow. See https://arxiv.org/abs/2112.03491 for a description of the generic antisymmetric layer. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') jastrow Callable function which computes a Jastrow factor from displacements. Has the signature ( r_ei of shape (batch_dims, n, nion, d), r_ee of shape (batch_dims, n, n, d), ) -> log jastrow of shape (batch_dims,) ndense_resnet int number of dense nodes in each layer of the ResNet nlayers_resnet int number of layers in each antisymmetrized ResNet kernel_initializer_resnet WeightInitializer kernel initializer for the dense layers in the antisymmetrized ResNet. Has signature (key, shape, dtype) -> Array bias_initializer_resnet WeightInitializer bias initializer for the dense layers in the antisymmetrized ResNet. Has signature (key, shape, dtype) -> Array activation_fn_resnet Activation activation function in the antisymmetrized ResNet. Has the signature Array -> Array (shape is preserved) resnet_use_bias bool whether to add a bias term in the dense layers of the antisymmetrized ResNet. Defaults to True. setup ( self ) Setup backflow. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _jastrow = self . jastrow self . _activation_fn_resnet = self . activation_fn_resnet __call__ ( self , elec_pos ) special Compose FermiNet backflow -> antisymmetrized ResNet -> logabs. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose FermiNet backflow -> antisymmetrized ResNet -> logabs. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" input_stream_1e , input_stream_2e , r_ei , r_ee = self . _compute_input_streams ( elec_pos ) stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) split_spins = jnp . split ( stream_1e , self . spin_split , axis =- 2 ) sign_psi , log_antisym = GenericAntisymmetrize ( SimpleResNet ( self . ndense_resnet , 1 , self . nlayers_resnet , self . _activation_fn_resnet , self . kernel_initializer_resnet , self . bias_initializer_resnet , use_bias = self . resnet_use_bias , ) )( split_spins ) jastrow_part = self . _jastrow ( input_stream_1e , input_stream_2e , stream_1e , r_ei , r_ee ) return sign_psi , log_antisym + jastrow_part slog_psi_to_log_psi_apply ( slog_psi_apply ) Get a log|psi| model apply callable from a sign(psi), log|psi| apply callable. Source code in vmcnet/models/construct.py def slog_psi_to_log_psi_apply ( slog_psi_apply : Callable [ ... , SLArray ] ) -> Callable [ ... , Array ]: \"\"\"Get a log|psi| model apply callable from a sign(psi), log|psi| apply callable.\"\"\" def log_psi_apply ( * args ) -> Array : return slog_psi_apply ( * args )[ 1 ] return log_psi_apply get_model_from_config ( model_config , nelec , ion_pos , ion_charges , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) Get a model from a hyperparameter config. Source code in vmcnet/models/construct.py def get_model_from_config ( model_config : ConfigDict , nelec : Array , ion_pos : Array , ion_charges : Array , dtype = jnp . float32 , ) -> Module : \"\"\"Get a model from a hyperparameter config.\"\"\" spin_split = get_spin_split ( nelec ) compute_input_streams = get_compute_input_streams_from_config ( model_config . input_streams , ion_pos ) backflow = get_backflow_from_config ( model_config . backflow , spin_split , dtype = dtype , ) kernel_init_constructor , bias_init_constructor = _get_dtype_init_constructors ( dtype ) ferminet_model_types = [ \"ferminet\" , \"embedded_particle_ferminet\" , \"extended_orbital_matrix_ferminet\" , ] if model_config . type in ferminet_model_types : determinant_fn = None resnet_config = model_config . det_resnet if model_config . use_det_resnet : determinant_fn = get_resnet_determinant_fn_for_ferminet ( resnet_config . ndense , resnet_config . nlayers , _get_named_activation_fn ( resnet_config . activation ), kernel_init_constructor ( resnet_config . kernel_init ), bias_init_constructor ( resnet_config . bias_init ), resnet_config . use_bias , resnet_config . register_kfac , ) # TODO(Jeffmin): make interface more flexible w.r.t. different types of Jastrows if model_config . type == \"ferminet\" : return FermiNet ( spin_split , compute_input_streams , backflow , model_config . ndeterminants , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , determinant_fn = determinant_fn , determinant_fn_mode = DeterminantFnMode [ resnet_config . mode . upper ()], full_det = model_config . full_det , ) elif model_config . type == \"embedded_particle_ferminet\" : total_nelec = jnp . array ( model_config . nhidden_fermions_per_spin ) + nelec total_spin_split = get_spin_split ( total_nelec ) backflow = get_backflow_from_config ( model_config . backflow , total_spin_split , dtype = dtype , ) invariance_config = model_config . invariance invariance_compute_input_streams = get_compute_input_streams_from_config ( invariance_config . input_streams , ion_pos ) invariance_backflow : Optional [ Module ] = get_backflow_from_config ( invariance_config . backflow , spin_split , dtype = dtype , ) return EmbeddedParticleFermiNet ( spin_split , compute_input_streams , backflow , model_config . ndeterminants , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , determinant_fn = determinant_fn , determinant_fn_mode = DeterminantFnMode [ resnet_config . mode . upper ()], full_det = model_config . full_det , nhidden_fermions_per_spin = model_config . nhidden_fermions_per_spin , invariance_compute_input_streams = invariance_compute_input_streams , invariance_backflow = invariance_backflow , invariance_kernel_initializer = kernel_init_constructor ( invariance_config . kernel_initializer ), invariance_bias_initializer = bias_init_constructor ( invariance_config . bias_initializer ), invariance_use_bias = invariance_config . use_bias , invariance_register_kfac = invariance_config . register_kfac , ) elif model_config . type == \"extended_orbital_matrix_ferminet\" : invariance_config = model_config . invariance if model_config . use_separate_invariance_backflow : invariance_backflow = get_backflow_from_config ( invariance_config . backflow , spin_split , dtype = dtype , ) else : invariance_backflow = None return ExtendedOrbitalMatrixFermiNet ( spin_split , compute_input_streams , backflow , model_config . ndeterminants , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , determinant_fn = determinant_fn , determinant_fn_mode = DeterminantFnMode [ resnet_config . mode . upper ()], full_det = model_config . full_det , nhidden_fermions_per_spin = model_config . nhidden_fermions_per_spin , invariance_backflow = invariance_backflow , invariance_kernel_initializer = kernel_init_constructor ( invariance_config . kernel_initializer ), invariance_bias_initializer = bias_init_constructor ( invariance_config . bias_initializer ), invariance_use_bias = invariance_config . use_bias , invariance_register_kfac = invariance_config . register_kfac , ) else : raise ValueError ( \"FermiNet model type {} requested, but the only supported \" \"types are: {} \" . format ( model_config . type , ferminet_model_types ) ) elif model_config . type in [ \"orbital_cofactor_net\" , \"per_particle_dets_net\" ]: if model_config . type == \"orbital_cofactor_net\" : antieq_layer : Callable [ [ Array , Array ], ArrayList ] = antiequivariance . OrbitalCofactorAntiequivarianceLayer ( spin_split , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , ) elif model_config . type == \"per_particle_dets_net\" : antieq_layer = antiequivariance . PerParticleDeterminantAntiequivarianceLayer ( spin_split , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , ) array_list_sign_covariance = get_sign_covariance_from_config ( model_config , spin_split , kernel_init_constructor , dtype ) return AntiequivarianceNet ( spin_split , compute_input_streams , backflow , antieq_layer , array_list_sign_covariance , multiply_by_eq_features = model_config . multiply_by_eq_features , ) elif model_config . type == \"explicit_antisym\" : jastrow_config = model_config . jastrow def _get_two_body_decay_jastrow (): return get_two_body_decay_scaled_for_chargeless_molecules ( ion_pos , ion_charges , init_ee_strength = jastrow_config . two_body_decay . init_ee_strength , trainable = jastrow_config . two_body_decay . trainable , ) def _get_backflow_based_jastrow (): if jastrow_config . backflow_based . use_separate_jastrow_backflow : jastrow_backflow = get_backflow_from_config ( jastrow_config . backflow_based . backflow , spin_split , dtype = dtype , ) else : jastrow_backflow = None return BackflowJastrow ( backflow = jastrow_backflow ) if jastrow_config . type == \"one_body_decay\" : jastrow : Jastrow = OneBodyExpDecay ( kernel_initializer = kernel_init_constructor ( jastrow_config . one_body_decay . kernel_init ) ) elif jastrow_config . type == \"two_body_decay\" : jastrow = _get_two_body_decay_jastrow () elif jastrow_config . type == \"backflow_based\" : jastrow = _get_backflow_based_jastrow () elif jastrow_config . type == \"two_body_decay_and_backflow_based\" : two_body_decay_jastrow = _get_two_body_decay_jastrow () backflow_jastrow = _get_backflow_based_jastrow () jastrow = AddedModel ([ two_body_decay_jastrow , backflow_jastrow ]) else : raise ValueError ( \"Unsupported jastrow type; {} was requested, but the only supported \" \"types are: {} \" . format ( jastrow_config . type , \", \" . join ( VALID_JASTROW_TYPES ) ) ) if model_config . antisym_type == \"factorized\" : return FactorizedAntisymmetry ( spin_split , compute_input_streams , backflow , jastrow , rank = model_config . rank , ndense_resnet = model_config . ndense_resnet , nlayers_resnet = model_config . nlayers_resnet , kernel_initializer_resnet = kernel_init_constructor ( model_config . kernel_init_resnet ), bias_initializer_resnet = bias_init_constructor ( model_config . bias_init_resnet ), activation_fn_resnet = _get_named_activation_fn ( model_config . activation_fn_resnet ), resnet_use_bias = model_config . resnet_use_bias , ) elif model_config . antisym_type == \"generic\" : return GenericAntisymmetry ( spin_split , compute_input_streams , backflow , jastrow , ndense_resnet = model_config . ndense_resnet , nlayers_resnet = model_config . nlayers_resnet , kernel_initializer_resnet = kernel_init_constructor ( model_config . kernel_init_resnet ), bias_initializer_resnet = bias_init_constructor ( model_config . bias_init_resnet ), activation_fn_resnet = _get_named_activation_fn ( model_config . activation_fn_resnet ), resnet_use_bias = model_config . resnet_use_bias , ) else : raise ValueError ( \"Unsupported explicit antisymmetry type; {} was requested\" . format ( model_config . antisym_type ) ) else : raise ValueError ( \"Unsupported model type; {} was requested\" . format ( model_config . type ) ) get_compute_input_streams_from_config ( input_streams_config , ion_pos = None ) Get a function for computing input streams from a model configuration. Source code in vmcnet/models/construct.py def get_compute_input_streams_from_config ( input_streams_config : ConfigDict , ion_pos : Optional [ Array ] = None ) -> ComputeInputStreams : \"\"\"Get a function for computing input streams from a model configuration.\"\"\" return functools . partial ( compute_input_streams , ion_pos = ion_pos , include_2e_stream = input_streams_config . include_2e_stream , include_ei_norm = input_streams_config . include_ei_norm , include_ee_norm = input_streams_config . include_ee_norm , ) get_backflow_from_config ( backflow_config , spin_split , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) Get a FermiNet backflow from a model configuration. Source code in vmcnet/models/construct.py def get_backflow_from_config ( backflow_config , spin_split , dtype = jnp . float32 , ) -> Module : \"\"\"Get a FermiNet backflow from a model configuration.\"\"\" kernel_init_constructor , bias_init_constructor = _get_dtype_init_constructors ( dtype ) residual_blocks = get_residual_blocks_for_ferminet_backflow ( spin_split , backflow_config . ndense_list , kernel_initializer_unmixed = kernel_init_constructor ( backflow_config . kernel_init_unmixed ), kernel_initializer_mixed = kernel_init_constructor ( backflow_config . kernel_init_mixed ), kernel_initializer_2e_1e_stream = kernel_init_constructor ( backflow_config . kernel_init_2e_1e_stream ), kernel_initializer_2e_2e_stream = kernel_init_constructor ( backflow_config . kernel_init_2e_2e_stream ), bias_initializer_1e_stream = bias_init_constructor ( backflow_config . bias_init_1e_stream ), bias_initializer_2e_stream = bias_init_constructor ( backflow_config . bias_init_2e_stream ), activation_fn = _get_named_activation_fn ( backflow_config . activation_fn ), use_bias = backflow_config . use_bias , one_electron_skip = backflow_config . one_electron_skip , one_electron_skip_scale = backflow_config . one_electron_skip_scale , two_electron_skip = backflow_config . two_electron_skip , two_electron_skip_scale = backflow_config . two_electron_skip_scale , cyclic_spins = backflow_config . cyclic_spins , ) return FermiNetBackflow ( residual_blocks ) get_sign_covariance_from_config ( model_config , spin_split , kernel_init_constructor , dtype ) Get a sign covariance from a model config, for use in AntiequivarianceNet. Source code in vmcnet/models/construct.py def get_sign_covariance_from_config ( model_config : ConfigDict , spin_split : ParticleSplit , kernel_init_constructor : Callable [[ ConfigDict ], WeightInitializer ], dtype : jnp . dtype , ) -> Callable [[ ArrayList ], Array ]: \"\"\"Get a sign covariance from a model config, for use in AntiequivarianceNet.\"\"\" if model_config . use_products_covariance : return ProductsSignCovariance ( 1 , kernel_init_constructor ( model_config . products_covariance . kernel_init ), model_config . products_covariance . register_kfac , use_weights = model_config . products_covariance . use_weights , ) else : def backflow_based_equivariance ( x : ArrayList ) -> Array : concat_x = jnp . concatenate ( x , axis =- 2 ) return get_backflow_from_config ( model_config . invariance , spin_split = spin_split , dtype = dtype , )( concat_x ) odd_equivariance = make_array_list_fn_sign_covariant ( backflow_based_equivariance , axis =- 3 ) return lambda x : jnp . sum ( odd_equivariance ( x ), axis =- 2 ) get_residual_blocks_for_ferminet_backflow ( spin_split , ndense_list , kernel_initializer_unmixed , kernel_initializer_mixed , kernel_initializer_2e_1e_stream , kernel_initializer_2e_2e_stream , bias_initializer_1e_stream , bias_initializer_2e_stream , activation_fn , use_bias = True , one_electron_skip = True , one_electron_skip_scale = 1.0 , two_electron_skip = True , two_electron_skip_scale = 1.0 , cyclic_spins = True ) Construct a list of FermiNet residual blocks composed by FermiNetBackflow. Parameters: Name Type Description Default spin_split int or Sequence[int] number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. required ndense_list List[Tuple[int, ...]] (list of (int, ...)): number of dense nodes in each of the residual blocks, (ndense_1e, optional ndense_2e). The length of this list determines the number of residual blocks which are composed on top of the input streams. If ndense_2e is specified, then then a dense layer is applied to the two-electron stream with an optional skip connection, otherwise the two-electron stream is mixed into the one-electron stream but no transformation is done. required kernel_initializer_unmixed WeightInitializer kernel initializer for the unmixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the previous one-electron stream output. Has signature (key, shape, dtype) -> Array required kernel_initializer_mixed WeightInitializer kernel initializer for the mixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous one-electron stream output. Has signature (key, shape, dtype) -> Array required kernel_initializer_2e_1e_stream WeightInitializer kernel initializer for the two-electron part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous two-electron stream which is mixed into the one-electron stream. Has signature (key, shape, dtype) -> Array required kernel_initializer_2e_2e_stream WeightInitializer kernel initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array required bias_initializer_1e_stream WeightInitializer bias initializer for the one-electron stream. Has signature (key, shape, dtype) -> Array required bias_initializer_2e_stream WeightInitializer bias initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array required activation_fn Activation activation function in the electron streams. Has the signature Array -> Array (shape is preserved) required use_bias bool whether to add a bias term in the electron streams. Defaults to True. True one_electron_skip bool whether to add a residual skip connection to the one-electron layer whenever the shapes of the input and output match. Defaults to True. True one_electron_skip_scale float quantity to scale the one-electron output by if a skip connection is added. Defaults to 1.0. 1.0 two_electron_skip bool whether to add a residual skip connection to the two-electron layer whenever the shapes of the input and output match. Defaults to True. True two_electron_skip_scale float quantity to scale the two-electron output by if a skip connection is added. Defaults to 1.0. 1.0 cyclic_spins bool whether the the concatenation in the one-electron stream should satisfy a cyclic equivariance structure, i.e. if there are three spins (1, 2, 3), then in the mixed part of the stream, after averaging but before the linear transformation, cyclic equivariance means the inputs are [(1, 2, 3), (2, 3, 1), (3, 1, 2)]. If False, then the inputs are [(1, 2, 3), (1, 2, 3), (1, 2, 3)] (as in the original FermiNet). When there are only two spins (spin-1/2 case), then this is equivalent to true spin equivariance. Defaults to False (original FermiNet). True Source code in vmcnet/models/construct.py def get_residual_blocks_for_ferminet_backflow ( spin_split : ParticleSplit , ndense_list : List [ Tuple [ int , ... ]], kernel_initializer_unmixed : WeightInitializer , kernel_initializer_mixed : WeightInitializer , kernel_initializer_2e_1e_stream : WeightInitializer , kernel_initializer_2e_2e_stream : WeightInitializer , bias_initializer_1e_stream : WeightInitializer , bias_initializer_2e_stream : WeightInitializer , activation_fn : Activation , use_bias : bool = True , one_electron_skip : bool = True , one_electron_skip_scale : float = 1.0 , two_electron_skip : bool = True , two_electron_skip_scale : float = 1.0 , cyclic_spins : bool = True , ) -> List [ FermiNetResidualBlock ]: \"\"\"Construct a list of FermiNet residual blocks composed by FermiNetBackflow. Arguments: spin_split (int or Sequence[int]): number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and `spin_split` = 2, then the input is split (5, 5). If nelec = 10, and `spin_split` = (2, 4), then the input is split into (2, 4, 4) -- note when `spin_split` is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, `spin_split` should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. ndense_list: (list of (int, ...)): number of dense nodes in each of the residual blocks, (ndense_1e, optional ndense_2e). The length of this list determines the number of residual blocks which are composed on top of the input streams. If ndense_2e is specified, then then a dense layer is applied to the two-electron stream with an optional skip connection, otherwise the two-electron stream is mixed into the one-electron stream but no transformation is done. kernel_initializer_unmixed (WeightInitializer): kernel initializer for the unmixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_mixed (WeightInitializer): kernel initializer for the mixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_2e_1e_stream (WeightInitializer): kernel initializer for the two-electron part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous two-electron stream which is mixed into the one-electron stream. Has signature (key, shape, dtype) -> Array kernel_initializer_2e_2e_stream (WeightInitializer): kernel initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array bias_initializer_1e_stream (WeightInitializer): bias initializer for the one-electron stream. Has signature (key, shape, dtype) -> Array bias_initializer_2e_stream (WeightInitializer): bias initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array activation_fn (Activation): activation function in the electron streams. Has the signature Array -> Array (shape is preserved) use_bias (bool, optional): whether to add a bias term in the electron streams. Defaults to True. one_electron_skip (bool, optional): whether to add a residual skip connection to the one-electron layer whenever the shapes of the input and output match. Defaults to True. one_electron_skip_scale (float, optional): quantity to scale the one-electron output by if a skip connection is added. Defaults to 1.0. two_electron_skip (bool, optional): whether to add a residual skip connection to the two-electron layer whenever the shapes of the input and output match. Defaults to True. two_electron_skip_scale (float, optional): quantity to scale the two-electron output by if a skip connection is added. Defaults to 1.0. cyclic_spins (bool, optional): whether the the concatenation in the one-electron stream should satisfy a cyclic equivariance structure, i.e. if there are three spins (1, 2, 3), then in the mixed part of the stream, after averaging but before the linear transformation, cyclic equivariance means the inputs are [(1, 2, 3), (2, 3, 1), (3, 1, 2)]. If False, then the inputs are [(1, 2, 3), (1, 2, 3), (1, 2, 3)] (as in the original FermiNet). When there are only two spins (spin-1/2 case), then this is equivalent to true spin equivariance. Defaults to False (original FermiNet). \"\"\" residual_blocks = [] for ndense in ndense_list : one_electron_layer = FermiNetOneElectronLayer ( spin_split , ndense [ 0 ], kernel_initializer_unmixed , kernel_initializer_mixed , kernel_initializer_2e_1e_stream , bias_initializer_1e_stream , activation_fn , use_bias , skip_connection = one_electron_skip , skip_connection_scale = one_electron_skip_scale , cyclic_spins = cyclic_spins , ) two_electron_layer = None if len ( ndense ) > 1 : two_electron_layer = FermiNetTwoElectronLayer ( ndense [ 1 ], kernel_initializer_2e_2e_stream , bias_initializer_2e_stream , activation_fn , use_bias , skip_connection = two_electron_skip , skip_connection_scale = two_electron_skip_scale , ) residual_blocks . append ( FermiNetResidualBlock ( one_electron_layer , two_electron_layer ) ) return residual_blocks get_resnet_determinant_fn_for_ferminet ( ndense , nlayers , activation , kernel_initializer , bias_initializer , use_bias = True , register_kfac = False ) Get a resnet-based determinant function for FermiNet construction. The returned function is used as a more general way to combine the determinant outputs into the final wavefunction value, relative to the original method of a sum of products. The function takes as its first argument the number of requested output features because several variants of this method are supported, and each requires the function to generate a different output size. Parameters: Name Type Description Default ndense int the number of neurons in the dense layers of the ResNet. required nlayers int the number of layers in the ResNet. required activation Activation the activation function to use for the resnet. required kernel_initializer WeightInitializer kernel initializer for the resnet. required bias_initializer WeightInitializer bias initializer for the resnet. required use_bias bool Whether to use a bias in the ResNet. Defaults to True. True register_kfac bool Whether to register the ResNet Dense layers with KFAC. Currently, params for this ResNet explode to huge values and cause nans if register_kfac is True, so this flag defaults to false and should only be overridden with care. The reason for the instability is not known. False Returns: Type Description (DeterminantFn) A resnet-based function. Has the signature dout, [nspins: (..., ndeterminants)] -> (..., dout). Source code in vmcnet/models/construct.py def get_resnet_determinant_fn_for_ferminet ( ndense : int , nlayers : int , activation : Activation , kernel_initializer : WeightInitializer , bias_initializer : WeightInitializer , use_bias : bool = True , register_kfac : bool = False , ) -> DeterminantFn : \"\"\"Get a resnet-based determinant function for FermiNet construction. The returned function is used as a more general way to combine the determinant outputs into the final wavefunction value, relative to the original method of a sum of products. The function takes as its first argument the number of requested output features because several variants of this method are supported, and each requires the function to generate a different output size. Args: ndense (int): the number of neurons in the dense layers of the ResNet. nlayers (int): the number of layers in the ResNet. activation (Activation): the activation function to use for the resnet. kernel_initializer (WeightInitializer): kernel initializer for the resnet. bias_initializer (WeightInitializer): bias initializer for the resnet. use_bias (bool): Whether to use a bias in the ResNet. Defaults to True. register_kfac (bool): Whether to register the ResNet Dense layers with KFAC. Currently, params for this ResNet explode to huge values and cause nans if register_kfac is True, so this flag defaults to false and should only be overridden with care. The reason for the instability is not known. Returns: (DeterminantFn): A resnet-based function. Has the signature dout, [nspins: (..., ndeterminants)] -> (..., dout). \"\"\" def fn ( dout : int , det_values : ArrayList ) -> Array : concat_values = jnp . concatenate ( det_values , axis =- 1 ) return SimpleResNet ( ndense , dout , nlayers , activation , kernel_initializer , bias_initializer , use_bias , register_kfac = register_kfac , )( concat_values ) return fn","title":"construct"},{"location":"api/models/construct/#vmcnet.models.construct.AntiequivarianceNet","text":"Antisymmetry from anti-equivariance, backflow -> antieq -> odd invariance. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') antiequivariant_layer Callable function which computes antiequivariances-per- spin. Has the signature (stream_1e of shape (..., n, d_backflow), r_ei of shape (..., n, nion, d)) -> (antieqs of shapes [spin: (..., n[spin], d_antieq)]) array_list_sign_covariance Callable function which is sign- covariant with respect to each spin. Has the signature [(..., nelec[spin], d_antieq)] -> (..., d_antisym). Since this function is sign covariant, its outputs are antisymmetric, so Psi can be calculated by summing over the final axis of the result. multiply_by_eq_features bool If True, the antiequivariance from the antiequivariant_layer is multiplied by the equivariant features from the backflow before being fed into the sign covariant function. If False, the antiequivariance is processed directly by the sign covariant function. Defaults to False.","title":"AntiequivarianceNet"},{"location":"api/models/construct/#vmcnet.models.construct.AntiequivarianceNet.setup","text":"Setup backflow. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _antiequivariant_layer = self . antiequivariant_layer self . _array_list_sign_covariance = self . array_list_sign_covariance","title":"setup()"},{"location":"api/models/construct/#vmcnet.models.construct.AntiequivarianceNet.__call__","text":"Compose backflow -> antiequivariance -> sign covariant equivariance -> sum. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array log(abs(psi)), where psi is a general odd invariance of an anti-equivariant backflow. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose backflow -> antiequivariance -> sign covariant equivariance -> sum. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: log(abs(psi)), where psi is a general odd invariance of an anti-equivariant backflow. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" stream_1e , stream_2e , r_ei , _ = self . _compute_input_streams ( elec_pos ) backflow_out = self . _backflow ( stream_1e , stream_2e ) antiequivariant_out = self . _antiequivariant_layer ( backflow_out , r_ei ) if self . multiply_by_eq_features : antiequivariant_out = antiequivariance . multiply_antieq_by_eq_features ( antiequivariant_out , backflow_out , self . spin_split ) antisym_vector = self . _array_list_sign_covariance ( antiequivariant_out ) return array_to_slog ( jnp . sum ( antisym_vector , axis =- 1 ))","title":"__call__()"},{"location":"api/models/construct/#vmcnet.models.construct.DeterminantFnMode","text":"Enum specifying how to use determinant resnet in FermiNet model.","title":"DeterminantFnMode"},{"location":"api/models/construct/#vmcnet.models.construct.EmbeddedParticleFermiNet","text":"Model that expands its inputs with extra hidden particles, then applies FermiNet. Note: the backflow argument supplied for the construction of this model should use the spin_split for the TOTAL number of particles, visible and hidden, for each spin, not the standard spin_split for the visible particles only. Attributes: Name Type Description nhidden_fermions_per_spin Sequence[int] number of hidden fermions to generate for each spin. Must have length nspins. invariance_compute_input_streams ComputeInputStreams function to compute input streams from electron positions, for the invariance that is used to generate the hidden particle positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) invariance_backflow Callable backflow function to be used for the invariance which generates the hidden fermion positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') invariance_kernel_initializer WeightInitializer kernel initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array invariance_bias_initializer WeightInitializer bias initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array invariance_use_bias (bool, optional): whether to add a bias term in the dense layer of the invariance. Defaults to True. invariance_register_kfac bool whether to register the dense layer of the invariance with KFAC. Defaults to True.","title":"EmbeddedParticleFermiNet"},{"location":"api/models/construct/#vmcnet.models.construct.EmbeddedParticleFermiNet.setup","text":"Setup EmbeddedParticleFermiNet. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup EmbeddedParticleFermiNet.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 super () . setup () self . _invariance_compute_input_streams = self . invariance_compute_input_streams self . _invariance_backflow = self . invariance_backflow","title":"setup()"},{"location":"api/models/construct/#vmcnet.models.construct.ExtendedOrbitalMatrixFermiNet","text":"FermiNet-based model with larger orbital matrices via padding with invariance. Attributes: Name Type Description nhidden_fermions_per_spin Sequence[int] sequence of integers specifying how many extra hidden particle dimensions and corresponding virtual orbitals to add to the orbital matrices. If not None, must have length nspins. Defaults to None (no extra dims added, equivalent to FermiNet). invariance_kernel_initializer WeightInitializer kernel initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array. Defaults to an orthogonal initializer. invariance_bias_initializer WeightInitializer bias initializer for the invariance dense layer. Has signature (key, shape, dtype) -> Array. Defaults to a scaled random normal initializer. invariance_use_bias (bool, optional): whether to add a bias term in the dense layer of the invariance. Defaults to True. invariance_register_kfac bool whether to register the dense layer of the invariance with KFAC. Defaults to True. invariance_backflow Callable backflow function to be used for the invariance which generates the hidden fermion positions. If None, the outputs of the regular FermiNet backflow are used instead to form an invariance. Defaults to None.","title":"ExtendedOrbitalMatrixFermiNet"},{"location":"api/models/construct/#vmcnet.models.construct.FactorizedAntisymmetry","text":"A sum of products of explicitly antisymmetrized ResNets, composed with backflow. This connects the computational graph between a backflow, a factorized antisymmetrized ResNet, and a jastrow. See https://arxiv.org/abs/2112.03491 for a description of the factorized antisymmetric layer. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') jastrow Callable function which computes a Jastrow factor from displacements. Has the signature ( r_ei of shape (batch_dims, n, nion, d), r_ee of shape (batch_dims, n, n, d), ) -> log jastrow of shape (batch_dims,) rank int The rank of the explicit antisymmetry. In practical terms, the number of resnets to antisymmetrize for each spin. This is analogous to ndeterminants for regular FermiNet. ndense_resnet int number of dense nodes in each layer of each antisymmetrized ResNet nlayers_resnet int number of layers in each antisymmetrized ResNet kernel_initializer_resnet WeightInitializer kernel initializer for the dense layers in the antisymmetrized ResNets. Has signature (key, shape, dtype) -> Array bias_initializer_resnet WeightInitializer bias initializer for the dense layers in the antisymmetrized ResNets. Has signature (key, shape, dtype) -> Array activation_fn_resnet Activation activation function in the antisymmetrized ResNets. Has the signature Array -> Array (shape is preserved) resnet_use_bias bool whether to add a bias term in the dense layers of the antisymmetrized ResNets. Defaults to True.","title":"FactorizedAntisymmetry"},{"location":"api/models/construct/#vmcnet.models.construct.FactorizedAntisymmetry.setup","text":"Setup backflow. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _jastrow = self . jastrow","title":"setup()"},{"location":"api/models/construct/#vmcnet.models.construct.FactorizedAntisymmetry.__call__","text":"Compose FermiNet backflow -> antisymmetrized ResNets -> logabs product. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose FermiNet backflow -> antisymmetrized ResNets -> logabs product. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" input_stream_1e , input_stream_2e , r_ei , r_ee = self . _compute_input_streams ( elec_pos ) stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) split_spins = jnp . split ( stream_1e , self . spin_split , axis =- 2 ) def fn_to_antisymmetrize ( x_one_spin ): resnet_outputs = [ SimpleResNet ( self . ndense_resnet , 1 , self . nlayers_resnet , self . activation_fn_resnet , self . kernel_initializer_resnet , self . bias_initializer_resnet , use_bias = self . resnet_use_bias , )( x_one_spin ) for _ in range ( self . rank ) ] return jnp . concatenate ( resnet_outputs , axis =- 1 ) # TODO (ggoldsh/jeffminlin): better typing for the Array vs SLArray version of # this model to avoid having to cast the return type. slog_antisyms = cast ( SLArray , FactorizedAntisymmetrize ([ fn_to_antisymmetrize for _ in split_spins ])( split_spins ), ) sign_psi , log_antisyms = slog_sum_over_axis ( slog_antisyms , axis =- 1 ) jastrow_part = self . _jastrow ( input_stream_1e , input_stream_2e , stream_1e , r_ei , r_ee ) return sign_psi , log_antisyms + jastrow_part","title":"__call__()"},{"location":"api/models/construct/#vmcnet.models.construct.FermiNet","text":"FermiNet/generalized Slater determinant model. This model was first introduced in the following papers: https://journals.aps.org/prresearch/abstract/10.1103/PhysRevResearch.2.033429 https://arxiv.org/abs/2011.07125 Their repository can be found at https://github.com/deepmind/ferminet, which includes a JAX branch. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') ndeterminants int number of determinants in the FermiNet model, i.e. the number of distinct orbital layers applied kernel_initializer_orbital_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_orbital_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array orbitals_use_bias bool whether to add a bias term in the linear part of the orbitals. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. determinant_fn DeterminantFn or None A optional function with signature dout, [nspins: (..., ndeterminants)] -> (..., dout). If not None, the function will be used to calculate Psi based on the outputs of the orbital matrix determinants. Depending on the determinant_fn_mode selected, this function can be used in one of several ways. If the mode is SIGN_COVARIANCE, the function will use d=1 and will be explicitly symmetrized over the sign group, on a per-spin basis, to be sign-covariant (odd). If PARALLEL_EVEN or PAIRWISE_EVEN are selected, the function will be symmetrized to be spin-wise sign invariant (even). For PARALLEL_EVEN, the function will use d=ndeterminants, and each output will be multiplied by the product of corresponding determinants. That is, for 2 spins, with up determinants u_i and down determinants d_i, the ansatz will be sum_{i}(u_i * d_i * f_i(u,d)), where f_i(u,d) is the symmetrized determinant function. For PAIRWISE_EVEN, the function will use d=ndeterminants**nspins, and each output will again be multiplied by a product of determinants, but this time the determinants will range over all pairs. That is, for 2 spins, the ansatz will be sum_{i, j}(u_i * d_j * f_{i,j}(u,d)). Currently, PAIRWISE_EVEN mode only supports nspins = 2. If None, the equivalent of PARALLEL_EVEN mode (overriding any set determinant_fn_mode) is used without a symmetrized resnet (so the output, before any log-transformations, is a sum of products of determinants). determinant_fn_mode DeterminantFnMode One of SIGN_COVARIANCE, PARALLEL_EVEN, or PAIRWISE_EVEN. Used to decide how exactly to use the provided determinant_fn to calculate an ansatz for Psi; irrelevant if determinant_fn is set to None. full_det bool If True, the model will use a single, \"full\" determinant with orbitals from particles of all spins. For example, for a spin_split of (2,2), the original FermiNet with ndeterminants=1 would calculate two separate 2x2 orbital matrices and multiply their determinants together. A full determinant model would instead calculate a single 4x4 matrix, with the first two particle indices corresponding to the up-spin particles and the last two particle indices corresponding to the down-spin particles. The output of the model would then be the determinant of that single matrix, if ndeterminants=1, or the sum of multiple such determinants if ndeterminants>1.","title":"FermiNet"},{"location":"api/models/construct/#vmcnet.models.construct.FermiNet.setup","text":"Setup backflow and symmetrized determinant function. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow and symmetrized determinant function.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _symmetrized_det_fn = None if self . determinant_fn is not None : if self . determinant_fn_mode == DeterminantFnMode . SIGN_COVARIANCE : self . _symmetrized_det_fn = make_array_list_fn_sign_covariant ( functools . partial ( self . determinant_fn , 1 ) ) elif self . determinant_fn_mode == DeterminantFnMode . PARALLEL_EVEN : self . _symmetrized_det_fn = make_array_list_fn_sign_invariant ( functools . partial ( self . determinant_fn , self . ndeterminants ) ) elif self . determinant_fn_mode == DeterminantFnMode . PAIRWISE_EVEN : # TODO (ggoldsh): build support for PAIRWISE_EVEN for nspins != 2 self . _symmetrized_det_fn = make_array_list_fn_sign_invariant ( functools . partial ( self . determinant_fn , self . ndeterminants ** 2 ) ) else : raise self . _get_bad_determinant_fn_mode_error ()","title":"setup()"},{"location":"api/models/construct/#vmcnet.models.construct.FermiNet.__call__","text":"Compose FermiNet backflow -> orbitals -> logabs determinant product. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array FermiNet output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose FermiNet backflow -> orbitals -> logabs determinant product. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: FermiNet output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" elec_pos , orbitals_split = self . _get_elec_pos_and_orbitals_split ( elec_pos ) input_stream_1e , input_stream_2e , r_ei , _ = self . _compute_input_streams ( elec_pos ) stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) norbitals_per_split = self . _get_norbitals_per_split ( elec_pos , orbitals_split ) # orbitals is [norb_splits: (ndeterminants, ..., nelec[i], norbitals[i])] orbitals = self . _eval_orbitals ( orbitals_split , norbitals_per_split , input_stream_1e , input_stream_2e , stream_1e , r_ei , ) if self . full_det : orbitals = [ jnp . concatenate ( orbitals , axis =- 2 )] if self . _symmetrized_det_fn is not None : # dets is ArrayList of shape [norb_splits: (ndeterminants, ...)] dets = jax . tree_map ( jnp . linalg . det , orbitals ) # Move axis to get shape [norb_splits: (..., ndeterminants)] fn_inputs = jax . tree_map ( lambda x : jnp . moveaxis ( x , 0 , - 1 ), dets ) if self . determinant_fn_mode == DeterminantFnMode . SIGN_COVARIANCE : psi = jnp . squeeze ( self . _symmetrized_det_fn ( fn_inputs ), - 1 ) elif self . determinant_fn_mode == DeterminantFnMode . PARALLEL_EVEN : psi = self . _calculate_psi_parallel_even ( fn_inputs ) elif self . determinant_fn_mode == DeterminantFnMode . PAIRWISE_EVEN : psi = self . _calculate_psi_pairwise_even ( fn_inputs ) else : raise self . _get_bad_determinant_fn_mode_error () return array_to_slog ( psi ) # slog_det_prods is SLArray of shape (ndeterminants, ...) slog_det_prods = slogdet_product ( orbitals ) return slog_sum_over_axis ( slog_det_prods )","title":"__call__()"},{"location":"api/models/construct/#vmcnet.models.construct.GenericAntisymmetry","text":"A single ResNet antisymmetrized over all input leaves, composed with backflow. The ResNet is antisymmetrized with respect to each spin split separately (i.e. the antisymmetrization operators for each spin are composed and applied). This connects the computational graph between a backflow, a generic antisymmetrized ResNet, and a jastrow. See https://arxiv.org/abs/2112.03491 for a description of the generic antisymmetric layer. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. compute_input_streams ComputeInputStreams function to compute input streams from electron positions. Has the signature (elec_pos of shape (..., n, d)) -> ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), optional r_ei of shape (..., n, nion, d), optional r_ee of shape (..., n, n, d), ) backflow Callable function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d') jastrow Callable function which computes a Jastrow factor from displacements. Has the signature ( r_ei of shape (batch_dims, n, nion, d), r_ee of shape (batch_dims, n, n, d), ) -> log jastrow of shape (batch_dims,) ndense_resnet int number of dense nodes in each layer of the ResNet nlayers_resnet int number of layers in each antisymmetrized ResNet kernel_initializer_resnet WeightInitializer kernel initializer for the dense layers in the antisymmetrized ResNet. Has signature (key, shape, dtype) -> Array bias_initializer_resnet WeightInitializer bias initializer for the dense layers in the antisymmetrized ResNet. Has signature (key, shape, dtype) -> Array activation_fn_resnet Activation activation function in the antisymmetrized ResNet. Has the signature Array -> Array (shape is preserved) resnet_use_bias bool whether to add a bias term in the dense layers of the antisymmetrized ResNet. Defaults to True.","title":"GenericAntisymmetry"},{"location":"api/models/construct/#vmcnet.models.construct.GenericAntisymmetry.setup","text":"Setup backflow. Source code in vmcnet/models/construct.py def setup ( self ): \"\"\"Setup backflow.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _compute_input_streams = self . compute_input_streams self . _backflow = self . backflow self . _jastrow = self . jastrow self . _activation_fn_resnet = self . activation_fn_resnet","title":"setup()"},{"location":"api/models/construct/#vmcnet.models.construct.GenericAntisymmetry.__call__","text":"Compose FermiNet backflow -> antisymmetrized ResNet -> logabs. Parameters: Name Type Description Default elec_pos Array array of particle positions (..., nelec, d) required Returns: Type Description Array spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). Source code in vmcnet/models/construct.py @flax . linen . compact def __call__ ( self , elec_pos : Array ) -> SLArray : # type: ignore[override] \"\"\"Compose FermiNet backflow -> antisymmetrized ResNet -> logabs. Args: elec_pos (Array): array of particle positions (..., nelec, d) Returns: Array: spinful antisymmetrized output; logarithm of the absolute value of a anti-symmetric function of elec_pos, where the anti-symmetry is with respect to the second-to-last axis of elec_pos. The anti-symmetry holds for particles within the same split, but not for permutations which swap particles across different spin splits. If the inputs have shape (batch_dims, nelec, d), then the output has shape (batch_dims,). \"\"\" input_stream_1e , input_stream_2e , r_ei , r_ee = self . _compute_input_streams ( elec_pos ) stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) split_spins = jnp . split ( stream_1e , self . spin_split , axis =- 2 ) sign_psi , log_antisym = GenericAntisymmetrize ( SimpleResNet ( self . ndense_resnet , 1 , self . nlayers_resnet , self . _activation_fn_resnet , self . kernel_initializer_resnet , self . bias_initializer_resnet , use_bias = self . resnet_use_bias , ) )( split_spins ) jastrow_part = self . _jastrow ( input_stream_1e , input_stream_2e , stream_1e , r_ei , r_ee ) return sign_psi , log_antisym + jastrow_part","title":"__call__()"},{"location":"api/models/construct/#vmcnet.models.construct.slog_psi_to_log_psi_apply","text":"Get a log|psi| model apply callable from a sign(psi), log|psi| apply callable. Source code in vmcnet/models/construct.py def slog_psi_to_log_psi_apply ( slog_psi_apply : Callable [ ... , SLArray ] ) -> Callable [ ... , Array ]: \"\"\"Get a log|psi| model apply callable from a sign(psi), log|psi| apply callable.\"\"\" def log_psi_apply ( * args ) -> Array : return slog_psi_apply ( * args )[ 1 ] return log_psi_apply","title":"slog_psi_to_log_psi_apply()"},{"location":"api/models/construct/#vmcnet.models.construct.get_model_from_config","text":"Get a model from a hyperparameter config. Source code in vmcnet/models/construct.py def get_model_from_config ( model_config : ConfigDict , nelec : Array , ion_pos : Array , ion_charges : Array , dtype = jnp . float32 , ) -> Module : \"\"\"Get a model from a hyperparameter config.\"\"\" spin_split = get_spin_split ( nelec ) compute_input_streams = get_compute_input_streams_from_config ( model_config . input_streams , ion_pos ) backflow = get_backflow_from_config ( model_config . backflow , spin_split , dtype = dtype , ) kernel_init_constructor , bias_init_constructor = _get_dtype_init_constructors ( dtype ) ferminet_model_types = [ \"ferminet\" , \"embedded_particle_ferminet\" , \"extended_orbital_matrix_ferminet\" , ] if model_config . type in ferminet_model_types : determinant_fn = None resnet_config = model_config . det_resnet if model_config . use_det_resnet : determinant_fn = get_resnet_determinant_fn_for_ferminet ( resnet_config . ndense , resnet_config . nlayers , _get_named_activation_fn ( resnet_config . activation ), kernel_init_constructor ( resnet_config . kernel_init ), bias_init_constructor ( resnet_config . bias_init ), resnet_config . use_bias , resnet_config . register_kfac , ) # TODO(Jeffmin): make interface more flexible w.r.t. different types of Jastrows if model_config . type == \"ferminet\" : return FermiNet ( spin_split , compute_input_streams , backflow , model_config . ndeterminants , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , determinant_fn = determinant_fn , determinant_fn_mode = DeterminantFnMode [ resnet_config . mode . upper ()], full_det = model_config . full_det , ) elif model_config . type == \"embedded_particle_ferminet\" : total_nelec = jnp . array ( model_config . nhidden_fermions_per_spin ) + nelec total_spin_split = get_spin_split ( total_nelec ) backflow = get_backflow_from_config ( model_config . backflow , total_spin_split , dtype = dtype , ) invariance_config = model_config . invariance invariance_compute_input_streams = get_compute_input_streams_from_config ( invariance_config . input_streams , ion_pos ) invariance_backflow : Optional [ Module ] = get_backflow_from_config ( invariance_config . backflow , spin_split , dtype = dtype , ) return EmbeddedParticleFermiNet ( spin_split , compute_input_streams , backflow , model_config . ndeterminants , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , determinant_fn = determinant_fn , determinant_fn_mode = DeterminantFnMode [ resnet_config . mode . upper ()], full_det = model_config . full_det , nhidden_fermions_per_spin = model_config . nhidden_fermions_per_spin , invariance_compute_input_streams = invariance_compute_input_streams , invariance_backflow = invariance_backflow , invariance_kernel_initializer = kernel_init_constructor ( invariance_config . kernel_initializer ), invariance_bias_initializer = bias_init_constructor ( invariance_config . bias_initializer ), invariance_use_bias = invariance_config . use_bias , invariance_register_kfac = invariance_config . register_kfac , ) elif model_config . type == \"extended_orbital_matrix_ferminet\" : invariance_config = model_config . invariance if model_config . use_separate_invariance_backflow : invariance_backflow = get_backflow_from_config ( invariance_config . backflow , spin_split , dtype = dtype , ) else : invariance_backflow = None return ExtendedOrbitalMatrixFermiNet ( spin_split , compute_input_streams , backflow , model_config . ndeterminants , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , determinant_fn = determinant_fn , determinant_fn_mode = DeterminantFnMode [ resnet_config . mode . upper ()], full_det = model_config . full_det , nhidden_fermions_per_spin = model_config . nhidden_fermions_per_spin , invariance_backflow = invariance_backflow , invariance_kernel_initializer = kernel_init_constructor ( invariance_config . kernel_initializer ), invariance_bias_initializer = bias_init_constructor ( invariance_config . bias_initializer ), invariance_use_bias = invariance_config . use_bias , invariance_register_kfac = invariance_config . register_kfac , ) else : raise ValueError ( \"FermiNet model type {} requested, but the only supported \" \"types are: {} \" . format ( model_config . type , ferminet_model_types ) ) elif model_config . type in [ \"orbital_cofactor_net\" , \"per_particle_dets_net\" ]: if model_config . type == \"orbital_cofactor_net\" : antieq_layer : Callable [ [ Array , Array ], ArrayList ] = antiequivariance . OrbitalCofactorAntiequivarianceLayer ( spin_split , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , ) elif model_config . type == \"per_particle_dets_net\" : antieq_layer = antiequivariance . PerParticleDeterminantAntiequivarianceLayer ( spin_split , kernel_initializer_orbital_linear = kernel_init_constructor ( model_config . kernel_init_orbital_linear ), kernel_initializer_envelope_dim = kernel_init_constructor ( model_config . kernel_init_envelope_dim ), kernel_initializer_envelope_ion = kernel_init_constructor ( model_config . kernel_init_envelope_ion ), bias_initializer_orbital_linear = bias_init_constructor ( model_config . bias_init_orbital_linear ), orbitals_use_bias = model_config . orbitals_use_bias , isotropic_decay = model_config . isotropic_decay , ) array_list_sign_covariance = get_sign_covariance_from_config ( model_config , spin_split , kernel_init_constructor , dtype ) return AntiequivarianceNet ( spin_split , compute_input_streams , backflow , antieq_layer , array_list_sign_covariance , multiply_by_eq_features = model_config . multiply_by_eq_features , ) elif model_config . type == \"explicit_antisym\" : jastrow_config = model_config . jastrow def _get_two_body_decay_jastrow (): return get_two_body_decay_scaled_for_chargeless_molecules ( ion_pos , ion_charges , init_ee_strength = jastrow_config . two_body_decay . init_ee_strength , trainable = jastrow_config . two_body_decay . trainable , ) def _get_backflow_based_jastrow (): if jastrow_config . backflow_based . use_separate_jastrow_backflow : jastrow_backflow = get_backflow_from_config ( jastrow_config . backflow_based . backflow , spin_split , dtype = dtype , ) else : jastrow_backflow = None return BackflowJastrow ( backflow = jastrow_backflow ) if jastrow_config . type == \"one_body_decay\" : jastrow : Jastrow = OneBodyExpDecay ( kernel_initializer = kernel_init_constructor ( jastrow_config . one_body_decay . kernel_init ) ) elif jastrow_config . type == \"two_body_decay\" : jastrow = _get_two_body_decay_jastrow () elif jastrow_config . type == \"backflow_based\" : jastrow = _get_backflow_based_jastrow () elif jastrow_config . type == \"two_body_decay_and_backflow_based\" : two_body_decay_jastrow = _get_two_body_decay_jastrow () backflow_jastrow = _get_backflow_based_jastrow () jastrow = AddedModel ([ two_body_decay_jastrow , backflow_jastrow ]) else : raise ValueError ( \"Unsupported jastrow type; {} was requested, but the only supported \" \"types are: {} \" . format ( jastrow_config . type , \", \" . join ( VALID_JASTROW_TYPES ) ) ) if model_config . antisym_type == \"factorized\" : return FactorizedAntisymmetry ( spin_split , compute_input_streams , backflow , jastrow , rank = model_config . rank , ndense_resnet = model_config . ndense_resnet , nlayers_resnet = model_config . nlayers_resnet , kernel_initializer_resnet = kernel_init_constructor ( model_config . kernel_init_resnet ), bias_initializer_resnet = bias_init_constructor ( model_config . bias_init_resnet ), activation_fn_resnet = _get_named_activation_fn ( model_config . activation_fn_resnet ), resnet_use_bias = model_config . resnet_use_bias , ) elif model_config . antisym_type == \"generic\" : return GenericAntisymmetry ( spin_split , compute_input_streams , backflow , jastrow , ndense_resnet = model_config . ndense_resnet , nlayers_resnet = model_config . nlayers_resnet , kernel_initializer_resnet = kernel_init_constructor ( model_config . kernel_init_resnet ), bias_initializer_resnet = bias_init_constructor ( model_config . bias_init_resnet ), activation_fn_resnet = _get_named_activation_fn ( model_config . activation_fn_resnet ), resnet_use_bias = model_config . resnet_use_bias , ) else : raise ValueError ( \"Unsupported explicit antisymmetry type; {} was requested\" . format ( model_config . antisym_type ) ) else : raise ValueError ( \"Unsupported model type; {} was requested\" . format ( model_config . type ) )","title":"get_model_from_config()"},{"location":"api/models/construct/#vmcnet.models.construct.get_compute_input_streams_from_config","text":"Get a function for computing input streams from a model configuration. Source code in vmcnet/models/construct.py def get_compute_input_streams_from_config ( input_streams_config : ConfigDict , ion_pos : Optional [ Array ] = None ) -> ComputeInputStreams : \"\"\"Get a function for computing input streams from a model configuration.\"\"\" return functools . partial ( compute_input_streams , ion_pos = ion_pos , include_2e_stream = input_streams_config . include_2e_stream , include_ei_norm = input_streams_config . include_ei_norm , include_ee_norm = input_streams_config . include_ee_norm , )","title":"get_compute_input_streams_from_config()"},{"location":"api/models/construct/#vmcnet.models.construct.get_backflow_from_config","text":"Get a FermiNet backflow from a model configuration. Source code in vmcnet/models/construct.py def get_backflow_from_config ( backflow_config , spin_split , dtype = jnp . float32 , ) -> Module : \"\"\"Get a FermiNet backflow from a model configuration.\"\"\" kernel_init_constructor , bias_init_constructor = _get_dtype_init_constructors ( dtype ) residual_blocks = get_residual_blocks_for_ferminet_backflow ( spin_split , backflow_config . ndense_list , kernel_initializer_unmixed = kernel_init_constructor ( backflow_config . kernel_init_unmixed ), kernel_initializer_mixed = kernel_init_constructor ( backflow_config . kernel_init_mixed ), kernel_initializer_2e_1e_stream = kernel_init_constructor ( backflow_config . kernel_init_2e_1e_stream ), kernel_initializer_2e_2e_stream = kernel_init_constructor ( backflow_config . kernel_init_2e_2e_stream ), bias_initializer_1e_stream = bias_init_constructor ( backflow_config . bias_init_1e_stream ), bias_initializer_2e_stream = bias_init_constructor ( backflow_config . bias_init_2e_stream ), activation_fn = _get_named_activation_fn ( backflow_config . activation_fn ), use_bias = backflow_config . use_bias , one_electron_skip = backflow_config . one_electron_skip , one_electron_skip_scale = backflow_config . one_electron_skip_scale , two_electron_skip = backflow_config . two_electron_skip , two_electron_skip_scale = backflow_config . two_electron_skip_scale , cyclic_spins = backflow_config . cyclic_spins , ) return FermiNetBackflow ( residual_blocks )","title":"get_backflow_from_config()"},{"location":"api/models/construct/#vmcnet.models.construct.get_sign_covariance_from_config","text":"Get a sign covariance from a model config, for use in AntiequivarianceNet. Source code in vmcnet/models/construct.py def get_sign_covariance_from_config ( model_config : ConfigDict , spin_split : ParticleSplit , kernel_init_constructor : Callable [[ ConfigDict ], WeightInitializer ], dtype : jnp . dtype , ) -> Callable [[ ArrayList ], Array ]: \"\"\"Get a sign covariance from a model config, for use in AntiequivarianceNet.\"\"\" if model_config . use_products_covariance : return ProductsSignCovariance ( 1 , kernel_init_constructor ( model_config . products_covariance . kernel_init ), model_config . products_covariance . register_kfac , use_weights = model_config . products_covariance . use_weights , ) else : def backflow_based_equivariance ( x : ArrayList ) -> Array : concat_x = jnp . concatenate ( x , axis =- 2 ) return get_backflow_from_config ( model_config . invariance , spin_split = spin_split , dtype = dtype , )( concat_x ) odd_equivariance = make_array_list_fn_sign_covariant ( backflow_based_equivariance , axis =- 3 ) return lambda x : jnp . sum ( odd_equivariance ( x ), axis =- 2 )","title":"get_sign_covariance_from_config()"},{"location":"api/models/construct/#vmcnet.models.construct.get_residual_blocks_for_ferminet_backflow","text":"Construct a list of FermiNet residual blocks composed by FermiNetBackflow. Parameters: Name Type Description Default spin_split int or Sequence[int] number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. required ndense_list List[Tuple[int, ...]] (list of (int, ...)): number of dense nodes in each of the residual blocks, (ndense_1e, optional ndense_2e). The length of this list determines the number of residual blocks which are composed on top of the input streams. If ndense_2e is specified, then then a dense layer is applied to the two-electron stream with an optional skip connection, otherwise the two-electron stream is mixed into the one-electron stream but no transformation is done. required kernel_initializer_unmixed WeightInitializer kernel initializer for the unmixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the previous one-electron stream output. Has signature (key, shape, dtype) -> Array required kernel_initializer_mixed WeightInitializer kernel initializer for the mixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous one-electron stream output. Has signature (key, shape, dtype) -> Array required kernel_initializer_2e_1e_stream WeightInitializer kernel initializer for the two-electron part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous two-electron stream which is mixed into the one-electron stream. Has signature (key, shape, dtype) -> Array required kernel_initializer_2e_2e_stream WeightInitializer kernel initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array required bias_initializer_1e_stream WeightInitializer bias initializer for the one-electron stream. Has signature (key, shape, dtype) -> Array required bias_initializer_2e_stream WeightInitializer bias initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array required activation_fn Activation activation function in the electron streams. Has the signature Array -> Array (shape is preserved) required use_bias bool whether to add a bias term in the electron streams. Defaults to True. True one_electron_skip bool whether to add a residual skip connection to the one-electron layer whenever the shapes of the input and output match. Defaults to True. True one_electron_skip_scale float quantity to scale the one-electron output by if a skip connection is added. Defaults to 1.0. 1.0 two_electron_skip bool whether to add a residual skip connection to the two-electron layer whenever the shapes of the input and output match. Defaults to True. True two_electron_skip_scale float quantity to scale the two-electron output by if a skip connection is added. Defaults to 1.0. 1.0 cyclic_spins bool whether the the concatenation in the one-electron stream should satisfy a cyclic equivariance structure, i.e. if there are three spins (1, 2, 3), then in the mixed part of the stream, after averaging but before the linear transformation, cyclic equivariance means the inputs are [(1, 2, 3), (2, 3, 1), (3, 1, 2)]. If False, then the inputs are [(1, 2, 3), (1, 2, 3), (1, 2, 3)] (as in the original FermiNet). When there are only two spins (spin-1/2 case), then this is equivalent to true spin equivariance. Defaults to False (original FermiNet). True Source code in vmcnet/models/construct.py def get_residual_blocks_for_ferminet_backflow ( spin_split : ParticleSplit , ndense_list : List [ Tuple [ int , ... ]], kernel_initializer_unmixed : WeightInitializer , kernel_initializer_mixed : WeightInitializer , kernel_initializer_2e_1e_stream : WeightInitializer , kernel_initializer_2e_2e_stream : WeightInitializer , bias_initializer_1e_stream : WeightInitializer , bias_initializer_2e_stream : WeightInitializer , activation_fn : Activation , use_bias : bool = True , one_electron_skip : bool = True , one_electron_skip_scale : float = 1.0 , two_electron_skip : bool = True , two_electron_skip_scale : float = 1.0 , cyclic_spins : bool = True , ) -> List [ FermiNetResidualBlock ]: \"\"\"Construct a list of FermiNet residual blocks composed by FermiNetBackflow. Arguments: spin_split (int or Sequence[int]): number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and `spin_split` = 2, then the input is split (5, 5). If nelec = 10, and `spin_split` = (2, 4), then the input is split into (2, 4, 4) -- note when `spin_split` is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, `spin_split` should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. ndense_list: (list of (int, ...)): number of dense nodes in each of the residual blocks, (ndense_1e, optional ndense_2e). The length of this list determines the number of residual blocks which are composed on top of the input streams. If ndense_2e is specified, then then a dense layer is applied to the two-electron stream with an optional skip connection, otherwise the two-electron stream is mixed into the one-electron stream but no transformation is done. kernel_initializer_unmixed (WeightInitializer): kernel initializer for the unmixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_mixed (WeightInitializer): kernel initializer for the mixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_2e_1e_stream (WeightInitializer): kernel initializer for the two-electron part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous two-electron stream which is mixed into the one-electron stream. Has signature (key, shape, dtype) -> Array kernel_initializer_2e_2e_stream (WeightInitializer): kernel initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array bias_initializer_1e_stream (WeightInitializer): bias initializer for the one-electron stream. Has signature (key, shape, dtype) -> Array bias_initializer_2e_stream (WeightInitializer): bias initializer for the two-electron stream. Has signature (key, shape, dtype) -> Array activation_fn (Activation): activation function in the electron streams. Has the signature Array -> Array (shape is preserved) use_bias (bool, optional): whether to add a bias term in the electron streams. Defaults to True. one_electron_skip (bool, optional): whether to add a residual skip connection to the one-electron layer whenever the shapes of the input and output match. Defaults to True. one_electron_skip_scale (float, optional): quantity to scale the one-electron output by if a skip connection is added. Defaults to 1.0. two_electron_skip (bool, optional): whether to add a residual skip connection to the two-electron layer whenever the shapes of the input and output match. Defaults to True. two_electron_skip_scale (float, optional): quantity to scale the two-electron output by if a skip connection is added. Defaults to 1.0. cyclic_spins (bool, optional): whether the the concatenation in the one-electron stream should satisfy a cyclic equivariance structure, i.e. if there are three spins (1, 2, 3), then in the mixed part of the stream, after averaging but before the linear transformation, cyclic equivariance means the inputs are [(1, 2, 3), (2, 3, 1), (3, 1, 2)]. If False, then the inputs are [(1, 2, 3), (1, 2, 3), (1, 2, 3)] (as in the original FermiNet). When there are only two spins (spin-1/2 case), then this is equivalent to true spin equivariance. Defaults to False (original FermiNet). \"\"\" residual_blocks = [] for ndense in ndense_list : one_electron_layer = FermiNetOneElectronLayer ( spin_split , ndense [ 0 ], kernel_initializer_unmixed , kernel_initializer_mixed , kernel_initializer_2e_1e_stream , bias_initializer_1e_stream , activation_fn , use_bias , skip_connection = one_electron_skip , skip_connection_scale = one_electron_skip_scale , cyclic_spins = cyclic_spins , ) two_electron_layer = None if len ( ndense ) > 1 : two_electron_layer = FermiNetTwoElectronLayer ( ndense [ 1 ], kernel_initializer_2e_2e_stream , bias_initializer_2e_stream , activation_fn , use_bias , skip_connection = two_electron_skip , skip_connection_scale = two_electron_skip_scale , ) residual_blocks . append ( FermiNetResidualBlock ( one_electron_layer , two_electron_layer ) ) return residual_blocks","title":"get_residual_blocks_for_ferminet_backflow()"},{"location":"api/models/construct/#vmcnet.models.construct.get_resnet_determinant_fn_for_ferminet","text":"Get a resnet-based determinant function for FermiNet construction. The returned function is used as a more general way to combine the determinant outputs into the final wavefunction value, relative to the original method of a sum of products. The function takes as its first argument the number of requested output features because several variants of this method are supported, and each requires the function to generate a different output size. Parameters: Name Type Description Default ndense int the number of neurons in the dense layers of the ResNet. required nlayers int the number of layers in the ResNet. required activation Activation the activation function to use for the resnet. required kernel_initializer WeightInitializer kernel initializer for the resnet. required bias_initializer WeightInitializer bias initializer for the resnet. required use_bias bool Whether to use a bias in the ResNet. Defaults to True. True register_kfac bool Whether to register the ResNet Dense layers with KFAC. Currently, params for this ResNet explode to huge values and cause nans if register_kfac is True, so this flag defaults to false and should only be overridden with care. The reason for the instability is not known. False Returns: Type Description (DeterminantFn) A resnet-based function. Has the signature dout, [nspins: (..., ndeterminants)] -> (..., dout). Source code in vmcnet/models/construct.py def get_resnet_determinant_fn_for_ferminet ( ndense : int , nlayers : int , activation : Activation , kernel_initializer : WeightInitializer , bias_initializer : WeightInitializer , use_bias : bool = True , register_kfac : bool = False , ) -> DeterminantFn : \"\"\"Get a resnet-based determinant function for FermiNet construction. The returned function is used as a more general way to combine the determinant outputs into the final wavefunction value, relative to the original method of a sum of products. The function takes as its first argument the number of requested output features because several variants of this method are supported, and each requires the function to generate a different output size. Args: ndense (int): the number of neurons in the dense layers of the ResNet. nlayers (int): the number of layers in the ResNet. activation (Activation): the activation function to use for the resnet. kernel_initializer (WeightInitializer): kernel initializer for the resnet. bias_initializer (WeightInitializer): bias initializer for the resnet. use_bias (bool): Whether to use a bias in the ResNet. Defaults to True. register_kfac (bool): Whether to register the ResNet Dense layers with KFAC. Currently, params for this ResNet explode to huge values and cause nans if register_kfac is True, so this flag defaults to false and should only be overridden with care. The reason for the instability is not known. Returns: (DeterminantFn): A resnet-based function. Has the signature dout, [nspins: (..., ndeterminants)] -> (..., dout). \"\"\" def fn ( dout : int , det_values : ArrayList ) -> Array : concat_values = jnp . concatenate ( det_values , axis =- 1 ) return SimpleResNet ( ndense , dout , nlayers , activation , kernel_initializer , bias_initializer , use_bias , register_kfac = register_kfac , )( concat_values ) return fn","title":"get_resnet_determinant_fn_for_ferminet()"},{"location":"api/models/core/","text":"Core model building parts. AddedModel ( Module ) dataclass A model made from added parts. Attributes: Name Type Description submodels Sequence[Union[Callable, Module]] a sequence of functions or Modules which are called on the same args and can be added __call__ ( self , * args ) special Add the outputs of the submodels. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , * args ): \"\"\"Add the outputs of the submodels.\"\"\" return sum ( submodel ( * args ) for submodel in self . submodels ) ComposedModel ( Module ) dataclass A model made from composable parts. Attributes: Name Type Description submodels Sequence[Union[Callable, Module]] a sequence of functions or Modules which can be composed sequentially __call__ ( self , x ) special Call submodels on the output of the previous one one at a time. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , x ): \"\"\"Call submodels on the output of the previous one one at a time.\"\"\" outputs = x for model in self . submodels : outputs = model ( outputs ) return outputs Dense ( Module ) dataclass A linear transformation applied over the last dimension of the input. This is a copy of the flax Dense layer, but with registration of the weights for use with KFAC. Attributes: Name Type Description features int the number of output features. kernel_init WeightInitializer initializer function for the weight matrix. Defaults to orthogonal initialization. bias_init WeightInitializer initializer function for the bias. Defaults to random normal initialization. use_bias bool whether to add a bias to the output. Defaults to True. register_kfac bool whether to register the computation with KFAC. Defaults to True. __call__ ( self , inputs ) special Applies a linear transformation with optional bias along the last dimension. Parameters: Name Type Description Default inputs Array The nd-array to be transformed. required Returns: Type Description Array The transformed input. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , inputs : Array ) -> Array : # type: ignore[override] \"\"\"Applies a linear transformation with optional bias along the last dimension. Args: inputs (Array): The nd-array to be transformed. Returns: Array: The transformed input. \"\"\" kernel = self . param ( \"kernel\" , self . kernel_init , ( inputs . shape [ - 1 ], self . features ) ) y = jnp . dot ( inputs , kernel ) bias = None if self . use_bias : bias = self . param ( \"bias\" , self . bias_init , ( self . features ,)) y = y + bias if self . register_kfac : return register_batch_dense ( y , inputs , kernel , bias ) else : return y LogDomainDense ( Module ) dataclass A linear transformation applied on the last axis of the input, in the log domain. If the inputs are (sign(x), log(abs(x))), the outputs are (sign(Wx + b), log(abs(Wx + b))). The bias is implemented by extending the inputs with a vector of ones. Attributes: Name Type Description features int the number of output features. kernel_init WeightInitializer initializer function for the weight matrix. Defaults to orthogonal initialization. use_bias bool whether to add a bias to the output. Defaults to True. register_kfac bool whether to register the computation with KFAC. Defaults to True. __call__ ( self , x ) special Applies a linear transformation with optional bias along the last dimension. Parameters: Name Type Description Default x SLArray The nd-array in slog form to be transformed. required Returns: Type Description SLArray The transformed input, in slog form. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , x : SLArray ) -> SLArray : # type: ignore[override] \"\"\"Applies a linear transformation with optional bias along the last dimension. Args: x (SLArray): The nd-array in slog form to be transformed. Returns: SLArray: The transformed input, in slog form. \"\"\" sign_x , log_abs_x = x input_dim = log_abs_x . shape [ - 1 ] if self . use_bias : input_dim += 1 sign_x = jnp . concatenate ([ sign_x , jnp . ones_like ( sign_x [ ... , 0 : 1 ])], axis =- 1 ) log_abs_x = jnp . concatenate ( [ log_abs_x , jnp . zeros_like ( log_abs_x [ ... , 0 : 1 ])], axis =- 1 ) kernel = self . param ( \"kernel\" , self . kernel_init , ( input_dim , self . features )) return log_linear_exp ( sign_x , log_abs_x , kernel , axis =- 1 , register_kfac = self . register_kfac , ) LogDomainResNet ( Module ) dataclass Simplest fully-connected ResNet, implemented in the log domain. Attributes: Name Type Description ndense_inner int number of dense nodes in layers before the final layer. ndense_final int number of output features, i.e. the number of dense nodes in the final Dense call. nlayers int number of dense layers applied to the input, including the final layer. If this is 0, the final dense layer will still be applied. activation_fn SLActivation activation function between intermediate layers (is not applied after the final dense layer). Has the signature SLArray -> SLArray (shape is preserved). kernel_init WeightInitializer initializer function for the weight matrices of each layer. Defaults to orthogonal initialization. use_bias bool whether the dense layers should all have bias terms or not. Defaults to True. setup ( self ) Setup dense layers. Source code in vmcnet/models/core.py def setup ( self ): \"\"\"Setup dense layers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . inner_dense = [ LogDomainDense ( self . ndense_inner , kernel_init = self . kernel_init , use_bias = self . use_bias , ) for _ in range ( self . nlayers - 1 ) ] self . final_dense = LogDomainDense ( self . ndense_final , kernel_init = self . kernel_init , use_bias = False , ) __call__ ( self , x ) special Repeated application of (dense layer -> activation -> optional skip) block. Parameters: Name Type Description Default x SLArray an slog input array of shape (..., d) required Returns: Type Description SLArray slog array of shape (..., self.ndense_final) Source code in vmcnet/models/core.py def __call__ ( self , x : SLArray ) -> SLArray : # type: ignore[override] \"\"\"Repeated application of (dense layer -> activation -> optional skip) block. Args: x (SLArray): an slog input array of shape (..., d) Returns: SLArray: slog array of shape (..., self.ndense_final) \"\"\" for dense_layer in self . inner_dense : prev_x = x x = dense_layer ( prev_x ) x = self . _activation_fn ( x ) if _sl_valid_skip ( prev_x , x ): x = slog_sum ( x , prev_x ) return self . final_dense ( x ) SimpleResNet ( Module ) dataclass Simplest fully-connected ResNet. Attributes: Name Type Description ndense_inner int number of dense nodes in layers before the final layer. ndense_final int number of output features, i.e. the number of dense nodes in the final Dense call. nlayers int number of dense layers applied to the input, including the final layer. If this is 0, the final dense layer will still be applied. kernel_init WeightInitializer initializer function for the weight matrices of each layer. Defaults to orthogonal initialization. bias_init WeightInitializer initializer function for the bias. Defaults to random normal initialization. activation_fn Activation activation function between intermediate layers (is not applied after the final dense layer). Has the signature Array -> Array (shape is preserved) use_bias bool whether the dense layers should all have bias terms or not. Defaults to True. register_kfac bool whether to register the dense layers with KFAC. Defaults to True. setup ( self ) Setup dense layers. Source code in vmcnet/models/core.py def setup ( self ): \"\"\"Setup dense layers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . inner_dense = [ Dense ( self . ndense_inner , kernel_init = self . kernel_init , bias_init = self . bias_init , use_bias = self . use_bias , register_kfac = self . register_kfac , ) for _ in range ( self . nlayers - 1 ) ] self . final_dense = Dense ( self . ndense_final , kernel_init = self . kernel_init , bias_init = self . bias_init , use_bias = False , register_kfac = self . register_kfac , ) __call__ ( self , x ) special Repeated application of (dense layer -> activation -> optional skip) block. Parameters: Name Type Description Default x Array an input array of shape (..., d) required Returns: Type Description Array array of shape (..., self.ndense_final) Source code in vmcnet/models/core.py def __call__ ( self , x : Array ) -> Array : # type: ignore[override] \"\"\"Repeated application of (dense layer -> activation -> optional skip) block. Args: x (Array): an input array of shape (..., d) Returns: Array: array of shape (..., self.ndense_final) \"\"\" for dense_layer in self . inner_dense : prev_x = x x = dense_layer ( prev_x ) x = self . _activation_fn ( x ) if _valid_skip ( prev_x , x ): x = cast ( Array , x + prev_x ) return self . final_dense ( x ) compute_ee_norm_with_safe_diag ( r_ee ) Get electron-electron distances with a safe derivative along the diagonal. Avoids computing norm(x - x) along the diagonal, since autograd will be unhappy about differentiating through the norm function evaluated at 0. Instead compute 0 * norm(x - x + 1) along the diagonal. Parameters: Name Type Description Default r_ee Array electron-electron displacements wth shape (..., n, n, d) required Returns: Type Description Array electron-electrondists with shape (..., n, n, 1) Source code in vmcnet/models/core.py def compute_ee_norm_with_safe_diag ( r_ee ): \"\"\"Get electron-electron distances with a safe derivative along the diagonal. Avoids computing norm(x - x) along the diagonal, since autograd will be unhappy about differentiating through the norm function evaluated at 0. Instead compute 0 * norm(x - x + 1) along the diagonal. Args: r_ee (Array): electron-electron displacements wth shape (..., n, n, d) Returns: Array: electron-electrondists with shape (..., n, n, 1) \"\"\" n = r_ee . shape [ - 2 ] eye_n = jnp . expand_dims ( jnp . eye ( n ), axis =- 1 ) r_ee_diag_ones = r_ee + eye_n return jnp . linalg . norm ( r_ee_diag_ones , axis =- 1 , keepdims = True ) * ( 1.0 - eye_n ) is_tuple_of_arrays ( x ) Returns True if x is a tuple of Array objects. Source code in vmcnet/models/core.py def is_tuple_of_arrays ( x : PyTree ) -> bool : \"\"\"Returns True if x is a tuple of Array objects.\"\"\" return isinstance ( x , tuple ) and all ( isinstance ( x_i , jnp . ndarray ) for x_i in x ) get_alternating_signs ( n ) Return alternating series of 1 and -1, of length n. Source code in vmcnet/models/core.py def get_alternating_signs ( n : int ) -> Array : \"\"\"Return alternating series of 1 and -1, of length n.\"\"\" return jax . ops . index_update ( jnp . ones ( n ), jax . ops . index [ 1 :: 2 ], - 1.0 ) get_nsplits ( split ) Get the number of splits from a particle split specification. Source code in vmcnet/models/core.py def get_nsplits ( split : ParticleSplit ) -> int : \"\"\"Get the number of splits from a particle split specification.\"\"\" if isinstance ( split , int ): return split return len ( split ) + 1 get_nelec_per_split ( split , nelec_total ) From a particle split and nelec_total, get the number of particles per split. If the number of particles per split is nelec_per_spin = (n1, n2, ..., nk), then split should be jnp.cumsum(nelec_per_spin)[:-1], or an integer of these are all equal. This function is the inverse of this operation. Source code in vmcnet/models/core.py def get_nelec_per_split ( split : ParticleSplit , nelec_total : int ) -> Tuple [ int , ... ]: \"\"\"From a particle split and nelec_total, get the number of particles per split. If the number of particles per split is nelec_per_spin = (n1, n2, ..., nk), then split should be jnp.cumsum(nelec_per_spin)[:-1], or an integer of these are all equal. This function is the inverse of this operation. \"\"\" if isinstance ( split , int ): return ( nelec_total // split ,) * split else : spin_diffs = jnp . diff ( jnp . array ( split )) return ( split [ 0 ], * tuple ([ int ( i ) for i in spin_diffs ]), nelec_total - split [ - 1 ], ) get_spin_split ( n_per_split ) Calculate spin split from n_per_split, making sure to output a Tuple of ints. Source code in vmcnet/models/core.py def get_spin_split ( n_per_split : Union [ Sequence [ int ], Array ]) -> Tuple [ int , ... ]: \"\"\"Calculate spin split from n_per_split, making sure to output a Tuple of ints.\"\"\" cumsum = np . cumsum ( n_per_split [: - 1 ]) # Convert to tuple of python ints. return tuple ([ int ( i ) for i in cumsum ])","title":"core"},{"location":"api/models/core/#vmcnet.models.core.AddedModel","text":"A model made from added parts. Attributes: Name Type Description submodels Sequence[Union[Callable, Module]] a sequence of functions or Modules which are called on the same args and can be added","title":"AddedModel"},{"location":"api/models/core/#vmcnet.models.core.AddedModel.__call__","text":"Add the outputs of the submodels. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , * args ): \"\"\"Add the outputs of the submodels.\"\"\" return sum ( submodel ( * args ) for submodel in self . submodels )","title":"__call__()"},{"location":"api/models/core/#vmcnet.models.core.ComposedModel","text":"A model made from composable parts. Attributes: Name Type Description submodels Sequence[Union[Callable, Module]] a sequence of functions or Modules which can be composed sequentially","title":"ComposedModel"},{"location":"api/models/core/#vmcnet.models.core.ComposedModel.__call__","text":"Call submodels on the output of the previous one one at a time. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , x ): \"\"\"Call submodels on the output of the previous one one at a time.\"\"\" outputs = x for model in self . submodels : outputs = model ( outputs ) return outputs","title":"__call__()"},{"location":"api/models/core/#vmcnet.models.core.Dense","text":"A linear transformation applied over the last dimension of the input. This is a copy of the flax Dense layer, but with registration of the weights for use with KFAC. Attributes: Name Type Description features int the number of output features. kernel_init WeightInitializer initializer function for the weight matrix. Defaults to orthogonal initialization. bias_init WeightInitializer initializer function for the bias. Defaults to random normal initialization. use_bias bool whether to add a bias to the output. Defaults to True. register_kfac bool whether to register the computation with KFAC. Defaults to True.","title":"Dense"},{"location":"api/models/core/#vmcnet.models.core.Dense.__call__","text":"Applies a linear transformation with optional bias along the last dimension. Parameters: Name Type Description Default inputs Array The nd-array to be transformed. required Returns: Type Description Array The transformed input. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , inputs : Array ) -> Array : # type: ignore[override] \"\"\"Applies a linear transformation with optional bias along the last dimension. Args: inputs (Array): The nd-array to be transformed. Returns: Array: The transformed input. \"\"\" kernel = self . param ( \"kernel\" , self . kernel_init , ( inputs . shape [ - 1 ], self . features ) ) y = jnp . dot ( inputs , kernel ) bias = None if self . use_bias : bias = self . param ( \"bias\" , self . bias_init , ( self . features ,)) y = y + bias if self . register_kfac : return register_batch_dense ( y , inputs , kernel , bias ) else : return y","title":"__call__()"},{"location":"api/models/core/#vmcnet.models.core.LogDomainDense","text":"A linear transformation applied on the last axis of the input, in the log domain. If the inputs are (sign(x), log(abs(x))), the outputs are (sign(Wx + b), log(abs(Wx + b))). The bias is implemented by extending the inputs with a vector of ones. Attributes: Name Type Description features int the number of output features. kernel_init WeightInitializer initializer function for the weight matrix. Defaults to orthogonal initialization. use_bias bool whether to add a bias to the output. Defaults to True. register_kfac bool whether to register the computation with KFAC. Defaults to True.","title":"LogDomainDense"},{"location":"api/models/core/#vmcnet.models.core.LogDomainDense.__call__","text":"Applies a linear transformation with optional bias along the last dimension. Parameters: Name Type Description Default x SLArray The nd-array in slog form to be transformed. required Returns: Type Description SLArray The transformed input, in slog form. Source code in vmcnet/models/core.py @flax . linen . compact def __call__ ( self , x : SLArray ) -> SLArray : # type: ignore[override] \"\"\"Applies a linear transformation with optional bias along the last dimension. Args: x (SLArray): The nd-array in slog form to be transformed. Returns: SLArray: The transformed input, in slog form. \"\"\" sign_x , log_abs_x = x input_dim = log_abs_x . shape [ - 1 ] if self . use_bias : input_dim += 1 sign_x = jnp . concatenate ([ sign_x , jnp . ones_like ( sign_x [ ... , 0 : 1 ])], axis =- 1 ) log_abs_x = jnp . concatenate ( [ log_abs_x , jnp . zeros_like ( log_abs_x [ ... , 0 : 1 ])], axis =- 1 ) kernel = self . param ( \"kernel\" , self . kernel_init , ( input_dim , self . features )) return log_linear_exp ( sign_x , log_abs_x , kernel , axis =- 1 , register_kfac = self . register_kfac , )","title":"__call__()"},{"location":"api/models/core/#vmcnet.models.core.LogDomainResNet","text":"Simplest fully-connected ResNet, implemented in the log domain. Attributes: Name Type Description ndense_inner int number of dense nodes in layers before the final layer. ndense_final int number of output features, i.e. the number of dense nodes in the final Dense call. nlayers int number of dense layers applied to the input, including the final layer. If this is 0, the final dense layer will still be applied. activation_fn SLActivation activation function between intermediate layers (is not applied after the final dense layer). Has the signature SLArray -> SLArray (shape is preserved). kernel_init WeightInitializer initializer function for the weight matrices of each layer. Defaults to orthogonal initialization. use_bias bool whether the dense layers should all have bias terms or not. Defaults to True.","title":"LogDomainResNet"},{"location":"api/models/core/#vmcnet.models.core.LogDomainResNet.setup","text":"Setup dense layers. Source code in vmcnet/models/core.py def setup ( self ): \"\"\"Setup dense layers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . inner_dense = [ LogDomainDense ( self . ndense_inner , kernel_init = self . kernel_init , use_bias = self . use_bias , ) for _ in range ( self . nlayers - 1 ) ] self . final_dense = LogDomainDense ( self . ndense_final , kernel_init = self . kernel_init , use_bias = False , )","title":"setup()"},{"location":"api/models/core/#vmcnet.models.core.LogDomainResNet.__call__","text":"Repeated application of (dense layer -> activation -> optional skip) block. Parameters: Name Type Description Default x SLArray an slog input array of shape (..., d) required Returns: Type Description SLArray slog array of shape (..., self.ndense_final) Source code in vmcnet/models/core.py def __call__ ( self , x : SLArray ) -> SLArray : # type: ignore[override] \"\"\"Repeated application of (dense layer -> activation -> optional skip) block. Args: x (SLArray): an slog input array of shape (..., d) Returns: SLArray: slog array of shape (..., self.ndense_final) \"\"\" for dense_layer in self . inner_dense : prev_x = x x = dense_layer ( prev_x ) x = self . _activation_fn ( x ) if _sl_valid_skip ( prev_x , x ): x = slog_sum ( x , prev_x ) return self . final_dense ( x )","title":"__call__()"},{"location":"api/models/core/#vmcnet.models.core.SimpleResNet","text":"Simplest fully-connected ResNet. Attributes: Name Type Description ndense_inner int number of dense nodes in layers before the final layer. ndense_final int number of output features, i.e. the number of dense nodes in the final Dense call. nlayers int number of dense layers applied to the input, including the final layer. If this is 0, the final dense layer will still be applied. kernel_init WeightInitializer initializer function for the weight matrices of each layer. Defaults to orthogonal initialization. bias_init WeightInitializer initializer function for the bias. Defaults to random normal initialization. activation_fn Activation activation function between intermediate layers (is not applied after the final dense layer). Has the signature Array -> Array (shape is preserved) use_bias bool whether the dense layers should all have bias terms or not. Defaults to True. register_kfac bool whether to register the dense layers with KFAC. Defaults to True.","title":"SimpleResNet"},{"location":"api/models/core/#vmcnet.models.core.SimpleResNet.setup","text":"Setup dense layers. Source code in vmcnet/models/core.py def setup ( self ): \"\"\"Setup dense layers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . inner_dense = [ Dense ( self . ndense_inner , kernel_init = self . kernel_init , bias_init = self . bias_init , use_bias = self . use_bias , register_kfac = self . register_kfac , ) for _ in range ( self . nlayers - 1 ) ] self . final_dense = Dense ( self . ndense_final , kernel_init = self . kernel_init , bias_init = self . bias_init , use_bias = False , register_kfac = self . register_kfac , )","title":"setup()"},{"location":"api/models/core/#vmcnet.models.core.SimpleResNet.__call__","text":"Repeated application of (dense layer -> activation -> optional skip) block. Parameters: Name Type Description Default x Array an input array of shape (..., d) required Returns: Type Description Array array of shape (..., self.ndense_final) Source code in vmcnet/models/core.py def __call__ ( self , x : Array ) -> Array : # type: ignore[override] \"\"\"Repeated application of (dense layer -> activation -> optional skip) block. Args: x (Array): an input array of shape (..., d) Returns: Array: array of shape (..., self.ndense_final) \"\"\" for dense_layer in self . inner_dense : prev_x = x x = dense_layer ( prev_x ) x = self . _activation_fn ( x ) if _valid_skip ( prev_x , x ): x = cast ( Array , x + prev_x ) return self . final_dense ( x )","title":"__call__()"},{"location":"api/models/core/#vmcnet.models.core.compute_ee_norm_with_safe_diag","text":"Get electron-electron distances with a safe derivative along the diagonal. Avoids computing norm(x - x) along the diagonal, since autograd will be unhappy about differentiating through the norm function evaluated at 0. Instead compute 0 * norm(x - x + 1) along the diagonal. Parameters: Name Type Description Default r_ee Array electron-electron displacements wth shape (..., n, n, d) required Returns: Type Description Array electron-electrondists with shape (..., n, n, 1) Source code in vmcnet/models/core.py def compute_ee_norm_with_safe_diag ( r_ee ): \"\"\"Get electron-electron distances with a safe derivative along the diagonal. Avoids computing norm(x - x) along the diagonal, since autograd will be unhappy about differentiating through the norm function evaluated at 0. Instead compute 0 * norm(x - x + 1) along the diagonal. Args: r_ee (Array): electron-electron displacements wth shape (..., n, n, d) Returns: Array: electron-electrondists with shape (..., n, n, 1) \"\"\" n = r_ee . shape [ - 2 ] eye_n = jnp . expand_dims ( jnp . eye ( n ), axis =- 1 ) r_ee_diag_ones = r_ee + eye_n return jnp . linalg . norm ( r_ee_diag_ones , axis =- 1 , keepdims = True ) * ( 1.0 - eye_n )","title":"compute_ee_norm_with_safe_diag()"},{"location":"api/models/core/#vmcnet.models.core.is_tuple_of_arrays","text":"Returns True if x is a tuple of Array objects. Source code in vmcnet/models/core.py def is_tuple_of_arrays ( x : PyTree ) -> bool : \"\"\"Returns True if x is a tuple of Array objects.\"\"\" return isinstance ( x , tuple ) and all ( isinstance ( x_i , jnp . ndarray ) for x_i in x )","title":"is_tuple_of_arrays()"},{"location":"api/models/core/#vmcnet.models.core.get_alternating_signs","text":"Return alternating series of 1 and -1, of length n. Source code in vmcnet/models/core.py def get_alternating_signs ( n : int ) -> Array : \"\"\"Return alternating series of 1 and -1, of length n.\"\"\" return jax . ops . index_update ( jnp . ones ( n ), jax . ops . index [ 1 :: 2 ], - 1.0 )","title":"get_alternating_signs()"},{"location":"api/models/core/#vmcnet.models.core.get_nsplits","text":"Get the number of splits from a particle split specification. Source code in vmcnet/models/core.py def get_nsplits ( split : ParticleSplit ) -> int : \"\"\"Get the number of splits from a particle split specification.\"\"\" if isinstance ( split , int ): return split return len ( split ) + 1","title":"get_nsplits()"},{"location":"api/models/core/#vmcnet.models.core.get_nelec_per_split","text":"From a particle split and nelec_total, get the number of particles per split. If the number of particles per split is nelec_per_spin = (n1, n2, ..., nk), then split should be jnp.cumsum(nelec_per_spin)[:-1], or an integer of these are all equal. This function is the inverse of this operation. Source code in vmcnet/models/core.py def get_nelec_per_split ( split : ParticleSplit , nelec_total : int ) -> Tuple [ int , ... ]: \"\"\"From a particle split and nelec_total, get the number of particles per split. If the number of particles per split is nelec_per_spin = (n1, n2, ..., nk), then split should be jnp.cumsum(nelec_per_spin)[:-1], or an integer of these are all equal. This function is the inverse of this operation. \"\"\" if isinstance ( split , int ): return ( nelec_total // split ,) * split else : spin_diffs = jnp . diff ( jnp . array ( split )) return ( split [ 0 ], * tuple ([ int ( i ) for i in spin_diffs ]), nelec_total - split [ - 1 ], )","title":"get_nelec_per_split()"},{"location":"api/models/core/#vmcnet.models.core.get_spin_split","text":"Calculate spin split from n_per_split, making sure to output a Tuple of ints. Source code in vmcnet/models/core.py def get_spin_split ( n_per_split : Union [ Sequence [ int ], Array ]) -> Tuple [ int , ... ]: \"\"\"Calculate spin split from n_per_split, making sure to output a Tuple of ints.\"\"\" cumsum = np . cumsum ( n_per_split [: - 1 ]) # Convert to tuple of python ints. return tuple ([ int ( i ) for i in cumsum ])","title":"get_spin_split()"},{"location":"api/models/equivariance/","text":"Permutation equivariant functions. DoublyEquivariantOrbitalLayer ( Module ) dataclass Equivariantly generate an orbital matrix corresponding to each input stream. The calculation being done here is a bit subtle, so it's worth explaining here in some detail. Let the equivariant input vectors to this layer be y_i. Then, this layer will generate an orbital matrix M_p for each particle P, such that the (i,j)th element of M_p satisfies M_(p,i,j) = phi_j(y_p, y_i). This is essentially the usual orbital matrix formula M_(i,j) = phi_j(y_i), except with an added dependence on the particle index p which allows us to generate a distinct matrix for each input particle. This construction allows us to generate a unique antisymmetric determinant D_p = det(M_p) for each input particle, which can then be the basis for an expressive antiequivariant layer. If r_ei is provided in addition to the main inputs y_i, then an exponentially decaying envelope is also applied equally to every orbital matrix M_p in order to ensure that the orbital values decay to zero far from the ions. Attributes: Name Type Description orbitals_split ParticleSplit number of pieces to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and orbitals_split = 2, then the input is split (5, 5). If nelec = 10, and orbitals_split = (2, 4), then the input is split into (2, 4, 4) -- note when orbitals_split is a sequence, there will be one more split than the length of the sequence. In the original use-case of spin-1/2 particles, split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. norbitals_per_split Sequence[int] sequence of integers specifying the number of orbitals to create for each split. This determines the output shapes for each split, i.e. the outputs are shaped (..., split_size[i], norbitals[i]) kernel_initializer_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. setup ( self ) Setup envelope kernel initializers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup envelope kernel initializers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _kernel_initializer_envelope_dim = self . kernel_initializer_envelope_dim self . _kernel_initializer_envelope_ion = self . kernel_initializer_envelope_ion __call__ ( self , x , r_ei = None ) special Calculate an equivariant orbital matrix for each input particle. Parameters: Name Type Description Default x Array array of shape (..., nelec, d) required r_ei Array array of shape (..., nelec, nion, d) None Returns: Type Description (ArrayList) list of length nsplits of arrays of shape (..., nelec[i], nelec[i], self.norbitals_per_split[i]). Here nelec[i] is the number of particles in the ith split. The output arrays have both their -2 and -3 axes equivariant with respect to the input particles. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say in order to be able to take antiequivariant per-particle determinants, nelec[i] should be equal to self.norbitals_per_split[i]. Source code in vmcnet/models/equivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , x : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Calculate an equivariant orbital matrix for each input particle. Args: x (Array): array of shape (..., nelec, d) r_ei (Array): array of shape (..., nelec, nion, d) Returns: (ArrayList): list of length nsplits of arrays of shape (..., nelec[i], nelec[i], self.norbitals_per_split[i]). Here nelec[i] is the number of particles in the ith split. The output arrays have both their -2 and -3 axes equivariant with respect to the input particles. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say in order to be able to take antiequivariant per-particle determinants, nelec[i] should be equal to self.norbitals_per_split[i]. \"\"\" # split_x is a list of nsplits arrays of shape (..., nelec[i], d)] split_x = jnp . split ( x , self . orbitals_split , - 2 ) # orbs is a list of nsplits arrays of shape # (..., nelec[i], nelec[i], norbitals[i]) orbs = [ self . _get_orbital_matrices_one_split ( x , self . norbitals_per_split [ i ]) for ( i , x ) in enumerate ( split_x ) ] if r_ei is not None : exp_envelopes = _compute_exponential_envelopes_all_splits ( r_ei , self . orbitals_split , self . norbitals_per_split , self . _kernel_initializer_envelope_dim , self . _kernel_initializer_envelope_ion , self . isotropic_decay , ) # Envelope must be expanded to apply equally to each per-particle matrix. exp_envelopes = jax . tree_map ( lambda x : jnp . expand_dims ( x , axis =- 3 ), exp_envelopes ) orbs = tree_prod ( orbs , exp_envelopes ) return orbs FermiNetBackflow ( Module ) dataclass The FermiNet equivariant part up until, but not including, the orbitals. Repeated composition of the residual blocks in the parallel one-electron and two-electron streams. Attributes: Name Type Description residual_blocks Sequence sequence of callable residual blocks which apply the one- and two- electron layers. Each residual block has the signature (in_1e, optional in_2e) -> (out_1e, optional out_2e), where in_1e has shape (..., n, d_1e) out_1e has shape (..., n, d_1e') in_2e has shape (..., n, n, d_2e) out_2d has shape (..., n, n, d_2e') setup ( self ) Setup called residual blocks. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup called residual blocks.\"\"\" self . _residual_block_list = [ block for block in self . residual_blocks ] __call__ ( self , stream_1e , stream_2e = None ) special Iteratively apply residual blocks to Ferminet input streams. Parameters: Name Type Description Default stream_1e Array one-electron input stream of shape (..., nelec, d1). required stream_2e Array two-electron input of shape (..., nelec, nelec, d2). None Returns: Type Description (Array) the output of the one-electron stream after applying self.residual_blocks to the initial input streams. Source code in vmcnet/models/equivariance.py def __call__ ( # type: ignore[override] self , stream_1e : Array , stream_2e : Optional [ Array ] = None , ) -> Array : \"\"\"Iteratively apply residual blocks to Ferminet input streams. Args: stream_1e (Array): one-electron input stream of shape (..., nelec, d1). stream_2e (Array, optional): two-electron input of shape (..., nelec, nelec, d2). Returns: (Array): the output of the one-electron stream after applying self.residual_blocks to the initial input streams. \"\"\" for block in self . _residual_block_list : stream_1e , stream_2e = block ( stream_1e , stream_2e ) return stream_1e FermiNetOneElectronLayer ( Module ) dataclass A single layer in the one-electron stream of the FermiNet equivariant part. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. ndense int number of dense nodes kernel_initializer_unmixed WeightInitializer kernel initializer for the unmixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_mixed WeightInitializer kernel initializer for the mixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_2e WeightInitializer kernel initializer for the two-electron part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous two-electron stream which is mixed into the one-electron stream. Has signature (key, shape, dtype) -> Array bias_initializer WeightInitializer bias initializer. Has signature (key, shape, dtype) -> Array activation_fn Activation activation function. Has the signature Array -> Array (shape is preserved) use_bias bool whether to add a bias term. Defaults to True. skip_connection bool whether to add residual skip connections whenever the shapes of the input and output match. Defaults to True. skip_connection_scale float quantity to scale the final output by if a skip connection is added. Defaults to 1.0. cyclic_spins bool whether the the concatenation in the one-electron stream should satisfy a cyclic equivariance structure, i.e. if there are three spins (1, 2, 3), then in the mixed part of the stream, after averaging but before the linear transformation, cyclic equivariance means the inputs are [(1, 2, 3), (2, 3, 1), (3, 1, 2)]. If False, then the inputs are [(1, 2, 3), (1, 2, 3), (1, 2, 3)] (as in the original FermiNet). When there are only two spins (spin-1/2 case), then this is equivalent to true spin equivariance. Defaults to False (original FermiNet). setup ( self ) Setup Dense layers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup Dense layers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . _unmixed_dense = Dense ( self . ndense , kernel_init = self . kernel_initializer_unmixed , bias_init = self . bias_initializer , use_bias = self . use_bias , ) self . _mixed_dense = Dense ( self . ndense , kernel_init = self . kernel_initializer_mixed , use_bias = False ) self . _dense_2e = Dense ( self . ndense , kernel_init = self . kernel_initializer_2e , use_bias = False ) __call__ ( self , in_1e , in_2e = None ) special Add dense outputs on unmixed, mixed, and 2e terms to get the 1e output. This implementation breaks the one-electron stream into three parts: 1) the unmixed one-particle part, which is a linear transformation applied in parallel for each particle to the inputs 2) the mixed one-particle part, which is a linear transformation applied to the averages of the inputs (concatenated over spin) 3) the two-particle part, which is a linear transformation applied in parallel for each particle to the average of the input interactions between that particle and all the other particles. For 1), we take in_1e of shape (..., n_total, d_1e), batch apply a linear transformation to get (..., n_total, d'), and split over the spins i to get [i: (..., n[i], d')]. For 2), we split in_1e over the spins along the particle axis to get [i: (..., n[i], d_1e)], average over each spin to get [i: (..., 1, d_1e)], concatenate all averages for each spin to get [i: (..., 1, d_1e * nspins)], and apply a linear transformation to get [i: (..., 1, d')]. For 3) we split in_2e of shape (..., n_total, n_total, d_2e) over the spins along a particle axis to get [i: (..., n[i], n_total, d_2e)], average over the other particle axis to get [i: [j: (..., n[i], d_2e)]], concatenate the averages for each spin to get [i: (..., n[i], d_2e * nspins)], and apply a linear transformation to get [i: (..., n[i], d')]. Finally, for each spin, we add the three parts, each equivariant or symmetric, to get a final equivariant linear transformation of the inputs, to which a non-linearity is then applied and a skip connection optionally added. Parameters: Name Type Description Default in_1e Array array of shape (..., n_total, d_1e) required in_2e Array array of shape (..., n_total, n_total, d_2e). Defaults to None. None Returns: Type Description ndarray Array of shape (..., n_total, self.ndense), the output one-electron stream Source code in vmcnet/models/equivariance.py def __call__ ( # type: ignore[override] self , in_1e : Array , in_2e : Array = None ) -> Array : \"\"\"Add dense outputs on unmixed, mixed, and 2e terms to get the 1e output. This implementation breaks the one-electron stream into three parts: 1) the unmixed one-particle part, which is a linear transformation applied in parallel for each particle to the inputs 2) the mixed one-particle part, which is a linear transformation applied to the averages of the inputs (concatenated over spin) 3) the two-particle part, which is a linear transformation applied in parallel for each particle to the average of the input interactions between that particle and all the other particles. For 1), we take `in_1e` of shape (..., n_total, d_1e), batch apply a linear transformation to get (..., n_total, d'), and split over the spins i to get [i: (..., n[i], d')]. For 2), we split `in_1e` over the spins along the particle axis to get [i: (..., n[i], d_1e)], average over each spin to get [i: (..., 1, d_1e)], concatenate all averages for each spin to get [i: (..., 1, d_1e * nspins)], and apply a linear transformation to get [i: (..., 1, d')]. For 3) we split in_2e of shape (..., n_total, n_total, d_2e) over the spins along a particle axis to get [i: (..., n[i], n_total, d_2e)], average over the other particle axis to get [i: [j: (..., n[i], d_2e)]], concatenate the averages for each spin to get [i: (..., n[i], d_2e * nspins)], and apply a linear transformation to get [i: (..., n[i], d')]. Finally, for each spin, we add the three parts, each equivariant or symmetric, to get a final equivariant linear transformation of the inputs, to which a non-linearity is then applied and a skip connection optionally added. Args: in_1e (Array): array of shape (..., n_total, d_1e) in_2e (Array, optional): array of shape (..., n_total, n_total, d_2e). Defaults to None. Returns: Array of shape (..., n_total, self.ndense), the output one-electron stream \"\"\" dense_unmixed = self . _unmixed_dense ( in_1e ) dense_unmixed_split = jnp . split ( dense_unmixed , self . spin_split , axis =- 2 ) split_1e_means = _split_mean ( in_1e , self . spin_split , axis =- 2 , keepdims = True ) dense_mixed_split = self . _compute_transformed_1e_means ( split_1e_means ) # adds the unmixed [i: (..., n[i], d')] to the mixed [i: (..., 1, d')] to get # an equivariant function. Without the two-electron mixing, this is a spinful # version of DeepSet's Lemma 3: https://arxiv.org/pdf/1703.06114.pdf dense_out = tree_sum ( dense_unmixed_split , dense_mixed_split ) if in_2e is not None : dense_2e_split = self . _compute_transformed_2e_means ( in_2e ) dense_out = tree_sum ( dense_out , dense_2e_split ) dense_out_concat = jnp . concatenate ( dense_out , axis =- 2 ) nonlinear_out = self . _activation_fn ( dense_out_concat ) if self . skip_connection and _valid_skip ( in_1e , nonlinear_out ): nonlinear_out = self . skip_connection_scale * ( nonlinear_out + in_1e ) return nonlinear_out FermiNetOrbitalLayer ( Module ) dataclass Make the FermiNet orbitals (parallel linear layers with exp decay envelopes). Attributes: Name Type Description orbitals_split ParticleSplit number of pieces to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and orbitals_split = 2, then the input is split (5, 5). If nelec = 10, and orbitals_split = (2, 4), then the input is split into (2, 4, 4) -- note when orbitals_split is a sequence, there will be one more split than the length of the sequence. In the original use-case of spin-1/2 particles, split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. norbitals_per_split Sequence[int] sequence of integers specifying the number of orbitals to create for each split. This determines the output shapes for each split, i.e. the outputs are shaped (..., split_size[i], norbitals[i]) kernel_initializer_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a. setup ( self ) Setup envelope kernel initializers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup envelope kernel initializers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _kernel_initializer_envelope_dim = self . kernel_initializer_envelope_dim self . _kernel_initializer_envelope_ion = self . kernel_initializer_envelope_ion __call__ ( self , x , r_ei = None ) special Apply a dense layer R -> R^n for each split and multiply by exp envelopes. Parameters: Name Type Description Default x Array array of shape (..., nelec, d) required r_ei Array array of shape (..., nelec, nion, d) None Returns: Type Description [(..., nelec[i], self.norbitals_per_split[i])] list of FermiNet orbital matrices computed from an output stream x and the electron-ion displacements r_ei. Here n[i] is the number of particles in the ith split. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say for composing with the determinant anti-symmetry, nelec[i] should be equal to self.norbitals_per_split[i]. Source code in vmcnet/models/equivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , x : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Apply a dense layer R -> R^n for each split and multiply by exp envelopes. Args: x (Array): array of shape (..., nelec, d) r_ei (Array): array of shape (..., nelec, nion, d) Returns: [(..., nelec[i], self.norbitals_per_split[i])]: list of FermiNet orbital matrices computed from an output stream x and the electron-ion displacements r_ei. Here n[i] is the number of particles in the ith split. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say for composing with the determinant anti-symmetry, nelec[i] should be equal to self.norbitals_per_split[i]. \"\"\" orbs = SplitDense ( self . orbitals_split , self . norbitals_per_split , self . kernel_initializer_linear , self . bias_initializer_linear , use_bias = self . use_bias , )( x ) if r_ei is not None : exp_envelopes = _compute_exponential_envelopes_all_splits ( r_ei , self . orbitals_split , self . norbitals_per_split , self . _kernel_initializer_envelope_dim , self . _kernel_initializer_envelope_ion , self . isotropic_decay , ) orbs = tree_prod ( orbs , exp_envelopes ) return orbs FermiNetResidualBlock ( Module ) dataclass A single residual block in the FermiNet equivariant part. Combines the one-electron and two-electron streams. Attributes: Name Type Description one_electron_layer Callable function which takes in a previous one-electron stream output and two-electron stream output and mixes/transforms them to create a new one-electron stream output. Has the signature: (array of shape (..., n, d_1e), optional array of shape (..., n, n, d_2e)) -> array of shape (..., n, d_1e') two_electron_layer Callable function which takes in a previous two-electron stream output and batch applies a Dense layer along the last axis. Has the signature: array of shape (..., n, n, d_2e) -> array of shape (..., n, n, d_2e') setup ( self ) Setup called one- and two- electron layers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup called one- and two- electron layers.\"\"\" self . _one_electron_layer = self . one_electron_layer self . _two_electron_layer = self . two_electron_layer __call__ ( self , in_1e , in_2e = None ) special Apply the one-electron layer and optionally the two-electron layer. Parameters: Name Type Description Default in_1e Array array of shape (..., n_total, d_1e) required in_2e Array array of shape (..., n_total, n_total, d_2e). Defaults to None. None Returns: Type Description (Array, optional Array) tuple of (out_1e, out_2e) where out_1e is the output from the one-electron layer and out_2e is the output of the two-electron stream Source code in vmcnet/models/equivariance.py def __call__ ( # type: ignore[override] self , in_1e : Array , in_2e : Array = None ) -> Tuple [ Array , Optional [ Array ]]: \"\"\"Apply the one-electron layer and optionally the two-electron layer. Args: in_1e (Array): array of shape (..., n_total, d_1e) in_2e (Array, optional): array of shape (..., n_total, n_total, d_2e). Defaults to None. Returns: (Array, optional Array): tuple of (out_1e, out_2e) where out_1e is the output from the one-electron layer and out_2e is the output of the two-electron stream \"\"\" out_1e = self . _one_electron_layer ( in_1e , in_2e ) out_2e = in_2e if self . two_electron_layer is not None and in_2e is not None : out_2e = self . _two_electron_layer ( in_2e ) return out_1e , out_2e FermiNetTwoElectronLayer ( Module ) dataclass A single layer in the two-electron stream of the FermiNet equivariance. Attributes: Name Type Description ndense int number of dense nodes kernel_initializer WeightInitializer kernel initializer. Has signature (key, shape, dtype) -> Array bias_initializer WeightInitializer bias initializer. Has signature (key, shape, dtype) -> Array activation_fn Activation activation function. Has the signature Array -> Array (shape is preserved) use_bias bool whether to add a bias term. Defaults to True. skip_connection bool whether to add residual skip connections whenever the shapes of the input and output match. Defaults to True. skip_connection_scale float quantity to scale the final output by if a skip connection is added. Defaults to 1.0. setup ( self ) Setup Dense layer. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup Dense layer.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . _dense = Dense ( self . ndense , kernel_init = self . kernel_initializer , bias_init = self . bias_initializer , use_bias = self . use_bias , ) __call__ ( self , x ) special Apply a Dense layer in parallel to all electron pairs. The expected use-case of this is to batch apply a dense layer to an input x of shape (..., n_total, n_total, d), getting an output of shape (..., n_total, n_total, d'), and optionally adding a skip connection. The function itself is just a standard residual network layer. Source code in vmcnet/models/equivariance.py def __call__ ( self , x : Array ) -> Array : # type: ignore[override] \"\"\"Apply a Dense layer in parallel to all electron pairs. The expected use-case of this is to batch apply a dense layer to an input x of shape (..., n_total, n_total, d), getting an output of shape (..., n_total, n_total, d'), and optionally adding a skip connection. The function itself is just a standard residual network layer. \"\"\" dense_out = self . _dense ( x ) nonlinear_out = self . _activation_fn ( dense_out ) if self . skip_connection and _valid_skip ( x , nonlinear_out ): nonlinear_out = self . skip_connection_scale * ( nonlinear_out + x ) return nonlinear_out SplitDense ( Module ) dataclass Split input on the 2nd-to-last axis and apply unique Dense layers to each split. Attributes: Name Type Description split ParticleSplit number of pieces to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and split = 2, then the input is split (5, 5). If nelec = 10, and split = (2, 4), then the input is split into (2, 4, 4) -- note when split is a sequence, there will be one more split than the length of the sequence. In the original use-case of spin-1/2 particles, split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. ndense_per_split Sequence[int] sequence of integers specifying the number of dense nodes in the unique dense layer applied to each split of the input. This determines the output shapes for each split, i.e. the outputs are shaped (..., split_size[i], ndense[i]) kernel_initializer WeightInitializer kernel initializer. Has signature (key, shape, dtype) -> Array bias_initializer WeightInitializer bias initializer. Has signature (key, shape, dtype) -> Array. Defaults to random normal initialization. use_bias bool whether to add a bias term. Defaults to True. register_kfac bool whether to register the dense computations with KFAC. Defaults to True. setup ( self ) Set up the dense layers for each split. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Set up the dense layers for each split.\"\"\" nsplits = get_nsplits ( self . split ) if len ( self . ndense_per_split ) != nsplits : raise ValueError ( \"Incorrect number of dense output shapes specified for number of \" \"splits, should be one shape per split: shapes {} specified for the \" \"given split {} \" . format ( self . ndense_per_split , self . split ) ) self . _dense_layers = [ Dense ( self . ndense_per_split [ i ], kernel_init = self . kernel_initializer , bias_init = self . bias_initializer , use_bias = self . use_bias , register_kfac = self . register_kfac , ) for i in range ( nsplits ) ] __call__ ( self , x ) special Split the input and apply a dense layer to each split. Parameters: Name Type Description Default x Array array of shape (..., n, d) required Returns: Type Description [(..., n[i], self.ndense_per_split[i])] list of length nsplits, where nsplits is the number of splits created by jnp.split(x, self.split, axis=-2), and the ith entry of the output is the ith split transformed by a dense layer with self.ndense_per_split[i] nodes. Source code in vmcnet/models/equivariance.py def __call__ ( self , x : Array ) -> ArrayList : # type: ignore[override] \"\"\"Split the input and apply a dense layer to each split. Args: x (Array): array of shape (..., n, d) Returns: [(..., n[i], self.ndense_per_split[i])]: list of length nsplits, where nsplits is the number of splits created by jnp.split(x, self.split, axis=-2), and the ith entry of the output is the ith split transformed by a dense layer with self.ndense_per_split[i] nodes. \"\"\" x_split = jnp . split ( x , self . split , axis =- 2 ) return [ self . _dense_layers [ i ]( split ) for i , split in enumerate ( x_split )] compute_input_streams ( elec_pos , ion_pos = None , include_2e_stream = True , include_ei_norm = True , include_ee_norm = True ) Create input streams with electron and optionally ion data. If ion_pos is given, computes the electron-ion displacements (i.e. nuclear coordinates) and concatenates/flattens them along the ion dimension. If include_ei_norm is True, then the distances are also concatenated, so the map is elec_pos = (..., nelec, d) -> input_1e = (..., nelec, nion * (d + 1)). If include_2e_stream is True, then a two-electron stream of shape (..., nelec, nelec, d) is also computed and returned (otherwise None is returned). If include_ee_norm is True, then this becomes (..., nelec, nelec, d + 1) by concatenating pairwise distances onto the stream. Parameters: Name Type Description Default elec_pos Array electron positions of shape (..., nelec, d) required ion_pos Array locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. None include_2e_stream bool whether to compute pairwise electron displacements/distances. Defaults to True. True include_ei_norm bool whether to include electron-ion distances in the one-electron input. Defaults to True. True include_ee_norm bool whether to include electron-electron distances in the two-electron input. Defaults to True. True Returns: Type Description ( Array, Optional[Array], Optional[Array], Optional[Array], ) first output: one-electron input of shape (..., nelec, d'), where d' = d if ion_pos is None, d' = nion * d if ion_pos is given and include_ei_norm is False, and d' = nion * (d + 1) if ion_pos is given and include_ei_norm is True. second output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if include_ee_norm is False, and d' = d + 1 if include_ee_norm is True third output: electron-ion displacements of shape (..., nelec, nion, d) fourth output: electron-electron displacements of shape (..., nelec, nelec, d) If include_2e_stream is False, then the second and fourth outputs are None. If ion_pos is None, then the third output is None. Source code in vmcnet/models/equivariance.py def compute_input_streams ( elec_pos : Array , ion_pos : Array = None , include_2e_stream : bool = True , include_ei_norm : bool = True , include_ee_norm : bool = True , ) -> InputStreams : \"\"\"Create input streams with electron and optionally ion data. If `ion_pos` is given, computes the electron-ion displacements (i.e. nuclear coordinates) and concatenates/flattens them along the ion dimension. If `include_ei_norm` is True, then the distances are also concatenated, so the map is elec_pos = (..., nelec, d) -> input_1e = (..., nelec, nion * (d + 1)). If `include_2e_stream` is True, then a two-electron stream of shape (..., nelec, nelec, d) is also computed and returned (otherwise None is returned). If `include_ee_norm` is True, then this becomes (..., nelec, nelec, d + 1) by concatenating pairwise distances onto the stream. Args: elec_pos (Array): electron positions of shape (..., nelec, d) ion_pos (Array, optional): locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. include_2e_stream (bool, optional): whether to compute pairwise electron displacements/distances. Defaults to True. include_ei_norm (bool, optional): whether to include electron-ion distances in the one-electron input. Defaults to True. include_ee_norm (bool, optional): whether to include electron-electron distances in the two-electron input. Defaults to True. Returns: ( Array, Optional[Array], Optional[Array], Optional[Array], ): first output: one-electron input of shape (..., nelec, d'), where d' = d if `ion_pos` is None, d' = nion * d if `ion_pos` is given and `include_ei_norm` is False, and d' = nion * (d + 1) if `ion_pos` is given and `include_ei_norm` is True. second output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if `include_ee_norm` is False, and d' = d + 1 if `include_ee_norm` is True third output: electron-ion displacements of shape (..., nelec, nion, d) fourth output: electron-electron displacements of shape (..., nelec, nelec, d) If `include_2e_stream` is False, then the second and fourth outputs are None. If `ion_pos` is None, then the third output is None. \"\"\" input_1e , r_ei = compute_electron_ion ( elec_pos , ion_pos , include_ei_norm ) input_2e = None r_ee = None if include_2e_stream : input_2e , r_ee = compute_electron_electron ( elec_pos , include_ee_norm ) return input_1e , input_2e , r_ei , r_ee compute_electron_ion ( elec_pos , ion_pos = None , include_ei_norm = True ) Compute electron-ion displacements and optionally add on the distances. Parameters: Name Type Description Default elec_pos Array electron positions of shape (..., nelec, d) required ion_pos Array locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. None include_ei_norm bool whether to include electron-ion distances in the one-electron input. Defaults to True. True Returns: Type Description (Array, Optional[Array]) first output: one-electron input of shape (..., nelec, d'), where d' = d if ion_pos is None, d' = nion * d if ion_pos is given and include_ei_norm is False, and d' = nion * (d + 1) if ion_pos is given and include_ei_norm is True. second output: electron-ion displacements of shape (..., nelec, nion, d) If ion_pos is None, then the second output is None. Source code in vmcnet/models/equivariance.py def compute_electron_ion ( elec_pos : Array , ion_pos : Array = None , include_ei_norm : bool = True ) -> Tuple [ Array , Optional [ Array ]]: \"\"\"Compute electron-ion displacements and optionally add on the distances. Args: elec_pos (Array): electron positions of shape (..., nelec, d) ion_pos (Array, optional): locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. include_ei_norm (bool, optional): whether to include electron-ion distances in the one-electron input. Defaults to True. Returns: (Array, Optional[Array]): first output: one-electron input of shape (..., nelec, d'), where d' = d if `ion_pos` is None, d' = nion * d if `ion_pos` is given and `include_ei_norm` is False, and d' = nion * (d + 1) if `ion_pos` is given and `include_ei_norm` is True. second output: electron-ion displacements of shape (..., nelec, nion, d) If `ion_pos` is None, then the second output is None. \"\"\" r_ei = None input_1e = elec_pos if ion_pos is not None : r_ei = _compute_displacements ( input_1e , ion_pos ) input_1e = r_ei if include_ei_norm : input_norm = jnp . linalg . norm ( input_1e , axis =- 1 , keepdims = True ) input_with_norm = jnp . concatenate ([ input_1e , input_norm ], axis =- 1 ) input_1e = jnp . reshape ( input_with_norm , input_with_norm . shape [: - 2 ] + ( - 1 ,)) return input_1e , r_ei compute_electron_electron ( elec_pos , include_ee_norm = True ) Compute electron-electron displacements and optionally add on the distances. Parameters: Name Type Description Default elec_pos Array electron positions of shape (..., nelec, d) required include_ee_norm bool whether to include electron-electron distances in the two-electron input. Defaults to True. True Returns: Type Description (Array, Array) first output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if include_ee_norm is False, and d' = d + 1 if include_ee_norm is True second output: two-electron displacements of shape (..., nelec, nelec, d) Source code in vmcnet/models/equivariance.py def compute_electron_electron ( elec_pos : Array , include_ee_norm : bool = True ) -> Tuple [ Array , Array ]: \"\"\"Compute electron-electron displacements and optionally add on the distances. Args: elec_pos (Array): electron positions of shape (..., nelec, d) include_ee_norm (bool, optional): whether to include electron-electron distances in the two-electron input. Defaults to True. Returns: (Array, Array): first output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if `include_ee_norm` is False, and d' = d + 1 if `include_ee_norm` is True second output: two-electron displacements of shape (..., nelec, nelec, d) \"\"\" r_ee = _compute_displacements ( elec_pos , elec_pos ) input_2e = r_ee if include_ee_norm : r_ee_norm = compute_ee_norm_with_safe_diag ( r_ee ) input_2e = jnp . concatenate ([ input_2e , r_ee_norm ], axis =- 1 ) return input_2e , r_ee","title":"equivariance"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.DoublyEquivariantOrbitalLayer","text":"Equivariantly generate an orbital matrix corresponding to each input stream. The calculation being done here is a bit subtle, so it's worth explaining here in some detail. Let the equivariant input vectors to this layer be y_i. Then, this layer will generate an orbital matrix M_p for each particle P, such that the (i,j)th element of M_p satisfies M_(p,i,j) = phi_j(y_p, y_i). This is essentially the usual orbital matrix formula M_(i,j) = phi_j(y_i), except with an added dependence on the particle index p which allows us to generate a distinct matrix for each input particle. This construction allows us to generate a unique antisymmetric determinant D_p = det(M_p) for each input particle, which can then be the basis for an expressive antiequivariant layer. If r_ei is provided in addition to the main inputs y_i, then an exponentially decaying envelope is also applied equally to every orbital matrix M_p in order to ensure that the orbital values decay to zero far from the ions. Attributes: Name Type Description orbitals_split ParticleSplit number of pieces to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and orbitals_split = 2, then the input is split (5, 5). If nelec = 10, and orbitals_split = (2, 4), then the input is split into (2, 4, 4) -- note when orbitals_split is a sequence, there will be one more split than the length of the sequence. In the original use-case of spin-1/2 particles, split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. norbitals_per_split Sequence[int] sequence of integers specifying the number of orbitals to create for each split. This determines the output shapes for each split, i.e. the outputs are shaped (..., split_size[i], norbitals[i]) kernel_initializer_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a.","title":"DoublyEquivariantOrbitalLayer"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.DoublyEquivariantOrbitalLayer.setup","text":"Setup envelope kernel initializers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup envelope kernel initializers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _kernel_initializer_envelope_dim = self . kernel_initializer_envelope_dim self . _kernel_initializer_envelope_ion = self . kernel_initializer_envelope_ion","title":"setup()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.DoublyEquivariantOrbitalLayer.__call__","text":"Calculate an equivariant orbital matrix for each input particle. Parameters: Name Type Description Default x Array array of shape (..., nelec, d) required r_ei Array array of shape (..., nelec, nion, d) None Returns: Type Description (ArrayList) list of length nsplits of arrays of shape (..., nelec[i], nelec[i], self.norbitals_per_split[i]). Here nelec[i] is the number of particles in the ith split. The output arrays have both their -2 and -3 axes equivariant with respect to the input particles. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say in order to be able to take antiequivariant per-particle determinants, nelec[i] should be equal to self.norbitals_per_split[i]. Source code in vmcnet/models/equivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , x : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Calculate an equivariant orbital matrix for each input particle. Args: x (Array): array of shape (..., nelec, d) r_ei (Array): array of shape (..., nelec, nion, d) Returns: (ArrayList): list of length nsplits of arrays of shape (..., nelec[i], nelec[i], self.norbitals_per_split[i]). Here nelec[i] is the number of particles in the ith split. The output arrays have both their -2 and -3 axes equivariant with respect to the input particles. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say in order to be able to take antiequivariant per-particle determinants, nelec[i] should be equal to self.norbitals_per_split[i]. \"\"\" # split_x is a list of nsplits arrays of shape (..., nelec[i], d)] split_x = jnp . split ( x , self . orbitals_split , - 2 ) # orbs is a list of nsplits arrays of shape # (..., nelec[i], nelec[i], norbitals[i]) orbs = [ self . _get_orbital_matrices_one_split ( x , self . norbitals_per_split [ i ]) for ( i , x ) in enumerate ( split_x ) ] if r_ei is not None : exp_envelopes = _compute_exponential_envelopes_all_splits ( r_ei , self . orbitals_split , self . norbitals_per_split , self . _kernel_initializer_envelope_dim , self . _kernel_initializer_envelope_ion , self . isotropic_decay , ) # Envelope must be expanded to apply equally to each per-particle matrix. exp_envelopes = jax . tree_map ( lambda x : jnp . expand_dims ( x , axis =- 3 ), exp_envelopes ) orbs = tree_prod ( orbs , exp_envelopes ) return orbs","title":"__call__()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetBackflow","text":"The FermiNet equivariant part up until, but not including, the orbitals. Repeated composition of the residual blocks in the parallel one-electron and two-electron streams. Attributes: Name Type Description residual_blocks Sequence sequence of callable residual blocks which apply the one- and two- electron layers. Each residual block has the signature (in_1e, optional in_2e) -> (out_1e, optional out_2e), where in_1e has shape (..., n, d_1e) out_1e has shape (..., n, d_1e') in_2e has shape (..., n, n, d_2e) out_2d has shape (..., n, n, d_2e')","title":"FermiNetBackflow"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetBackflow.setup","text":"Setup called residual blocks. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup called residual blocks.\"\"\" self . _residual_block_list = [ block for block in self . residual_blocks ]","title":"setup()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetBackflow.__call__","text":"Iteratively apply residual blocks to Ferminet input streams. Parameters: Name Type Description Default stream_1e Array one-electron input stream of shape (..., nelec, d1). required stream_2e Array two-electron input of shape (..., nelec, nelec, d2). None Returns: Type Description (Array) the output of the one-electron stream after applying self.residual_blocks to the initial input streams. Source code in vmcnet/models/equivariance.py def __call__ ( # type: ignore[override] self , stream_1e : Array , stream_2e : Optional [ Array ] = None , ) -> Array : \"\"\"Iteratively apply residual blocks to Ferminet input streams. Args: stream_1e (Array): one-electron input stream of shape (..., nelec, d1). stream_2e (Array, optional): two-electron input of shape (..., nelec, nelec, d2). Returns: (Array): the output of the one-electron stream after applying self.residual_blocks to the initial input streams. \"\"\" for block in self . _residual_block_list : stream_1e , stream_2e = block ( stream_1e , stream_2e ) return stream_1e","title":"__call__()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetOneElectronLayer","text":"A single layer in the one-electron stream of the FermiNet equivariant part. Attributes: Name Type Description spin_split ParticleSplit number of spins to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and spin_split = 2, then the input is split (5, 5). If nelec = 10, and spin_split = (2, 4), then the input is split into (2, 4, 4) -- note when spin_split is a sequence, there will be one more spin than the length of the sequence. In the original use-case of spin-1/2 particles, spin_split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. ndense int number of dense nodes kernel_initializer_unmixed WeightInitializer kernel initializer for the unmixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_mixed WeightInitializer kernel initializer for the mixed part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous one-electron stream output. Has signature (key, shape, dtype) -> Array kernel_initializer_2e WeightInitializer kernel initializer for the two-electron part of the one-electron stream. This initializes the part of the dense kernel which multiplies the average of the previous two-electron stream which is mixed into the one-electron stream. Has signature (key, shape, dtype) -> Array bias_initializer WeightInitializer bias initializer. Has signature (key, shape, dtype) -> Array activation_fn Activation activation function. Has the signature Array -> Array (shape is preserved) use_bias bool whether to add a bias term. Defaults to True. skip_connection bool whether to add residual skip connections whenever the shapes of the input and output match. Defaults to True. skip_connection_scale float quantity to scale the final output by if a skip connection is added. Defaults to 1.0. cyclic_spins bool whether the the concatenation in the one-electron stream should satisfy a cyclic equivariance structure, i.e. if there are three spins (1, 2, 3), then in the mixed part of the stream, after averaging but before the linear transformation, cyclic equivariance means the inputs are [(1, 2, 3), (2, 3, 1), (3, 1, 2)]. If False, then the inputs are [(1, 2, 3), (1, 2, 3), (1, 2, 3)] (as in the original FermiNet). When there are only two spins (spin-1/2 case), then this is equivalent to true spin equivariance. Defaults to False (original FermiNet).","title":"FermiNetOneElectronLayer"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetOneElectronLayer.setup","text":"Setup Dense layers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup Dense layers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . _unmixed_dense = Dense ( self . ndense , kernel_init = self . kernel_initializer_unmixed , bias_init = self . bias_initializer , use_bias = self . use_bias , ) self . _mixed_dense = Dense ( self . ndense , kernel_init = self . kernel_initializer_mixed , use_bias = False ) self . _dense_2e = Dense ( self . ndense , kernel_init = self . kernel_initializer_2e , use_bias = False )","title":"setup()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetOneElectronLayer.__call__","text":"Add dense outputs on unmixed, mixed, and 2e terms to get the 1e output. This implementation breaks the one-electron stream into three parts: 1) the unmixed one-particle part, which is a linear transformation applied in parallel for each particle to the inputs 2) the mixed one-particle part, which is a linear transformation applied to the averages of the inputs (concatenated over spin) 3) the two-particle part, which is a linear transformation applied in parallel for each particle to the average of the input interactions between that particle and all the other particles. For 1), we take in_1e of shape (..., n_total, d_1e), batch apply a linear transformation to get (..., n_total, d'), and split over the spins i to get [i: (..., n[i], d')]. For 2), we split in_1e over the spins along the particle axis to get [i: (..., n[i], d_1e)], average over each spin to get [i: (..., 1, d_1e)], concatenate all averages for each spin to get [i: (..., 1, d_1e * nspins)], and apply a linear transformation to get [i: (..., 1, d')]. For 3) we split in_2e of shape (..., n_total, n_total, d_2e) over the spins along a particle axis to get [i: (..., n[i], n_total, d_2e)], average over the other particle axis to get [i: [j: (..., n[i], d_2e)]], concatenate the averages for each spin to get [i: (..., n[i], d_2e * nspins)], and apply a linear transformation to get [i: (..., n[i], d')]. Finally, for each spin, we add the three parts, each equivariant or symmetric, to get a final equivariant linear transformation of the inputs, to which a non-linearity is then applied and a skip connection optionally added. Parameters: Name Type Description Default in_1e Array array of shape (..., n_total, d_1e) required in_2e Array array of shape (..., n_total, n_total, d_2e). Defaults to None. None Returns: Type Description ndarray Array of shape (..., n_total, self.ndense), the output one-electron stream Source code in vmcnet/models/equivariance.py def __call__ ( # type: ignore[override] self , in_1e : Array , in_2e : Array = None ) -> Array : \"\"\"Add dense outputs on unmixed, mixed, and 2e terms to get the 1e output. This implementation breaks the one-electron stream into three parts: 1) the unmixed one-particle part, which is a linear transformation applied in parallel for each particle to the inputs 2) the mixed one-particle part, which is a linear transformation applied to the averages of the inputs (concatenated over spin) 3) the two-particle part, which is a linear transformation applied in parallel for each particle to the average of the input interactions between that particle and all the other particles. For 1), we take `in_1e` of shape (..., n_total, d_1e), batch apply a linear transformation to get (..., n_total, d'), and split over the spins i to get [i: (..., n[i], d')]. For 2), we split `in_1e` over the spins along the particle axis to get [i: (..., n[i], d_1e)], average over each spin to get [i: (..., 1, d_1e)], concatenate all averages for each spin to get [i: (..., 1, d_1e * nspins)], and apply a linear transformation to get [i: (..., 1, d')]. For 3) we split in_2e of shape (..., n_total, n_total, d_2e) over the spins along a particle axis to get [i: (..., n[i], n_total, d_2e)], average over the other particle axis to get [i: [j: (..., n[i], d_2e)]], concatenate the averages for each spin to get [i: (..., n[i], d_2e * nspins)], and apply a linear transformation to get [i: (..., n[i], d')]. Finally, for each spin, we add the three parts, each equivariant or symmetric, to get a final equivariant linear transformation of the inputs, to which a non-linearity is then applied and a skip connection optionally added. Args: in_1e (Array): array of shape (..., n_total, d_1e) in_2e (Array, optional): array of shape (..., n_total, n_total, d_2e). Defaults to None. Returns: Array of shape (..., n_total, self.ndense), the output one-electron stream \"\"\" dense_unmixed = self . _unmixed_dense ( in_1e ) dense_unmixed_split = jnp . split ( dense_unmixed , self . spin_split , axis =- 2 ) split_1e_means = _split_mean ( in_1e , self . spin_split , axis =- 2 , keepdims = True ) dense_mixed_split = self . _compute_transformed_1e_means ( split_1e_means ) # adds the unmixed [i: (..., n[i], d')] to the mixed [i: (..., 1, d')] to get # an equivariant function. Without the two-electron mixing, this is a spinful # version of DeepSet's Lemma 3: https://arxiv.org/pdf/1703.06114.pdf dense_out = tree_sum ( dense_unmixed_split , dense_mixed_split ) if in_2e is not None : dense_2e_split = self . _compute_transformed_2e_means ( in_2e ) dense_out = tree_sum ( dense_out , dense_2e_split ) dense_out_concat = jnp . concatenate ( dense_out , axis =- 2 ) nonlinear_out = self . _activation_fn ( dense_out_concat ) if self . skip_connection and _valid_skip ( in_1e , nonlinear_out ): nonlinear_out = self . skip_connection_scale * ( nonlinear_out + in_1e ) return nonlinear_out","title":"__call__()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetOrbitalLayer","text":"Make the FermiNet orbitals (parallel linear layers with exp decay envelopes). Attributes: Name Type Description orbitals_split ParticleSplit number of pieces to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and orbitals_split = 2, then the input is split (5, 5). If nelec = 10, and orbitals_split = (2, 4), then the input is split into (2, 4, 4) -- note when orbitals_split is a sequence, there will be one more split than the length of the sequence. In the original use-case of spin-1/2 particles, split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. norbitals_per_split Sequence[int] sequence of integers specifying the number of orbitals to create for each split. This determines the output shapes for each split, i.e. the outputs are shaped (..., split_size[i], norbitals[i]) kernel_initializer_linear WeightInitializer kernel initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_dim WeightInitializer kernel initializer for the decay rate in the exponential envelopes. If isotropic_decay is True, then this initializes a single decay rate number per ion and orbital. If isotropic_decay is False, then this initializes a 3x3 matrix per ion and orbital. Has signature (key, shape, dtype) -> Array kernel_initializer_envelope_ion WeightInitializer kernel initializer for the linear combination over the ions of exponential envelopes. Has signature (key, shape, dtype) -> Array bias_initializer_linear WeightInitializer bias initializer for the linear part of the orbitals. Has signature (key, shape, dtype) -> Array use_bias bool whether to add a bias term to the linear part of the orbitals. Defaults to True. isotropic_decay bool whether the decay for each ion should be anisotropic (w.r.t. the dimensions of the input), giving envelopes of the form exp(-||A(r - R)||) for a dxd matrix A or isotropic, giving exp(-||a(r - R||)) for a number a.","title":"FermiNetOrbitalLayer"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetOrbitalLayer.setup","text":"Setup envelope kernel initializers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup envelope kernel initializers.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _kernel_initializer_envelope_dim = self . kernel_initializer_envelope_dim self . _kernel_initializer_envelope_ion = self . kernel_initializer_envelope_ion","title":"setup()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetOrbitalLayer.__call__","text":"Apply a dense layer R -> R^n for each split and multiply by exp envelopes. Parameters: Name Type Description Default x Array array of shape (..., nelec, d) required r_ei Array array of shape (..., nelec, nion, d) None Returns: Type Description [(..., nelec[i], self.norbitals_per_split[i])] list of FermiNet orbital matrices computed from an output stream x and the electron-ion displacements r_ei. Here n[i] is the number of particles in the ith split. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say for composing with the determinant anti-symmetry, nelec[i] should be equal to self.norbitals_per_split[i]. Source code in vmcnet/models/equivariance.py @flax . linen . compact def __call__ ( # type: ignore[override] self , x : Array , r_ei : Array = None ) -> ArrayList : \"\"\"Apply a dense layer R -> R^n for each split and multiply by exp envelopes. Args: x (Array): array of shape (..., nelec, d) r_ei (Array): array of shape (..., nelec, nion, d) Returns: [(..., nelec[i], self.norbitals_per_split[i])]: list of FermiNet orbital matrices computed from an output stream x and the electron-ion displacements r_ei. Here n[i] is the number of particles in the ith split. The exponential envelopes are computed only when r_ei is not None (so, when connected to FermiNetBackflow, when ion locations are specified). To output square matrices, say for composing with the determinant anti-symmetry, nelec[i] should be equal to self.norbitals_per_split[i]. \"\"\" orbs = SplitDense ( self . orbitals_split , self . norbitals_per_split , self . kernel_initializer_linear , self . bias_initializer_linear , use_bias = self . use_bias , )( x ) if r_ei is not None : exp_envelopes = _compute_exponential_envelopes_all_splits ( r_ei , self . orbitals_split , self . norbitals_per_split , self . _kernel_initializer_envelope_dim , self . _kernel_initializer_envelope_ion , self . isotropic_decay , ) orbs = tree_prod ( orbs , exp_envelopes ) return orbs","title":"__call__()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetResidualBlock","text":"A single residual block in the FermiNet equivariant part. Combines the one-electron and two-electron streams. Attributes: Name Type Description one_electron_layer Callable function which takes in a previous one-electron stream output and two-electron stream output and mixes/transforms them to create a new one-electron stream output. Has the signature: (array of shape (..., n, d_1e), optional array of shape (..., n, n, d_2e)) -> array of shape (..., n, d_1e') two_electron_layer Callable function which takes in a previous two-electron stream output and batch applies a Dense layer along the last axis. Has the signature: array of shape (..., n, n, d_2e) -> array of shape (..., n, n, d_2e')","title":"FermiNetResidualBlock"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetResidualBlock.setup","text":"Setup called one- and two- electron layers. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup called one- and two- electron layers.\"\"\" self . _one_electron_layer = self . one_electron_layer self . _two_electron_layer = self . two_electron_layer","title":"setup()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetResidualBlock.__call__","text":"Apply the one-electron layer and optionally the two-electron layer. Parameters: Name Type Description Default in_1e Array array of shape (..., n_total, d_1e) required in_2e Array array of shape (..., n_total, n_total, d_2e). Defaults to None. None Returns: Type Description (Array, optional Array) tuple of (out_1e, out_2e) where out_1e is the output from the one-electron layer and out_2e is the output of the two-electron stream Source code in vmcnet/models/equivariance.py def __call__ ( # type: ignore[override] self , in_1e : Array , in_2e : Array = None ) -> Tuple [ Array , Optional [ Array ]]: \"\"\"Apply the one-electron layer and optionally the two-electron layer. Args: in_1e (Array): array of shape (..., n_total, d_1e) in_2e (Array, optional): array of shape (..., n_total, n_total, d_2e). Defaults to None. Returns: (Array, optional Array): tuple of (out_1e, out_2e) where out_1e is the output from the one-electron layer and out_2e is the output of the two-electron stream \"\"\" out_1e = self . _one_electron_layer ( in_1e , in_2e ) out_2e = in_2e if self . two_electron_layer is not None and in_2e is not None : out_2e = self . _two_electron_layer ( in_2e ) return out_1e , out_2e","title":"__call__()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetTwoElectronLayer","text":"A single layer in the two-electron stream of the FermiNet equivariance. Attributes: Name Type Description ndense int number of dense nodes kernel_initializer WeightInitializer kernel initializer. Has signature (key, shape, dtype) -> Array bias_initializer WeightInitializer bias initializer. Has signature (key, shape, dtype) -> Array activation_fn Activation activation function. Has the signature Array -> Array (shape is preserved) use_bias bool whether to add a bias term. Defaults to True. skip_connection bool whether to add residual skip connections whenever the shapes of the input and output match. Defaults to True. skip_connection_scale float quantity to scale the final output by if a skip connection is added. Defaults to 1.0.","title":"FermiNetTwoElectronLayer"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetTwoElectronLayer.setup","text":"Setup Dense layer. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Setup Dense layer.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _activation_fn = self . activation_fn self . _dense = Dense ( self . ndense , kernel_init = self . kernel_initializer , bias_init = self . bias_initializer , use_bias = self . use_bias , )","title":"setup()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.FermiNetTwoElectronLayer.__call__","text":"Apply a Dense layer in parallel to all electron pairs. The expected use-case of this is to batch apply a dense layer to an input x of shape (..., n_total, n_total, d), getting an output of shape (..., n_total, n_total, d'), and optionally adding a skip connection. The function itself is just a standard residual network layer. Source code in vmcnet/models/equivariance.py def __call__ ( self , x : Array ) -> Array : # type: ignore[override] \"\"\"Apply a Dense layer in parallel to all electron pairs. The expected use-case of this is to batch apply a dense layer to an input x of shape (..., n_total, n_total, d), getting an output of shape (..., n_total, n_total, d'), and optionally adding a skip connection. The function itself is just a standard residual network layer. \"\"\" dense_out = self . _dense ( x ) nonlinear_out = self . _activation_fn ( dense_out ) if self . skip_connection and _valid_skip ( x , nonlinear_out ): nonlinear_out = self . skip_connection_scale * ( nonlinear_out + x ) return nonlinear_out","title":"__call__()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.SplitDense","text":"Split input on the 2nd-to-last axis and apply unique Dense layers to each split. Attributes: Name Type Description split ParticleSplit number of pieces to split the input equally, or specified sequence of locations to split along the 2nd-to-last axis. E.g., if nelec = 10, and split = 2, then the input is split (5, 5). If nelec = 10, and split = (2, 4), then the input is split into (2, 4, 4) -- note when split is a sequence, there will be one more split than the length of the sequence. In the original use-case of spin-1/2 particles, split should be either the number 2 (for closed-shell systems) or should be a Sequence with length 1 whose element is less than the total number of electrons. ndense_per_split Sequence[int] sequence of integers specifying the number of dense nodes in the unique dense layer applied to each split of the input. This determines the output shapes for each split, i.e. the outputs are shaped (..., split_size[i], ndense[i]) kernel_initializer WeightInitializer kernel initializer. Has signature (key, shape, dtype) -> Array bias_initializer WeightInitializer bias initializer. Has signature (key, shape, dtype) -> Array. Defaults to random normal initialization. use_bias bool whether to add a bias term. Defaults to True. register_kfac bool whether to register the dense computations with KFAC. Defaults to True.","title":"SplitDense"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.SplitDense.setup","text":"Set up the dense layers for each split. Source code in vmcnet/models/equivariance.py def setup ( self ): \"\"\"Set up the dense layers for each split.\"\"\" nsplits = get_nsplits ( self . split ) if len ( self . ndense_per_split ) != nsplits : raise ValueError ( \"Incorrect number of dense output shapes specified for number of \" \"splits, should be one shape per split: shapes {} specified for the \" \"given split {} \" . format ( self . ndense_per_split , self . split ) ) self . _dense_layers = [ Dense ( self . ndense_per_split [ i ], kernel_init = self . kernel_initializer , bias_init = self . bias_initializer , use_bias = self . use_bias , register_kfac = self . register_kfac , ) for i in range ( nsplits ) ]","title":"setup()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.SplitDense.__call__","text":"Split the input and apply a dense layer to each split. Parameters: Name Type Description Default x Array array of shape (..., n, d) required Returns: Type Description [(..., n[i], self.ndense_per_split[i])] list of length nsplits, where nsplits is the number of splits created by jnp.split(x, self.split, axis=-2), and the ith entry of the output is the ith split transformed by a dense layer with self.ndense_per_split[i] nodes. Source code in vmcnet/models/equivariance.py def __call__ ( self , x : Array ) -> ArrayList : # type: ignore[override] \"\"\"Split the input and apply a dense layer to each split. Args: x (Array): array of shape (..., n, d) Returns: [(..., n[i], self.ndense_per_split[i])]: list of length nsplits, where nsplits is the number of splits created by jnp.split(x, self.split, axis=-2), and the ith entry of the output is the ith split transformed by a dense layer with self.ndense_per_split[i] nodes. \"\"\" x_split = jnp . split ( x , self . split , axis =- 2 ) return [ self . _dense_layers [ i ]( split ) for i , split in enumerate ( x_split )]","title":"__call__()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.compute_input_streams","text":"Create input streams with electron and optionally ion data. If ion_pos is given, computes the electron-ion displacements (i.e. nuclear coordinates) and concatenates/flattens them along the ion dimension. If include_ei_norm is True, then the distances are also concatenated, so the map is elec_pos = (..., nelec, d) -> input_1e = (..., nelec, nion * (d + 1)). If include_2e_stream is True, then a two-electron stream of shape (..., nelec, nelec, d) is also computed and returned (otherwise None is returned). If include_ee_norm is True, then this becomes (..., nelec, nelec, d + 1) by concatenating pairwise distances onto the stream. Parameters: Name Type Description Default elec_pos Array electron positions of shape (..., nelec, d) required ion_pos Array locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. None include_2e_stream bool whether to compute pairwise electron displacements/distances. Defaults to True. True include_ei_norm bool whether to include electron-ion distances in the one-electron input. Defaults to True. True include_ee_norm bool whether to include electron-electron distances in the two-electron input. Defaults to True. True Returns: Type Description ( Array, Optional[Array], Optional[Array], Optional[Array], ) first output: one-electron input of shape (..., nelec, d'), where d' = d if ion_pos is None, d' = nion * d if ion_pos is given and include_ei_norm is False, and d' = nion * (d + 1) if ion_pos is given and include_ei_norm is True. second output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if include_ee_norm is False, and d' = d + 1 if include_ee_norm is True third output: electron-ion displacements of shape (..., nelec, nion, d) fourth output: electron-electron displacements of shape (..., nelec, nelec, d) If include_2e_stream is False, then the second and fourth outputs are None. If ion_pos is None, then the third output is None. Source code in vmcnet/models/equivariance.py def compute_input_streams ( elec_pos : Array , ion_pos : Array = None , include_2e_stream : bool = True , include_ei_norm : bool = True , include_ee_norm : bool = True , ) -> InputStreams : \"\"\"Create input streams with electron and optionally ion data. If `ion_pos` is given, computes the electron-ion displacements (i.e. nuclear coordinates) and concatenates/flattens them along the ion dimension. If `include_ei_norm` is True, then the distances are also concatenated, so the map is elec_pos = (..., nelec, d) -> input_1e = (..., nelec, nion * (d + 1)). If `include_2e_stream` is True, then a two-electron stream of shape (..., nelec, nelec, d) is also computed and returned (otherwise None is returned). If `include_ee_norm` is True, then this becomes (..., nelec, nelec, d + 1) by concatenating pairwise distances onto the stream. Args: elec_pos (Array): electron positions of shape (..., nelec, d) ion_pos (Array, optional): locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. include_2e_stream (bool, optional): whether to compute pairwise electron displacements/distances. Defaults to True. include_ei_norm (bool, optional): whether to include electron-ion distances in the one-electron input. Defaults to True. include_ee_norm (bool, optional): whether to include electron-electron distances in the two-electron input. Defaults to True. Returns: ( Array, Optional[Array], Optional[Array], Optional[Array], ): first output: one-electron input of shape (..., nelec, d'), where d' = d if `ion_pos` is None, d' = nion * d if `ion_pos` is given and `include_ei_norm` is False, and d' = nion * (d + 1) if `ion_pos` is given and `include_ei_norm` is True. second output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if `include_ee_norm` is False, and d' = d + 1 if `include_ee_norm` is True third output: electron-ion displacements of shape (..., nelec, nion, d) fourth output: electron-electron displacements of shape (..., nelec, nelec, d) If `include_2e_stream` is False, then the second and fourth outputs are None. If `ion_pos` is None, then the third output is None. \"\"\" input_1e , r_ei = compute_electron_ion ( elec_pos , ion_pos , include_ei_norm ) input_2e = None r_ee = None if include_2e_stream : input_2e , r_ee = compute_electron_electron ( elec_pos , include_ee_norm ) return input_1e , input_2e , r_ei , r_ee","title":"compute_input_streams()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.compute_electron_ion","text":"Compute electron-ion displacements and optionally add on the distances. Parameters: Name Type Description Default elec_pos Array electron positions of shape (..., nelec, d) required ion_pos Array locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. None include_ei_norm bool whether to include electron-ion distances in the one-electron input. Defaults to True. True Returns: Type Description (Array, Optional[Array]) first output: one-electron input of shape (..., nelec, d'), where d' = d if ion_pos is None, d' = nion * d if ion_pos is given and include_ei_norm is False, and d' = nion * (d + 1) if ion_pos is given and include_ei_norm is True. second output: electron-ion displacements of shape (..., nelec, nion, d) If ion_pos is None, then the second output is None. Source code in vmcnet/models/equivariance.py def compute_electron_ion ( elec_pos : Array , ion_pos : Array = None , include_ei_norm : bool = True ) -> Tuple [ Array , Optional [ Array ]]: \"\"\"Compute electron-ion displacements and optionally add on the distances. Args: elec_pos (Array): electron positions of shape (..., nelec, d) ion_pos (Array, optional): locations of (stationary) ions to compute relative electron positions, 2-d array of shape (nion, d). Defaults to None. include_ei_norm (bool, optional): whether to include electron-ion distances in the one-electron input. Defaults to True. Returns: (Array, Optional[Array]): first output: one-electron input of shape (..., nelec, d'), where d' = d if `ion_pos` is None, d' = nion * d if `ion_pos` is given and `include_ei_norm` is False, and d' = nion * (d + 1) if `ion_pos` is given and `include_ei_norm` is True. second output: electron-ion displacements of shape (..., nelec, nion, d) If `ion_pos` is None, then the second output is None. \"\"\" r_ei = None input_1e = elec_pos if ion_pos is not None : r_ei = _compute_displacements ( input_1e , ion_pos ) input_1e = r_ei if include_ei_norm : input_norm = jnp . linalg . norm ( input_1e , axis =- 1 , keepdims = True ) input_with_norm = jnp . concatenate ([ input_1e , input_norm ], axis =- 1 ) input_1e = jnp . reshape ( input_with_norm , input_with_norm . shape [: - 2 ] + ( - 1 ,)) return input_1e , r_ei","title":"compute_electron_ion()"},{"location":"api/models/equivariance/#vmcnet.models.equivariance.compute_electron_electron","text":"Compute electron-electron displacements and optionally add on the distances. Parameters: Name Type Description Default elec_pos Array electron positions of shape (..., nelec, d) required include_ee_norm bool whether to include electron-electron distances in the two-electron input. Defaults to True. True Returns: Type Description (Array, Array) first output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if include_ee_norm is False, and d' = d + 1 if include_ee_norm is True second output: two-electron displacements of shape (..., nelec, nelec, d) Source code in vmcnet/models/equivariance.py def compute_electron_electron ( elec_pos : Array , include_ee_norm : bool = True ) -> Tuple [ Array , Array ]: \"\"\"Compute electron-electron displacements and optionally add on the distances. Args: elec_pos (Array): electron positions of shape (..., nelec, d) include_ee_norm (bool, optional): whether to include electron-electron distances in the two-electron input. Defaults to True. Returns: (Array, Array): first output: two-electron input of shape (..., nelec, nelec, d'), where d' = d if `include_ee_norm` is False, and d' = d + 1 if `include_ee_norm` is True second output: two-electron displacements of shape (..., nelec, nelec, d) \"\"\" r_ee = _compute_displacements ( elec_pos , elec_pos ) input_2e = r_ee if include_ee_norm : r_ee_norm = compute_ee_norm_with_safe_diag ( r_ee ) input_2e = jnp . concatenate ([ input_2e , r_ee_norm ], axis =- 1 ) return input_2e , r_ee","title":"compute_electron_electron()"},{"location":"api/models/jastrow/","text":"Jastrow factors. BackflowJastrow ( Module ) dataclass Backflow-based general permutation invariant Jastrow. Attributes: Name Type Description backflow Callable or None function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d'). Can pass None here to use a stream_1e from an already computed backflow. logabs bool whether to return the log jastrow (True) or the jastrow (False). Defaults to True. setup ( self ) Set up the dense layers for each split. Source code in vmcnet/models/jastrow.py def setup ( self ): \"\"\"Set up the dense layers for each split.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _backflow = self . backflow __call__ ( self , input_stream_1e , input_stream_2e , stream_1e , r_ei , r_ee ) special Compute backflow-based general permutation invariant Jastrow. Parameters: Name Type Description Default input_stream_1e Array input one-electron stream required input_stream_2e Array input two-electron stream required stream_1e Array one-electron stream, post-backflow required r_ei Array electron-ion displacements with shape (..., nelec, nion, d); unused required r_ee Array electron-electron displacements with shape (..., nelec, nelec, d); unused required Returns: Type Description Array -mean_i ||Backflow_i||, or exp(-mean_i ||Backflow_i||) if logabs is False Source code in vmcnet/models/jastrow.py @flax . linen . compact def __call__ ( # type: ignore[override] self , input_stream_1e : Array , input_stream_2e : Array , stream_1e : Array , r_ei : Array , r_ee : Array , ) -> Array : \"\"\"Compute backflow-based general permutation invariant Jastrow. Args: input_stream_1e (Array): input one-electron stream input_stream_2e (Array): input two-electron stream stream_1e (Array): one-electron stream, post-backflow r_ei (Array): electron-ion displacements with shape (..., nelec, nion, d); unused r_ee (Array): electron-electron displacements with shape (..., nelec, nelec, d); unused Returns: Array: -mean_i ||Backflow_i||, or exp(-mean_i ||Backflow_i||) if logabs is False \"\"\" del r_ei , r_ee if self . _backflow is not None : stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) log_jastrow = - jnp . mean ( jnp . linalg . norm ( stream_1e , axis =- 1 ), axis =- 1 ) if self . logabs : return log_jastrow return jnp . exp ( log_jastrow ) OneBodyExpDecay ( Module ) dataclass Creates an isotropic exponential decay one-body Jastrow model. The decay is centered at the coordinates of the nuclei, and the electron-nuclei displacements are multiplied by trainable params before a sum and exp(-x). The decay is isotropic and equal for all electrons, so it computes -sum_ij ||a_j * (elec_i - ion_j)|| or the exponential if logabs is False. The tensor a_j * (elec_i - ion_j) is computed with a split dense operation. Attributes: Name Type Description kernel_initializer WeightInitializer kernel initializer for the decay rates a_j. This initializes a single decay rate number per ion. Has signature (key, shape, dtype) -> Array logabs bool whether to compute -sum_ij ||a_j * (elec_i - ion_j)||, when logabs is True, or exp of that expression when logabs is False. Defaults to True. setup ( self ) Setup the kernel initializer. Source code in vmcnet/models/jastrow.py def setup ( self ): \"\"\"Setup the kernel initializer.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _kernel_initializer = self . kernel_initializer __call__ ( self , input_stream_1e , input_stream_2e , stream_1e , r_ei , r_ee ) special Transform electron-ion displacements into an exp decay one-body Jastrow. Parameters: Name Type Description Default input_stream_1e Array input one-electron stream; unused required input_stream_2e Array input two-electron stream; unused required stream_1e Array one-electron stream, post-backflow; unused required r_ei Array electron-ion displacements of shape (..., nelec, nion, d) required r_ee Array electron-electron displacements of shape (..., nelec, nelec, d); unused required Returns: Type Description Array -sum_ij ||a_j * (elec_i - ion_j)||, when self.logabs is True, or exp of that expression when self.logabs is False. If the input has shape (batch_dims, nelec, nion, d), then the output has shape (batch_dims,) Source code in vmcnet/models/jastrow.py @flax . linen . compact def __call__ ( # type: ignore[override] self , input_stream_1e : Array , input_stream_2e : Array , stream_1e : Array , r_ei : Array , r_ee : Array , ) -> Array : \"\"\"Transform electron-ion displacements into an exp decay one-body Jastrow. Args: input_stream_1e (Array): input one-electron stream; unused input_stream_2e (Array): input two-electron stream; unused stream_1e (Array): one-electron stream, post-backflow; unused r_ei (Array): electron-ion displacements of shape (..., nelec, nion, d) r_ee (Array): electron-electron displacements of shape (..., nelec, nelec, d); unused Returns: Array: -sum_ij ||a_j * (elec_i - ion_j)||, when self.logabs is True, or exp of that expression when self.logabs is False. If the input has shape (batch_dims, nelec, nion, d), then the output has shape (batch_dims,) \"\"\" del input_stream_1e , input_stream_2e , stream_1e , r_ee # scale_out has shape (..., nelec, 1, nion, d) scale_out = _isotropy_on_leaf ( r_ei , 1 , self . _kernel_initializer , register_kfac = True ) scaled_distances = jnp . linalg . norm ( scale_out , axis =- 1 ) abs_lin_comb_distances = jnp . sum ( scaled_distances , axis = ( - 1 , - 2 , - 3 )) if self . logabs : return - abs_lin_comb_distances return jnp . exp ( - abs_lin_comb_distances ) TwoBodyExpDecay ( Module ) dataclass Isotropic exponential decay two-body Jastrow model. The decay is isotropic in the sense that each electron-nuclei and electron-electron term is isotropic, i.e. radially symmetric. The computed interactions are: sum_i(-sum_j Z_j ||elec_i - ion_j|| + sum_k Q ||elec_i - elec_k||) or the exponential if logabs is False. Z_j and Q are initialized to init_ei_strength and init_ee_strength, respectively, and are trainable if trainable is True. Attributes: Name Type Description init_ei_strength Array or Sequence[float] 1-d array or sequence of length nion which gives the initial strength of the electron-nucleus interaction per ion init_ee_strength float initial strength of the electron-electron interaction. Defaults to 1.0. log_scale_factor float Amount to add to the log jastrow (amounts to a multiplicative factor after exponentiation). Defaults to 0.0. register_kfac bool whether to register the computation with KFAC. Defaults to True. logabs bool whether to return the log jastrow (True) or the jastrow (False). Defaults to True. trainable bool whether to allow the jastrow to be trainable. Defaults to True. __call__ ( self , input_stream_1e , input_stream_2e , stream_1e , r_ei , r_ee ) special Compute jastrow with both electron-ion and electron-electron effects. Parameters: Name Type Description Default input_stream_1e Array input one-electron stream; unused required input_stream_2e Array input two-electron stream; unused required stream_1e Array one-electron stream, post-backflow; unused required r_ei Array electron-ion displacements with shape (..., nelec, nion, d) required r_ee Array electron-electron displacements with shape (..., nelec, nelec, d) required Returns: Type Description Array sum_i(-sum_j Z_j ||elec_i - ion_j|| + sum_k Q ||elec_i - elec_k||), where Z_j and Q are trainable if trainable is true, and an exponential is taken if logabs is False Source code in vmcnet/models/jastrow.py @flax . linen . compact def __call__ ( # type: ignore[override] self , input_stream_1e : Array , input_stream_2e : Array , stream_1e : Array , r_ei : Array , r_ee : Array , ) -> Array : \"\"\"Compute jastrow with both electron-ion and electron-electron effects. Args: input_stream_1e (Array): input one-electron stream; unused input_stream_2e (Array): input two-electron stream; unused stream_1e (Array): one-electron stream, post-backflow; unused r_ei (Array): electron-ion displacements with shape (..., nelec, nion, d) r_ee (Array): electron-electron displacements with shape (..., nelec, nelec, d) Returns: Array: sum_i(-sum_j Z_j ||elec_i - ion_j|| + sum_k Q ||elec_i - elec_k||), where Z_j and Q are trainable if trainable is true, and an exponential is taken if logabs is False \"\"\" del input_stream_1e , input_stream_2e , stream_1e ei_distances = jnp . linalg . norm ( r_ei , axis =- 1 ) ee_distances = jnp . squeeze ( compute_ee_norm_with_safe_diag ( r_ee ), axis =- 1 ) sum_ee_effect = jnp . sum ( jnp . triu ( ee_distances ), axis =- 1 , keepdims = True ) if self . trainable : split_over_ions = jnp . split ( ei_distances , ei_distances . shape [ - 1 ], axis =- 1 ) # TODO: potentially add support for this to SplitDense or otherwise? split_scaled_ei_distances = [ Dense ( 1 , kernel_init = get_constant_init ( self . init_ei_strength [ i ]), use_bias = False , register_kfac = self . register_kfac , )( single_ion_displacement ) for i , single_ion_displacement in enumerate ( split_over_ions ) ] scaled_ei_distances = jnp . concatenate ( split_scaled_ei_distances , axis =- 1 ) sum_ee_effect = Dense ( 1 , kernel_init = get_constant_init ( self . init_ee_strength ), use_bias = False , register_kfac = self . register_kfac , )( sum_ee_effect ) else : scaled_ei_distances = self . init_ei_strength * ei_distances sum_ee_effect = self . init_ee_strength * sum_ee_effect sum_ee_effect = jnp . squeeze ( sum_ee_effect , axis =- 1 ) sum_ei_effect = jnp . sum ( scaled_ei_distances , axis =- 1 ) unscaled_interaction = jnp . sum ( sum_ee_effect - sum_ei_effect , axis =- 1 ) interaction = unscaled_interaction + self . log_scale_factor if self . logabs : return interaction return jnp . exp ( interaction ) get_two_body_decay_scaled_for_chargeless_molecules ( ion_pos , ion_charges , init_ee_strength = 1.0 , register_kfac = True , logabs = True , trainable = True ) Make molecular decay jastrow, scaled for chargeless molecules. The scale factor is chosen so that the log jastrow is initialized to 0 when electrons are at ion positions. Parameters: Name Type Description Default ion_pos Array an (nion, d) array of ion positions. required ion_charges Array an (nion,) array of ion charges, in units of one elementary charge (the charge of one electron) required init_ee_strength float the initial strength of the electron-electron interaction. Defaults to 1.0. 1.0 register_kfac bool whether to register the computation with KFAC. Defaults to True. True logabs bool whether to return the log jastrow (True) or the jastrow (False). Defaults to True. True trainable bool whether to allow the jastrow to be trainable. Defaults to True. True Returns: Type Description Callable a flax Module with signature (r_ei, r_ee) -> jastrow or log jastrow Source code in vmcnet/models/jastrow.py def get_two_body_decay_scaled_for_chargeless_molecules ( ion_pos : Array , ion_charges : Array , init_ee_strength : float = 1.0 , register_kfac : bool = True , logabs : bool = True , trainable : bool = True , ) -> Jastrow : \"\"\"Make molecular decay jastrow, scaled for chargeless molecules. The scale factor is chosen so that the log jastrow is initialized to 0 when electrons are at ion positions. Args: ion_pos (Array): an (nion, d) array of ion positions. ion_charges (Array): an (nion,) array of ion charges, in units of one elementary charge (the charge of one electron) init_ee_strength (float, optional): the initial strength of the electron-electron interaction. Defaults to 1.0. register_kfac (bool, optional): whether to register the computation with KFAC. Defaults to True. logabs (bool, optional): whether to return the log jastrow (True) or the jastrow (False). Defaults to True. trainable (bool, optional): whether to allow the jastrow to be trainable. Defaults to True. Returns: Callable: a flax Module with signature (r_ei, r_ee) -> jastrow or log jastrow \"\"\" r_ii , charge_charge_prods = physics . potential . _get_ion_ion_info ( ion_pos , ion_charges ) jastrow_scale_factor = 0.5 * jnp . sum ( jnp . linalg . norm ( r_ii , axis =- 1 ) * charge_charge_prods ) jastrow = TwoBodyExpDecay ( ion_charges , init_ee_strength , log_scale_factor = jastrow_scale_factor , register_kfac = register_kfac , logabs = logabs , trainable = trainable , ) return jastrow","title":"jastrow"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.BackflowJastrow","text":"Backflow-based general permutation invariant Jastrow. Attributes: Name Type Description backflow Callable or None function which computes position features from the electron positions. Has the signature ( stream_1e of shape (..., n, d'), optional stream_2e of shape (..., nelec, nelec, d2), ) -> stream_1e of shape (..., n, d'). Can pass None here to use a stream_1e from an already computed backflow. logabs bool whether to return the log jastrow (True) or the jastrow (False). Defaults to True.","title":"BackflowJastrow"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.BackflowJastrow.setup","text":"Set up the dense layers for each split. Source code in vmcnet/models/jastrow.py def setup ( self ): \"\"\"Set up the dense layers for each split.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _backflow = self . backflow","title":"setup()"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.BackflowJastrow.__call__","text":"Compute backflow-based general permutation invariant Jastrow. Parameters: Name Type Description Default input_stream_1e Array input one-electron stream required input_stream_2e Array input two-electron stream required stream_1e Array one-electron stream, post-backflow required r_ei Array electron-ion displacements with shape (..., nelec, nion, d); unused required r_ee Array electron-electron displacements with shape (..., nelec, nelec, d); unused required Returns: Type Description Array -mean_i ||Backflow_i||, or exp(-mean_i ||Backflow_i||) if logabs is False Source code in vmcnet/models/jastrow.py @flax . linen . compact def __call__ ( # type: ignore[override] self , input_stream_1e : Array , input_stream_2e : Array , stream_1e : Array , r_ei : Array , r_ee : Array , ) -> Array : \"\"\"Compute backflow-based general permutation invariant Jastrow. Args: input_stream_1e (Array): input one-electron stream input_stream_2e (Array): input two-electron stream stream_1e (Array): one-electron stream, post-backflow r_ei (Array): electron-ion displacements with shape (..., nelec, nion, d); unused r_ee (Array): electron-electron displacements with shape (..., nelec, nelec, d); unused Returns: Array: -mean_i ||Backflow_i||, or exp(-mean_i ||Backflow_i||) if logabs is False \"\"\" del r_ei , r_ee if self . _backflow is not None : stream_1e = self . _backflow ( input_stream_1e , input_stream_2e ) log_jastrow = - jnp . mean ( jnp . linalg . norm ( stream_1e , axis =- 1 ), axis =- 1 ) if self . logabs : return log_jastrow return jnp . exp ( log_jastrow )","title":"__call__()"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.OneBodyExpDecay","text":"Creates an isotropic exponential decay one-body Jastrow model. The decay is centered at the coordinates of the nuclei, and the electron-nuclei displacements are multiplied by trainable params before a sum and exp(-x). The decay is isotropic and equal for all electrons, so it computes -sum_ij ||a_j * (elec_i - ion_j)|| or the exponential if logabs is False. The tensor a_j * (elec_i - ion_j) is computed with a split dense operation. Attributes: Name Type Description kernel_initializer WeightInitializer kernel initializer for the decay rates a_j. This initializes a single decay rate number per ion. Has signature (key, shape, dtype) -> Array logabs bool whether to compute -sum_ij ||a_j * (elec_i - ion_j)||, when logabs is True, or exp of that expression when logabs is False. Defaults to True.","title":"OneBodyExpDecay"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.OneBodyExpDecay.setup","text":"Setup the kernel initializer. Source code in vmcnet/models/jastrow.py def setup ( self ): \"\"\"Setup the kernel initializer.\"\"\" # workaround MyPy's typing error for callable attribute, see # https://github.com/python/mypy/issues/708 self . _kernel_initializer = self . kernel_initializer","title":"setup()"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.OneBodyExpDecay.__call__","text":"Transform electron-ion displacements into an exp decay one-body Jastrow. Parameters: Name Type Description Default input_stream_1e Array input one-electron stream; unused required input_stream_2e Array input two-electron stream; unused required stream_1e Array one-electron stream, post-backflow; unused required r_ei Array electron-ion displacements of shape (..., nelec, nion, d) required r_ee Array electron-electron displacements of shape (..., nelec, nelec, d); unused required Returns: Type Description Array -sum_ij ||a_j * (elec_i - ion_j)||, when self.logabs is True, or exp of that expression when self.logabs is False. If the input has shape (batch_dims, nelec, nion, d), then the output has shape (batch_dims,) Source code in vmcnet/models/jastrow.py @flax . linen . compact def __call__ ( # type: ignore[override] self , input_stream_1e : Array , input_stream_2e : Array , stream_1e : Array , r_ei : Array , r_ee : Array , ) -> Array : \"\"\"Transform electron-ion displacements into an exp decay one-body Jastrow. Args: input_stream_1e (Array): input one-electron stream; unused input_stream_2e (Array): input two-electron stream; unused stream_1e (Array): one-electron stream, post-backflow; unused r_ei (Array): electron-ion displacements of shape (..., nelec, nion, d) r_ee (Array): electron-electron displacements of shape (..., nelec, nelec, d); unused Returns: Array: -sum_ij ||a_j * (elec_i - ion_j)||, when self.logabs is True, or exp of that expression when self.logabs is False. If the input has shape (batch_dims, nelec, nion, d), then the output has shape (batch_dims,) \"\"\" del input_stream_1e , input_stream_2e , stream_1e , r_ee # scale_out has shape (..., nelec, 1, nion, d) scale_out = _isotropy_on_leaf ( r_ei , 1 , self . _kernel_initializer , register_kfac = True ) scaled_distances = jnp . linalg . norm ( scale_out , axis =- 1 ) abs_lin_comb_distances = jnp . sum ( scaled_distances , axis = ( - 1 , - 2 , - 3 )) if self . logabs : return - abs_lin_comb_distances return jnp . exp ( - abs_lin_comb_distances )","title":"__call__()"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.TwoBodyExpDecay","text":"Isotropic exponential decay two-body Jastrow model. The decay is isotropic in the sense that each electron-nuclei and electron-electron term is isotropic, i.e. radially symmetric. The computed interactions are: sum_i(-sum_j Z_j ||elec_i - ion_j|| + sum_k Q ||elec_i - elec_k||) or the exponential if logabs is False. Z_j and Q are initialized to init_ei_strength and init_ee_strength, respectively, and are trainable if trainable is True. Attributes: Name Type Description init_ei_strength Array or Sequence[float] 1-d array or sequence of length nion which gives the initial strength of the electron-nucleus interaction per ion init_ee_strength float initial strength of the electron-electron interaction. Defaults to 1.0. log_scale_factor float Amount to add to the log jastrow (amounts to a multiplicative factor after exponentiation). Defaults to 0.0. register_kfac bool whether to register the computation with KFAC. Defaults to True. logabs bool whether to return the log jastrow (True) or the jastrow (False). Defaults to True. trainable bool whether to allow the jastrow to be trainable. Defaults to True.","title":"TwoBodyExpDecay"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.TwoBodyExpDecay.__call__","text":"Compute jastrow with both electron-ion and electron-electron effects. Parameters: Name Type Description Default input_stream_1e Array input one-electron stream; unused required input_stream_2e Array input two-electron stream; unused required stream_1e Array one-electron stream, post-backflow; unused required r_ei Array electron-ion displacements with shape (..., nelec, nion, d) required r_ee Array electron-electron displacements with shape (..., nelec, nelec, d) required Returns: Type Description Array sum_i(-sum_j Z_j ||elec_i - ion_j|| + sum_k Q ||elec_i - elec_k||), where Z_j and Q are trainable if trainable is true, and an exponential is taken if logabs is False Source code in vmcnet/models/jastrow.py @flax . linen . compact def __call__ ( # type: ignore[override] self , input_stream_1e : Array , input_stream_2e : Array , stream_1e : Array , r_ei : Array , r_ee : Array , ) -> Array : \"\"\"Compute jastrow with both electron-ion and electron-electron effects. Args: input_stream_1e (Array): input one-electron stream; unused input_stream_2e (Array): input two-electron stream; unused stream_1e (Array): one-electron stream, post-backflow; unused r_ei (Array): electron-ion displacements with shape (..., nelec, nion, d) r_ee (Array): electron-electron displacements with shape (..., nelec, nelec, d) Returns: Array: sum_i(-sum_j Z_j ||elec_i - ion_j|| + sum_k Q ||elec_i - elec_k||), where Z_j and Q are trainable if trainable is true, and an exponential is taken if logabs is False \"\"\" del input_stream_1e , input_stream_2e , stream_1e ei_distances = jnp . linalg . norm ( r_ei , axis =- 1 ) ee_distances = jnp . squeeze ( compute_ee_norm_with_safe_diag ( r_ee ), axis =- 1 ) sum_ee_effect = jnp . sum ( jnp . triu ( ee_distances ), axis =- 1 , keepdims = True ) if self . trainable : split_over_ions = jnp . split ( ei_distances , ei_distances . shape [ - 1 ], axis =- 1 ) # TODO: potentially add support for this to SplitDense or otherwise? split_scaled_ei_distances = [ Dense ( 1 , kernel_init = get_constant_init ( self . init_ei_strength [ i ]), use_bias = False , register_kfac = self . register_kfac , )( single_ion_displacement ) for i , single_ion_displacement in enumerate ( split_over_ions ) ] scaled_ei_distances = jnp . concatenate ( split_scaled_ei_distances , axis =- 1 ) sum_ee_effect = Dense ( 1 , kernel_init = get_constant_init ( self . init_ee_strength ), use_bias = False , register_kfac = self . register_kfac , )( sum_ee_effect ) else : scaled_ei_distances = self . init_ei_strength * ei_distances sum_ee_effect = self . init_ee_strength * sum_ee_effect sum_ee_effect = jnp . squeeze ( sum_ee_effect , axis =- 1 ) sum_ei_effect = jnp . sum ( scaled_ei_distances , axis =- 1 ) unscaled_interaction = jnp . sum ( sum_ee_effect - sum_ei_effect , axis =- 1 ) interaction = unscaled_interaction + self . log_scale_factor if self . logabs : return interaction return jnp . exp ( interaction )","title":"__call__()"},{"location":"api/models/jastrow/#vmcnet.models.jastrow.get_two_body_decay_scaled_for_chargeless_molecules","text":"Make molecular decay jastrow, scaled for chargeless molecules. The scale factor is chosen so that the log jastrow is initialized to 0 when electrons are at ion positions. Parameters: Name Type Description Default ion_pos Array an (nion, d) array of ion positions. required ion_charges Array an (nion,) array of ion charges, in units of one elementary charge (the charge of one electron) required init_ee_strength float the initial strength of the electron-electron interaction. Defaults to 1.0. 1.0 register_kfac bool whether to register the computation with KFAC. Defaults to True. True logabs bool whether to return the log jastrow (True) or the jastrow (False). Defaults to True. True trainable bool whether to allow the jastrow to be trainable. Defaults to True. True Returns: Type Description Callable a flax Module with signature (r_ei, r_ee) -> jastrow or log jastrow Source code in vmcnet/models/jastrow.py def get_two_body_decay_scaled_for_chargeless_molecules ( ion_pos : Array , ion_charges : Array , init_ee_strength : float = 1.0 , register_kfac : bool = True , logabs : bool = True , trainable : bool = True , ) -> Jastrow : \"\"\"Make molecular decay jastrow, scaled for chargeless molecules. The scale factor is chosen so that the log jastrow is initialized to 0 when electrons are at ion positions. Args: ion_pos (Array): an (nion, d) array of ion positions. ion_charges (Array): an (nion,) array of ion charges, in units of one elementary charge (the charge of one electron) init_ee_strength (float, optional): the initial strength of the electron-electron interaction. Defaults to 1.0. register_kfac (bool, optional): whether to register the computation with KFAC. Defaults to True. logabs (bool, optional): whether to return the log jastrow (True) or the jastrow (False). Defaults to True. trainable (bool, optional): whether to allow the jastrow to be trainable. Defaults to True. Returns: Callable: a flax Module with signature (r_ei, r_ee) -> jastrow or log jastrow \"\"\" r_ii , charge_charge_prods = physics . potential . _get_ion_ion_info ( ion_pos , ion_charges ) jastrow_scale_factor = 0.5 * jnp . sum ( jnp . linalg . norm ( r_ii , axis =- 1 ) * charge_charge_prods ) jastrow = TwoBodyExpDecay ( ion_charges , init_ee_strength , log_scale_factor = jastrow_scale_factor , register_kfac = register_kfac , logabs = logabs , trainable = trainable , ) return jastrow","title":"get_two_body_decay_scaled_for_chargeless_molecules()"},{"location":"api/models/sign_symmetry/","text":"Routines for symmetrizing a function to be sign covariant. ProductsSignCovariance ( Module ) dataclass Sign covariance from a weighted sum of products of per-particle values. Only supports two spins at the moment. Given per-spin antiequivariant vectors a_1, a_2, ..., and b_1, b_2, ..., computes an antisymmetry of sum_{i,j} (w_{i,j} sum_{k} a_ik b_jk), or multiple such antisymmetries if features>1. If use_weights=False, then no weights are used, so that effectively w_{i,j} = 1 for all i,j. Attributes: Name Type Description features int the number of antisymmetric output features to generate. If use_weights is False, must be equal to 1. kernel_init WeightInitializer initializer for the weights of the dense layer. register_kfac bool whether to register the dense layer with KFAC. Defaults to True. use_weights bool whether to use a weighted sum of products. Defaults to False. __call__ ( self , x ) special Calculate weighted sum of products of up- and down-spin antiequivariances. Parameters: Name Type Description Default x ArrayList input antiequivariant arrays of shape [(..., nelec_up, d), (..., nelec_down, d)] required Returns: Type Description Array array of length features of antisymmetric values calculated by taking a weighted sum of the pairwise dot-products of the up- and down-spin antiequivariant inputs. Source code in vmcnet/models/sign_symmetry.py @flax . linen . compact def __call__ ( self , x : ArrayList ) -> Array : # type: ignore[override] \"\"\"Calculate weighted sum of products of up- and down-spin antiequivariances. Arguments: x (ArrayList): input antiequivariant arrays of shape [(..., nelec_up, d), (..., nelec_down, d)] Returns: Array: array of length features of antisymmetric values calculated by taking a weighted sum of the pairwise dot-products of the up- and down-spin antiequivariant inputs. \"\"\" # TODO (ggoldsh): update this to support nspins != 2 as well if len ( x ) != 2 : raise ValueError ( \"Products covariance only supported for nspins=2, got {} \" . format ( len ( x )) ) naxes = len ( x [ 0 ] . shape ) batch_dims = range ( naxes - 2 ) contraction_dim = ( naxes - 1 ,) # Since the second last axis is not specified as either a batch or contraction # dim, jax.lax.dot_general will automatically compute over all pairs of up and # down spins. pairwise_dots thus has shape (..., nelec_up, nelec_down). pairwise_dots = jax . lax . dot_general ( x [ 0 ], x [ 1 ], (( contraction_dim , contraction_dim ), ( batch_dims , batch_dims )) ) if not self . use_weights : if self . features != 1 : raise ValueError ( \"Can only return one output feature when use_weights is False. \" \"Received {} for features.\" . format ( self . features ) ) return jnp . expand_dims ( jnp . sum ( pairwise_dots , axis = ( - 1 , - 2 )), - 1 ) shape = pairwise_dots . shape # flattened_dots has shape (..., nelec_up * nelec_down) flattened_dots = jnp . reshape ( pairwise_dots , ( * shape [: - 2 ], shape [ - 1 ] * shape [ - 2 ]) ) return Dense ( self . features , kernel_init = self . kernel_init , use_bias = False , register_kfac = self . register_kfac , )( flattened_dots ) apply_sign_symmetry_to_fn ( fn , get_signs_and_syms , apply_output_signs , add_up_results ) Make a function of a list of inputs covariant in the sign of each input. That is, output a function g(s_1, s_2, ..., s_n) with is odd with respect to each input, such that g(s_1, ...) = -g(-s_1, ...), and likewise for every other input. This is done by taking the orbit of the inputs with respect to the sign group applied separately to each one, and adding up the results with appropriate covariant signs. For example, for two spins this calculates g(U,D) = f(U,D) - f(-U,D) - f(U, -D) + f(-U, -D). Inputs are assumed to be either Arrays or SLArrays, so that in either case the required symmetries can be stacked along a new axis of the underlying array values. The function fn is assumed to support the injection of a batch dimension, done in get_signs_and_syms , and pass it through to the output (e.g., a function which flattens the input would not be supported). The extra dimension is removed at the end via add_up_results . Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required get_signs_and_syms Callable a function which gets the signs and symmetries for the input array. Returns a tuple of the symmetries plus the associated signs as a 1D array. required apply_output_signs Callable function for applying signs to the outputs of the symmetrized function. For example, if the outputs are Arrays, this would simply multiply the arrays by the signs along the appropriate axis. required add_up_results Callable function for combining the signed outputs into a single, sign-covariant output. For example, simple addition for Arrays or the slog_sum function for SLArrays. required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. Source code in vmcnet/models/sign_symmetry.py def apply_sign_symmetry_to_fn ( fn : Callable [[ List [ A ]], A ], get_signs_and_syms : Callable [[ List [ A ]], Tuple [ List [ A ], Array ]], apply_output_signs : Callable [[ A , Array ], A ], add_up_results : Callable [[ A ], A ], ) -> Callable [[ List [ A ]], A ]: \"\"\"Make a function of a list of inputs covariant in the sign of each input. That is, output a function g(s_1, s_2, ..., s_n) with is odd with respect to each input, such that g(s_1, ...) = -g(-s_1, ...), and likewise for every other input. This is done by taking the orbit of the inputs with respect to the sign group applied separately to each one, and adding up the results with appropriate covariant signs. For example, for two spins this calculates g(U,D) = f(U,D) - f(-U,D) - f(U, -D) + f(-U, -D). Inputs are assumed to be either Arrays or SLArrays, so that in either case the required symmetries can be stacked along a new axis of the underlying array values. The function `fn` is assumed to support the injection of a batch dimension, done in `get_signs_and_syms`, and pass it through to the output (e.g., a function which flattens the input would not be supported). The extra dimension is removed at the end via `add_up_results`. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) get_signs_and_syms (Callable): a function which gets the signs and symmetries for the input array. Returns a tuple of the symmetries plus the associated signs as a 1D array. apply_output_signs (Callable): function for applying signs to the outputs of the symmetrized function. For example, if the outputs are Arrays, this would simply multiply the arrays by the signs along the appropriate axis. add_up_results (Callable): function for combining the signed outputs into a single, sign-covariant output. For example, simple addition for Arrays or the slog_sum function for SLArrays. Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. \"\"\" def sign_covariant_fn ( x : List [ A ]) -> A : symmetries , signs = get_signs_and_syms ( x ) outputs = fn ( symmetries ) signed_results = apply_output_signs ( outputs , signs ) return add_up_results ( signed_results ) return sign_covariant_fn make_sl_array_list_fn_sign_covariant ( fn , axis =- 2 ) Make a function of an SLArrayList sign-covariant in the sign of each SLArray. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. Source code in vmcnet/models/sign_symmetry.py def make_sl_array_list_fn_sign_covariant ( fn : Callable [[ SLArrayList ], SLArray ], axis : int = - 2 ) -> Callable [[ SLArrayList ], SLArray ]: \"\"\"Make a function of an SLArrayList sign-covariant in the sign of each SLArray. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. \"\"\" return apply_sign_symmetry_to_fn ( fn , functools . partial ( _get_sign_orbit_sl_array_list , axis = axis ), lambda x , s : ( _multiply_sign_along_axis ( x [ 0 ], s , axis ), x [ 1 ]), functools . partial ( slog_sum_over_axis , axis = axis ), ) make_array_list_fn_sign_covariant ( fn , axis =- 2 ) Make a function of an ArrayList sign-covariant in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. Source code in vmcnet/models/sign_symmetry.py def make_array_list_fn_sign_covariant ( fn : Callable [[ ArrayList ], Array ], axis : int = - 2 ) -> Callable [[ ArrayList ], Array ]: \"\"\"Make a function of an ArrayList sign-covariant in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. \"\"\" return apply_sign_symmetry_to_fn ( fn , functools . partial ( _get_sign_orbit_array_list , axis = axis ), functools . partial ( _multiply_sign_along_axis , axis = axis ), functools . partial ( jnp . sum , axis = axis ), ) make_array_list_fn_sign_invariant ( fn , axis =- 2 ) Make a function of an ArrayList sign-invariant (even) in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be invariant with respect to the sign of each input, or in other words, will be even. Source code in vmcnet/models/sign_symmetry.py def make_array_list_fn_sign_invariant ( fn : Callable [[ ArrayList ], Array ], axis : int = - 2 ) -> Callable [[ ArrayList ], Array ]: \"\"\"Make a function of an ArrayList sign-invariant (even) in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be invariant with respect to the sign of each input, or in other words, will be even. \"\"\" return apply_sign_symmetry_to_fn ( fn , functools . partial ( _get_sign_orbit_array_list , axis = axis ), lambda x , _ : x , # Ignore the signs to get an invariance functools . partial ( jnp . sum , axis = axis ), )","title":"sign_symmetry"},{"location":"api/models/sign_symmetry/#vmcnet.models.sign_symmetry.ProductsSignCovariance","text":"Sign covariance from a weighted sum of products of per-particle values. Only supports two spins at the moment. Given per-spin antiequivariant vectors a_1, a_2, ..., and b_1, b_2, ..., computes an antisymmetry of sum_{i,j} (w_{i,j} sum_{k} a_ik b_jk), or multiple such antisymmetries if features>1. If use_weights=False, then no weights are used, so that effectively w_{i,j} = 1 for all i,j. Attributes: Name Type Description features int the number of antisymmetric output features to generate. If use_weights is False, must be equal to 1. kernel_init WeightInitializer initializer for the weights of the dense layer. register_kfac bool whether to register the dense layer with KFAC. Defaults to True. use_weights bool whether to use a weighted sum of products. Defaults to False.","title":"ProductsSignCovariance"},{"location":"api/models/sign_symmetry/#vmcnet.models.sign_symmetry.ProductsSignCovariance.__call__","text":"Calculate weighted sum of products of up- and down-spin antiequivariances. Parameters: Name Type Description Default x ArrayList input antiequivariant arrays of shape [(..., nelec_up, d), (..., nelec_down, d)] required Returns: Type Description Array array of length features of antisymmetric values calculated by taking a weighted sum of the pairwise dot-products of the up- and down-spin antiequivariant inputs. Source code in vmcnet/models/sign_symmetry.py @flax . linen . compact def __call__ ( self , x : ArrayList ) -> Array : # type: ignore[override] \"\"\"Calculate weighted sum of products of up- and down-spin antiequivariances. Arguments: x (ArrayList): input antiequivariant arrays of shape [(..., nelec_up, d), (..., nelec_down, d)] Returns: Array: array of length features of antisymmetric values calculated by taking a weighted sum of the pairwise dot-products of the up- and down-spin antiequivariant inputs. \"\"\" # TODO (ggoldsh): update this to support nspins != 2 as well if len ( x ) != 2 : raise ValueError ( \"Products covariance only supported for nspins=2, got {} \" . format ( len ( x )) ) naxes = len ( x [ 0 ] . shape ) batch_dims = range ( naxes - 2 ) contraction_dim = ( naxes - 1 ,) # Since the second last axis is not specified as either a batch or contraction # dim, jax.lax.dot_general will automatically compute over all pairs of up and # down spins. pairwise_dots thus has shape (..., nelec_up, nelec_down). pairwise_dots = jax . lax . dot_general ( x [ 0 ], x [ 1 ], (( contraction_dim , contraction_dim ), ( batch_dims , batch_dims )) ) if not self . use_weights : if self . features != 1 : raise ValueError ( \"Can only return one output feature when use_weights is False. \" \"Received {} for features.\" . format ( self . features ) ) return jnp . expand_dims ( jnp . sum ( pairwise_dots , axis = ( - 1 , - 2 )), - 1 ) shape = pairwise_dots . shape # flattened_dots has shape (..., nelec_up * nelec_down) flattened_dots = jnp . reshape ( pairwise_dots , ( * shape [: - 2 ], shape [ - 1 ] * shape [ - 2 ]) ) return Dense ( self . features , kernel_init = self . kernel_init , use_bias = False , register_kfac = self . register_kfac , )( flattened_dots )","title":"__call__()"},{"location":"api/models/sign_symmetry/#vmcnet.models.sign_symmetry.apply_sign_symmetry_to_fn","text":"Make a function of a list of inputs covariant in the sign of each input. That is, output a function g(s_1, s_2, ..., s_n) with is odd with respect to each input, such that g(s_1, ...) = -g(-s_1, ...), and likewise for every other input. This is done by taking the orbit of the inputs with respect to the sign group applied separately to each one, and adding up the results with appropriate covariant signs. For example, for two spins this calculates g(U,D) = f(U,D) - f(-U,D) - f(U, -D) + f(-U, -D). Inputs are assumed to be either Arrays or SLArrays, so that in either case the required symmetries can be stacked along a new axis of the underlying array values. The function fn is assumed to support the injection of a batch dimension, done in get_signs_and_syms , and pass it through to the output (e.g., a function which flattens the input would not be supported). The extra dimension is removed at the end via add_up_results . Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required get_signs_and_syms Callable a function which gets the signs and symmetries for the input array. Returns a tuple of the symmetries plus the associated signs as a 1D array. required apply_output_signs Callable function for applying signs to the outputs of the symmetrized function. For example, if the outputs are Arrays, this would simply multiply the arrays by the signs along the appropriate axis. required add_up_results Callable function for combining the signed outputs into a single, sign-covariant output. For example, simple addition for Arrays or the slog_sum function for SLArrays. required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. Source code in vmcnet/models/sign_symmetry.py def apply_sign_symmetry_to_fn ( fn : Callable [[ List [ A ]], A ], get_signs_and_syms : Callable [[ List [ A ]], Tuple [ List [ A ], Array ]], apply_output_signs : Callable [[ A , Array ], A ], add_up_results : Callable [[ A ], A ], ) -> Callable [[ List [ A ]], A ]: \"\"\"Make a function of a list of inputs covariant in the sign of each input. That is, output a function g(s_1, s_2, ..., s_n) with is odd with respect to each input, such that g(s_1, ...) = -g(-s_1, ...), and likewise for every other input. This is done by taking the orbit of the inputs with respect to the sign group applied separately to each one, and adding up the results with appropriate covariant signs. For example, for two spins this calculates g(U,D) = f(U,D) - f(-U,D) - f(U, -D) + f(-U, -D). Inputs are assumed to be either Arrays or SLArrays, so that in either case the required symmetries can be stacked along a new axis of the underlying array values. The function `fn` is assumed to support the injection of a batch dimension, done in `get_signs_and_syms`, and pass it through to the output (e.g., a function which flattens the input would not be supported). The extra dimension is removed at the end via `add_up_results`. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) get_signs_and_syms (Callable): a function which gets the signs and symmetries for the input array. Returns a tuple of the symmetries plus the associated signs as a 1D array. apply_output_signs (Callable): function for applying signs to the outputs of the symmetrized function. For example, if the outputs are Arrays, this would simply multiply the arrays by the signs along the appropriate axis. add_up_results (Callable): function for combining the signed outputs into a single, sign-covariant output. For example, simple addition for Arrays or the slog_sum function for SLArrays. Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. \"\"\" def sign_covariant_fn ( x : List [ A ]) -> A : symmetries , signs = get_signs_and_syms ( x ) outputs = fn ( symmetries ) signed_results = apply_output_signs ( outputs , signs ) return add_up_results ( signed_results ) return sign_covariant_fn","title":"apply_sign_symmetry_to_fn()"},{"location":"api/models/sign_symmetry/#vmcnet.models.sign_symmetry.make_sl_array_list_fn_sign_covariant","text":"Make a function of an SLArrayList sign-covariant in the sign of each SLArray. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. Source code in vmcnet/models/sign_symmetry.py def make_sl_array_list_fn_sign_covariant ( fn : Callable [[ SLArrayList ], SLArray ], axis : int = - 2 ) -> Callable [[ SLArrayList ], SLArray ]: \"\"\"Make a function of an SLArrayList sign-covariant in the sign of each SLArray. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. \"\"\" return apply_sign_symmetry_to_fn ( fn , functools . partial ( _get_sign_orbit_sl_array_list , axis = axis ), lambda x , s : ( _multiply_sign_along_axis ( x [ 0 ], s , axis ), x [ 1 ]), functools . partial ( slog_sum_over_axis , axis = axis ), )","title":"make_sl_array_list_fn_sign_covariant()"},{"location":"api/models/sign_symmetry/#vmcnet.models.sign_symmetry.make_array_list_fn_sign_covariant","text":"Make a function of an ArrayList sign-covariant in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. Source code in vmcnet/models/sign_symmetry.py def make_array_list_fn_sign_covariant ( fn : Callable [[ ArrayList ], Array ], axis : int = - 2 ) -> Callable [[ ArrayList ], Array ]: \"\"\"Make a function of an ArrayList sign-covariant in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be covariant with respect to the sign of each input, or in other words, will be odd. \"\"\" return apply_sign_symmetry_to_fn ( fn , functools . partial ( _get_sign_orbit_array_list , axis = axis ), functools . partial ( _multiply_sign_along_axis , axis = axis ), functools . partial ( jnp . sum , axis = axis ), )","title":"make_array_list_fn_sign_covariant()"},{"location":"api/models/sign_symmetry/#vmcnet.models.sign_symmetry.make_array_list_fn_sign_invariant","text":"Make a function of an ArrayList sign-invariant (even) in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Parameters: Name Type Description Default fn Callable the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) required Returns: Type Description Callable a function with the same signature as the input function, but which has been symmetrized so that its output will be invariant with respect to the sign of each input, or in other words, will be even. Source code in vmcnet/models/sign_symmetry.py def make_array_list_fn_sign_invariant ( fn : Callable [[ ArrayList ], Array ], axis : int = - 2 ) -> Callable [[ ArrayList ], Array ]: \"\"\"Make a function of an ArrayList sign-invariant (even) in the sign of each array. Shallow wrapper around the generic apply_sign_symmetry_to_fn. Args: fn (Callable): the function to symmetrize. The given axis is injected into the inputs and the sign orbit is computed, so this function should be able to treat the given sign orbit axis as a batch dimension, and the overall tensor rank should not change (len(input.shape) == len(output.shape)) Returns: Callable: a function with the same signature as the input function, but which has been symmetrized so that its output will be invariant with respect to the sign of each input, or in other words, will be even. \"\"\" return apply_sign_symmetry_to_fn ( fn , functools . partial ( _get_sign_orbit_array_list , axis = axis ), lambda x , _ : x , # Ignore the signs to get an invariance functools . partial ( jnp . sum , axis = axis ), )","title":"make_array_list_fn_sign_invariant()"},{"location":"api/models/weights/","text":"Functions to get weight initializers from names. validate_kernel_initializer ( name ) Check that a kernel initializer name is in the list of supported kernel inits. Source code in vmcnet/models/weights.py def validate_kernel_initializer ( name : str ) -> None : \"\"\"Check that a kernel initializer name is in the list of supported kernel inits.\"\"\" if name not in VALID_KERNEL_INITIALIZERS : raise ValueError ( \"Invalid kernel initializer requested, {} was requested, but available \" \"initializers are: \" . format ( name ) + \", \" . join ( VALID_KERNEL_INITIALIZERS ) ) get_kernel_initializer ( name , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>, **kwargs) Get a kernel initializer. Source code in vmcnet/models/weights.py def get_kernel_initializer ( name : str , dtype = jnp . float32 , ** kwargs : Any ) -> WeightInitializer : \"\"\"Get a kernel initializer.\"\"\" validate_kernel_initializer ( name ) constructor = INITIALIZER_CONSTRUCTORS [ name ] if name == \"orthogonal\" or name == \"delta_orthogonal\" : return constructor ( scale = kwargs . get ( \"scale\" , 1.0 ), dtype = dtype ) else : return constructor ( dtype = dtype ) get_kernel_init_from_config ( config , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) Get a kernel initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. Source code in vmcnet/models/weights.py def get_kernel_init_from_config ( config : ConfigDict , dtype = jnp . float32 ): \"\"\"Get a kernel initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. \"\"\" return get_kernel_initializer ( config . type , dtype = dtype , ** config ) validate_bias_initializer ( name ) Check that a bias initializer name is in the list of supported bias inits. Source code in vmcnet/models/weights.py def validate_bias_initializer ( name : str ) -> None : \"\"\"Check that a bias initializer name is in the list of supported bias inits.\"\"\" if name not in VALID_BIAS_INITIALIZERS : raise ValueError ( \"Invalid bias initializer requested, {} was requested, but available \" \"initializers are: \" . format ( name ) + \", \" . join ( VALID_BIAS_INITIALIZERS ) ) get_bias_initializer ( name , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) Get a bias initializer. Source code in vmcnet/models/weights.py def get_bias_initializer ( name : str , dtype = jnp . float32 ) -> WeightInitializer : \"\"\"Get a bias initializer.\"\"\" validate_bias_initializer ( name ) return INITIALIZER_CONSTRUCTORS [ name ]( dtype = dtype ) get_bias_init_from_config ( config , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) Get a bias initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. Source code in vmcnet/models/weights.py def get_bias_init_from_config ( config , dtype = jnp . float32 ): \"\"\"Get a bias initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. \"\"\" return get_bias_initializer ( config . type , dtype = dtype ) get_constant_init ( constant ) Get a weight initializer for a constant array with specified dtype, ignoring key. Parameters: Name Type Description Default constant float the number to initialize to required Source code in vmcnet/models/weights.py def get_constant_init ( constant : float ): \"\"\"Get a weight initializer for a constant array with specified dtype, ignoring key. Args: constant (float): the number to initialize to \"\"\" def init_fn ( key , shape , dtype = jnp . float32 ): del key return jnp . ones ( shape , dtype = dtype ) * jnp . array ( constant , dtype = dtype ) return init_fn","title":"weights"},{"location":"api/models/weights/#vmcnet.models.weights.validate_kernel_initializer","text":"Check that a kernel initializer name is in the list of supported kernel inits. Source code in vmcnet/models/weights.py def validate_kernel_initializer ( name : str ) -> None : \"\"\"Check that a kernel initializer name is in the list of supported kernel inits.\"\"\" if name not in VALID_KERNEL_INITIALIZERS : raise ValueError ( \"Invalid kernel initializer requested, {} was requested, but available \" \"initializers are: \" . format ( name ) + \", \" . join ( VALID_KERNEL_INITIALIZERS ) )","title":"validate_kernel_initializer()"},{"location":"api/models/weights/#vmcnet.models.weights.get_kernel_initializer","text":"Get a kernel initializer. Source code in vmcnet/models/weights.py def get_kernel_initializer ( name : str , dtype = jnp . float32 , ** kwargs : Any ) -> WeightInitializer : \"\"\"Get a kernel initializer.\"\"\" validate_kernel_initializer ( name ) constructor = INITIALIZER_CONSTRUCTORS [ name ] if name == \"orthogonal\" or name == \"delta_orthogonal\" : return constructor ( scale = kwargs . get ( \"scale\" , 1.0 ), dtype = dtype ) else : return constructor ( dtype = dtype )","title":"get_kernel_initializer()"},{"location":"api/models/weights/#vmcnet.models.weights.get_kernel_init_from_config","text":"Get a kernel initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. Source code in vmcnet/models/weights.py def get_kernel_init_from_config ( config : ConfigDict , dtype = jnp . float32 ): \"\"\"Get a kernel initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. \"\"\" return get_kernel_initializer ( config . type , dtype = dtype , ** config )","title":"get_kernel_init_from_config()"},{"location":"api/models/weights/#vmcnet.models.weights.validate_bias_initializer","text":"Check that a bias initializer name is in the list of supported bias inits. Source code in vmcnet/models/weights.py def validate_bias_initializer ( name : str ) -> None : \"\"\"Check that a bias initializer name is in the list of supported bias inits.\"\"\" if name not in VALID_BIAS_INITIALIZERS : raise ValueError ( \"Invalid bias initializer requested, {} was requested, but available \" \"initializers are: \" . format ( name ) + \", \" . join ( VALID_BIAS_INITIALIZERS ) )","title":"validate_bias_initializer()"},{"location":"api/models/weights/#vmcnet.models.weights.get_bias_initializer","text":"Get a bias initializer. Source code in vmcnet/models/weights.py def get_bias_initializer ( name : str , dtype = jnp . float32 ) -> WeightInitializer : \"\"\"Get a bias initializer.\"\"\" validate_bias_initializer ( name ) return INITIALIZER_CONSTRUCTORS [ name ]( dtype = dtype )","title":"get_bias_initializer()"},{"location":"api/models/weights/#vmcnet.models.weights.get_bias_init_from_config","text":"Get a bias initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. Source code in vmcnet/models/weights.py def get_bias_init_from_config ( config , dtype = jnp . float32 ): \"\"\"Get a bias initializer from a ConfigDict. The ConfigDict should have the key \"type\", as well as any other kwargs to pass to the initializer constructor. \"\"\" return get_bias_initializer ( config . type , dtype = dtype )","title":"get_bias_init_from_config()"},{"location":"api/models/weights/#vmcnet.models.weights.get_constant_init","text":"Get a weight initializer for a constant array with specified dtype, ignoring key. Parameters: Name Type Description Default constant float the number to initialize to required Source code in vmcnet/models/weights.py def get_constant_init ( constant : float ): \"\"\"Get a weight initializer for a constant array with specified dtype, ignoring key. Args: constant (float): the number to initialize to \"\"\" def init_fn ( key , shape , dtype = jnp . float32 ): del key return jnp . ones ( shape , dtype = dtype ) * jnp . array ( constant , dtype = dtype ) return init_fn","title":"get_constant_init()"},{"location":"api/physics/core/","text":"Core local energy and gradient construction routines. initialize_molecular_pos ( key , nchains , ion_pos , ion_charges , nelec_total , init_width = 1.0 , dtype =< class ' jax . _src . numpy . lax_numpy . float32 '>) Initialize a set of plausible initial electron positions. For each chain, each electron is assigned to a random ion and then its position is sampled from a normal distribution centered at that ion with diagonal covariance with diagonal entries all equal to init_width. If there are no more electrons than there are ions, the assignment is done without replacement. If there are more electrons than ions, the assignment is done with replacement, and the probability of choosing ion i is its relative charge (as a fraction of the sum of the ion charges). Source code in vmcnet/physics/core.py def initialize_molecular_pos ( key : PRNGKey , nchains : int , ion_pos : Array , ion_charges : Array , nelec_total : int , init_width : float = 1.0 , dtype = jnp . float32 , ) -> Tuple [ PRNGKey , Array ]: \"\"\"Initialize a set of plausible initial electron positions. For each chain, each electron is assigned to a random ion and then its position is sampled from a normal distribution centered at that ion with diagonal covariance with diagonal entries all equal to init_width. If there are no more electrons than there are ions, the assignment is done without replacement. If there are more electrons than ions, the assignment is done with replacement, and the probability of choosing ion i is its relative charge (as a fraction of the sum of the ion charges). \"\"\" nion = len ( ion_charges ) replace = True if nelec_total <= nion : replace = False assignments = [] for _ in range ( nchains ): key , subkey = jax . random . split ( key ) choices = jax . random . choice ( subkey , nion , shape = ( nelec_total ,), replace = replace , p = ion_charges / jnp . sum ( ion_charges ), ) assignments . append ( ion_pos [ choices ]) elecs_at_ions = jnp . stack ( assignments , axis = 0 ) key , subkey = jax . random . split ( key ) return key , elecs_at_ions + init_width * jax . random . normal ( subkey , elecs_at_ions . shape , dtype = dtype ) combine_local_energy_terms ( local_energy_terms ) Combine a sequence of local energy terms by adding them. Parameters: Name Type Description Default local_energy_terms Sequence sequence of local energy terms, each with the signature (params, x) -> array of terms of shape (x.shape[0],) required Returns: Type Description Callable local energy function which computes the sum of the local energy terms. Has the signature (params, x) -> local energy array of shape (x.shape[0],) Source code in vmcnet/physics/core.py def combine_local_energy_terms ( local_energy_terms : Sequence [ ModelApply [ P ]], ) -> ModelApply [ P ]: \"\"\"Combine a sequence of local energy terms by adding them. Args: local_energy_terms (Sequence): sequence of local energy terms, each with the signature (params, x) -> array of terms of shape (x.shape[0],) Returns: Callable: local energy function which computes the sum of the local energy terms. Has the signature (params, x) -> local energy array of shape (x.shape[0],) \"\"\" def local_energy_fn ( params : P , x : Array ) -> Array : local_energy_sum = local_energy_terms [ 0 ]( params , x ) for term in local_energy_terms [ 1 :]: local_energy_sum = cast ( Array , local_energy_sum + term ( params , x )) return local_energy_sum return local_energy_fn laplacian_psi_over_psi ( grad_log_psi_apply , params , x ) Compute (nabla^2 psi) / psi at x given a function which evaluates psi'(x)/psi. The computation is done by computing (forward-mode) derivatives of the gradient to get the columns of the Hessian, and accumulating the (i, i)th entries (but this implementation is significantly more memory efficient than directly computing the Hessian). This function uses the identity (nabla^2 psi) / psi = (nabla^2 log|psi|) + (nabla log|psi|)^2 to avoid leaving the log domain during the computation. This function should be vmapped in order to be applied to batches of inputs, as it completely flattens x in order to take second derivatives w.r.t. each component. This is approach is extremely similar to the one in the FermiNet repo (in the jax branch, as of this writing -- see https://github.com/deepmind/ferminet/blob/aade61b3d30883b3238d6b50c85404d0e8176155/ferminet/hamiltonian.py). The main difference is that we are being explicit about the flattening of x within the Laplacian calculation, so that it does not have to be handled outside of this function (psi is free to take x shapes which are not flat). Parameters: Name Type Description Default grad_log_psi_apply Callable function which evaluates the derivative of log|psi(x)|, i.e. (nabla psi)(x) / psi(x), with respect to x. Has the signature (params, x) -> (nabla psi)(x) / psi(x), so the derivative should be over the second arg, x, and the output shape should be the same as x required params pytree model parameters, passed as the first arg of grad_log_psi required x Array second input to grad_log_psi required Returns: Type Description jnp.float32 \"local\" laplacian calculation, i.e. (nabla^2 psi) / psi Source code in vmcnet/physics/core.py def laplacian_psi_over_psi ( grad_log_psi_apply : ModelApply , params : P , x : Array , ) -> jnp . float32 : \"\"\"Compute (nabla^2 psi) / psi at x given a function which evaluates psi'(x)/psi. The computation is done by computing (forward-mode) derivatives of the gradient to get the columns of the Hessian, and accumulating the (i, i)th entries (but this implementation is significantly more memory efficient than directly computing the Hessian). This function uses the identity (nabla^2 psi) / psi = (nabla^2 log|psi|) + (nabla log|psi|)^2 to avoid leaving the log domain during the computation. This function should be vmapped in order to be applied to batches of inputs, as it completely flattens x in order to take second derivatives w.r.t. each component. This is approach is extremely similar to the one in the FermiNet repo (in the jax branch, as of this writing -- see https://github.com/deepmind/ferminet/blob/aade61b3d30883b3238d6b50c85404d0e8176155/ferminet/hamiltonian.py). The main difference is that we are being explicit about the flattening of x within the Laplacian calculation, so that it does not have to be handled outside of this function (psi is free to take x shapes which are not flat). Args: grad_log_psi_apply (Callable): function which evaluates the derivative of log|psi(x)|, i.e. (nabla psi)(x) / psi(x), with respect to x. Has the signature (params, x) -> (nabla psi)(x) / psi(x), so the derivative should be over the second arg, x, and the output shape should be the same as x params (pytree): model parameters, passed as the first arg of grad_log_psi x (Array): second input to grad_log_psi Returns: jnp.float32: \"local\" laplacian calculation, i.e. (nabla^2 psi) / psi \"\"\" x_shape = x . shape flat_x = jnp . reshape ( x , ( - 1 ,)) n = flat_x . shape [ 0 ] identity_mat = jnp . eye ( n ) def flattened_grad_log_psi_of_flat_x ( flat_x_in ): \"\"\"Flattened input to flattened output version of grad_log_psi.\"\"\" grad_log_psi_out = grad_log_psi_apply ( params , jnp . reshape ( flat_x_in , x_shape )) return jnp . reshape ( grad_log_psi_out , ( - 1 ,)) def step_fn ( carry , unused ): del unused i = carry [ 0 ] primals , tangents = jax . jvp ( flattened_grad_log_psi_of_flat_x , ( flat_x ,), ( identity_mat [ i ],) ) return ( i + 1 , carry [ 1 ] + jnp . square ( primals [ i ]) + tangents [ i ]), None out , _ = jax . lax . scan ( step_fn , ( 0 , 0.0 ), xs = None , length = n ) return out [ 1 ] get_statistics_from_local_energy ( local_energies , nchains , nan_safe = True ) Collectively reduce local energies to an average energy and variance. Parameters: Name Type Description Default local_energies Array local energies of shape (nchains,), possibly distributed across multiple devices via utils.distribute.pmap. required nchains int total number of chains across all devices, used to compute a sample variance estimate of the local energy required nan_safe bool flag which controls if jnp.nanmean is used instead of jnp.mean. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. True Returns: Type Description (jnp.float32, jnp.float32) local energy average, local energy (sample) variance Source code in vmcnet/physics/core.py def get_statistics_from_local_energy ( local_energies : Array , nchains : int , nan_safe : bool = True ) -> Tuple [ jnp . float32 , jnp . float32 ]: \"\"\"Collectively reduce local energies to an average energy and variance. Args: local_energies (Array): local energies of shape (nchains,), possibly distributed across multiple devices via utils.distribute.pmap. nchains (int): total number of chains across all devices, used to compute a sample variance estimate of the local energy nan_safe (bool, optional): flag which controls if jnp.nanmean is used instead of jnp.mean. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. Returns: (jnp.float32, jnp.float32): local energy average, local energy (sample) variance \"\"\" # TODO(Jeffmin) might be worth investigating the numerical stability of the XLA # compiled version of these two computations, since the quality of the gradients # is fairly crucial to the success of the algorithm if nan_safe : allreduce_mean = utils . distribute . nanmean_all_local_devices else : allreduce_mean = utils . distribute . mean_all_local_devices energy = allreduce_mean ( local_energies ) variance = ( allreduce_mean ( jnp . square ( local_energies - energy )) * nchains / ( nchains - 1 ) ) # adjust by n / (n - 1) to get an unbiased estimator return energy , variance get_default_energy_bwd ( log_psi_apply , mean_grad_fn ) Use a standard variance reduction formula to get the bwd pass of the energy. The formula is 2 * E_p[(local_e - E_p[local_e]) * grad_log_psi], where the symbol E_p[] refers to the expectation over the probability distribution p defined by p(x) = |psi(x)|^2 / . This is an unbiased estimator of the gradient of E_p[local_e], and has a lower variance than directly differentiating E_p[local_e] with respect to the parameters. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required mean_grad_fn Callable function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. required Returns: Type Description Callable function which computes the backward pass in the custom vjp of the total energy. Has the signature (res, cotangents) -> (gradients, None) Source code in vmcnet/physics/core.py def get_default_energy_bwd ( log_psi_apply : ModelApply [ P ], mean_grad_fn : Callable [[ Array ], Array ], ): \"\"\"Use a standard variance reduction formula to get the bwd pass of the energy. The formula is 2 * E_p[(local_e - E_p[local_e]) * grad_log_psi], where the symbol E_p[] refers to the expectation over the probability distribution p defined by p(x) = |psi(x)|^2 / <psi | psi>. This is an unbiased estimator of the gradient of E_p[local_e], and has a lower variance than directly differentiating E_p[local_e] with respect to the parameters. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| mean_grad_fn (Callable): function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. Returns: Callable: function which computes the backward pass in the custom vjp of the total energy. Has the signature (res, cotangents) -> (gradients, None) \"\"\" def scaled_by_local_e ( params : P , positions : Array , centered_local_energies : Array ) -> jnp . float32 : log_psi = log_psi_apply ( params , positions ) loss_functions . register_normal_predictive_distribution ( log_psi [:, None ]) return 2.0 * mean_grad_fn ( centered_local_energies * log_psi ) _get_energy_grad = jax . grad ( scaled_by_local_e , argnums = 0 ) def energy_bwd ( res , cotangents ) -> Tuple [ P , None ]: energy , local_energies , params , positions = res centered_local_energies = local_energies - energy gradient = _get_energy_grad ( params , positions , centered_local_energies ) return jax . tree_map ( lambda x : x * cotangents [ 0 ], gradient ), None return energy_bwd create_value_and_grad_energy_fn ( log_psi_apply , local_energy_fn , nchains , clipping_fn = None , nan_safe = True , get_energy_bwd =< function get_default_energy_bwd at 0x7ff1729ab940 > ) Create a function which computes unbiased energy gradients. Due to the Hermiticity of the Hamiltonian, we can get an unbiased lower variance estimate of the gradient of the expected energy than the naive gradient of the mean of sampled local energies. Specifically, the gradient of the expected energy expect[E_L] takes the form 2 * expect[(E_L - expect[E_L]) * (grad_psi / psi)(x)], where E_L is the local energy and expect[] denotes the expectation with respect to the distribution |psi|^2. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required local_energy_fn Callable computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) required nchains int total number of chains across all devices, used to compute a sample variance estimate of the local energy required clipping_fn Callable post-processing function on the local energy, e.g. a function which clips the values to be within some multiple of the total variation from the median. The post-processed values are used for the gradient calculation, if available. Defaults to None. None nan_safe bool flag which controls if jnp.nanmean and jnp.nansum are used instead of jnp.mean and jnp.sum for the terms in the gradient calculation. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. True get_energy_bwd Callable function which returns a custom backward pass for the total energy calculation. Has the signature (log_psi_apply, mean_grad_fn) -> energy_bwd. Defaults to get_default_energy_bwd, which computes the formula above. <function get_default_energy_bwd at 0x7ff1729ab940> Returns: Type Description Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) Source code in vmcnet/physics/core.py def create_value_and_grad_energy_fn ( log_psi_apply : ModelApply [ P ], local_energy_fn : ModelApply [ P ], nchains : int , clipping_fn : Optional [ Callable [[ Array ], Array ]] = None , nan_safe : bool = True , get_energy_bwd : Callable = get_default_energy_bwd , ) -> ValueGradEnergyFn [ P ]: \"\"\"Create a function which computes unbiased energy gradients. Due to the Hermiticity of the Hamiltonian, we can get an unbiased lower variance estimate of the gradient of the expected energy than the naive gradient of the mean of sampled local energies. Specifically, the gradient of the expected energy expect[E_L] takes the form 2 * expect[(E_L - expect[E_L]) * (grad_psi / psi)(x)], where E_L is the local energy and expect[] denotes the expectation with respect to the distribution |psi|^2. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| local_energy_fn (Callable): computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) nchains (int): total number of chains across all devices, used to compute a sample variance estimate of the local energy clipping_fn (Callable, optional): post-processing function on the local energy, e.g. a function which clips the values to be within some multiple of the total variation from the median. The post-processed values are used for the gradient calculation, if available. Defaults to None. nan_safe (bool, optional): flag which controls if jnp.nanmean and jnp.nansum are used instead of jnp.mean and jnp.sum for the terms in the gradient calculation. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. get_energy_bwd (Callable): function which returns a custom backward pass for the total energy calculation. Has the signature (log_psi_apply, mean_grad_fn) -> energy_bwd. Defaults to get_default_energy_bwd, which computes the formula above. Returns: Callable: function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) \"\"\" @jax . custom_vjp def compute_energy_data ( params : P , positions : Array ) -> EnergyData : local_energies_noclip = local_energy_fn ( params , positions ) if clipping_fn is not None : local_energies = clipping_fn ( local_energies_noclip ) energy , variance = get_statistics_from_local_energy ( local_energies , nchains , nan_safe = nan_safe ) # For the unclipped metrics, which are not used in the gradient, don't # do these in a nan-safe way. This makes nans more visible and makes sure # the command-line checkpoint_if_nans flag will work properly. energy_noclip , variance_noclip = get_statistics_from_local_energy ( local_energies_noclip , nchains , nan_safe = False ) aux_data = ( variance , local_energies , energy_noclip , variance_noclip ) else : local_energies = local_energies_noclip energy , variance = get_statistics_from_local_energy ( local_energies , nchains , nan_safe = nan_safe ) # Even though there's no clipping function, still record noclip metrics # without nan-safety so that checkpointing epochs with nans can be # supported. energy_noclip , variance_noclip = get_statistics_from_local_energy ( local_energies , nchains , nan_safe = False ) aux_data = ( variance , local_energies , energy_noclip , variance_noclip ) return energy , aux_data def energy_fwd ( params : P , positions : Array ): output = compute_energy_data ( params , positions ) energy = output [ 0 ] local_energies = output [ 1 ][ 1 ] return output , ( energy , local_energies , params , positions ) mean_grad_fn = utils . distribute . get_mean_over_first_axis_fn ( nan_safe = nan_safe ) energy_bwd = get_energy_bwd ( log_psi_apply , mean_grad_fn ) compute_energy_data . defvjp ( energy_fwd , energy_bwd ) energy_data_val_and_grad = jax . value_and_grad ( compute_energy_data , argnums = 0 , has_aux = True ) return energy_data_val_and_grad","title":"core"},{"location":"api/physics/core/#vmcnet.physics.core.initialize_molecular_pos","text":"Initialize a set of plausible initial electron positions. For each chain, each electron is assigned to a random ion and then its position is sampled from a normal distribution centered at that ion with diagonal covariance with diagonal entries all equal to init_width. If there are no more electrons than there are ions, the assignment is done without replacement. If there are more electrons than ions, the assignment is done with replacement, and the probability of choosing ion i is its relative charge (as a fraction of the sum of the ion charges). Source code in vmcnet/physics/core.py def initialize_molecular_pos ( key : PRNGKey , nchains : int , ion_pos : Array , ion_charges : Array , nelec_total : int , init_width : float = 1.0 , dtype = jnp . float32 , ) -> Tuple [ PRNGKey , Array ]: \"\"\"Initialize a set of plausible initial electron positions. For each chain, each electron is assigned to a random ion and then its position is sampled from a normal distribution centered at that ion with diagonal covariance with diagonal entries all equal to init_width. If there are no more electrons than there are ions, the assignment is done without replacement. If there are more electrons than ions, the assignment is done with replacement, and the probability of choosing ion i is its relative charge (as a fraction of the sum of the ion charges). \"\"\" nion = len ( ion_charges ) replace = True if nelec_total <= nion : replace = False assignments = [] for _ in range ( nchains ): key , subkey = jax . random . split ( key ) choices = jax . random . choice ( subkey , nion , shape = ( nelec_total ,), replace = replace , p = ion_charges / jnp . sum ( ion_charges ), ) assignments . append ( ion_pos [ choices ]) elecs_at_ions = jnp . stack ( assignments , axis = 0 ) key , subkey = jax . random . split ( key ) return key , elecs_at_ions + init_width * jax . random . normal ( subkey , elecs_at_ions . shape , dtype = dtype )","title":"initialize_molecular_pos()"},{"location":"api/physics/core/#vmcnet.physics.core.combine_local_energy_terms","text":"Combine a sequence of local energy terms by adding them. Parameters: Name Type Description Default local_energy_terms Sequence sequence of local energy terms, each with the signature (params, x) -> array of terms of shape (x.shape[0],) required Returns: Type Description Callable local energy function which computes the sum of the local energy terms. Has the signature (params, x) -> local energy array of shape (x.shape[0],) Source code in vmcnet/physics/core.py def combine_local_energy_terms ( local_energy_terms : Sequence [ ModelApply [ P ]], ) -> ModelApply [ P ]: \"\"\"Combine a sequence of local energy terms by adding them. Args: local_energy_terms (Sequence): sequence of local energy terms, each with the signature (params, x) -> array of terms of shape (x.shape[0],) Returns: Callable: local energy function which computes the sum of the local energy terms. Has the signature (params, x) -> local energy array of shape (x.shape[0],) \"\"\" def local_energy_fn ( params : P , x : Array ) -> Array : local_energy_sum = local_energy_terms [ 0 ]( params , x ) for term in local_energy_terms [ 1 :]: local_energy_sum = cast ( Array , local_energy_sum + term ( params , x )) return local_energy_sum return local_energy_fn","title":"combine_local_energy_terms()"},{"location":"api/physics/core/#vmcnet.physics.core.laplacian_psi_over_psi","text":"Compute (nabla^2 psi) / psi at x given a function which evaluates psi'(x)/psi. The computation is done by computing (forward-mode) derivatives of the gradient to get the columns of the Hessian, and accumulating the (i, i)th entries (but this implementation is significantly more memory efficient than directly computing the Hessian). This function uses the identity (nabla^2 psi) / psi = (nabla^2 log|psi|) + (nabla log|psi|)^2 to avoid leaving the log domain during the computation. This function should be vmapped in order to be applied to batches of inputs, as it completely flattens x in order to take second derivatives w.r.t. each component. This is approach is extremely similar to the one in the FermiNet repo (in the jax branch, as of this writing -- see https://github.com/deepmind/ferminet/blob/aade61b3d30883b3238d6b50c85404d0e8176155/ferminet/hamiltonian.py). The main difference is that we are being explicit about the flattening of x within the Laplacian calculation, so that it does not have to be handled outside of this function (psi is free to take x shapes which are not flat). Parameters: Name Type Description Default grad_log_psi_apply Callable function which evaluates the derivative of log|psi(x)|, i.e. (nabla psi)(x) / psi(x), with respect to x. Has the signature (params, x) -> (nabla psi)(x) / psi(x), so the derivative should be over the second arg, x, and the output shape should be the same as x required params pytree model parameters, passed as the first arg of grad_log_psi required x Array second input to grad_log_psi required Returns: Type Description jnp.float32 \"local\" laplacian calculation, i.e. (nabla^2 psi) / psi Source code in vmcnet/physics/core.py def laplacian_psi_over_psi ( grad_log_psi_apply : ModelApply , params : P , x : Array , ) -> jnp . float32 : \"\"\"Compute (nabla^2 psi) / psi at x given a function which evaluates psi'(x)/psi. The computation is done by computing (forward-mode) derivatives of the gradient to get the columns of the Hessian, and accumulating the (i, i)th entries (but this implementation is significantly more memory efficient than directly computing the Hessian). This function uses the identity (nabla^2 psi) / psi = (nabla^2 log|psi|) + (nabla log|psi|)^2 to avoid leaving the log domain during the computation. This function should be vmapped in order to be applied to batches of inputs, as it completely flattens x in order to take second derivatives w.r.t. each component. This is approach is extremely similar to the one in the FermiNet repo (in the jax branch, as of this writing -- see https://github.com/deepmind/ferminet/blob/aade61b3d30883b3238d6b50c85404d0e8176155/ferminet/hamiltonian.py). The main difference is that we are being explicit about the flattening of x within the Laplacian calculation, so that it does not have to be handled outside of this function (psi is free to take x shapes which are not flat). Args: grad_log_psi_apply (Callable): function which evaluates the derivative of log|psi(x)|, i.e. (nabla psi)(x) / psi(x), with respect to x. Has the signature (params, x) -> (nabla psi)(x) / psi(x), so the derivative should be over the second arg, x, and the output shape should be the same as x params (pytree): model parameters, passed as the first arg of grad_log_psi x (Array): second input to grad_log_psi Returns: jnp.float32: \"local\" laplacian calculation, i.e. (nabla^2 psi) / psi \"\"\" x_shape = x . shape flat_x = jnp . reshape ( x , ( - 1 ,)) n = flat_x . shape [ 0 ] identity_mat = jnp . eye ( n ) def flattened_grad_log_psi_of_flat_x ( flat_x_in ): \"\"\"Flattened input to flattened output version of grad_log_psi.\"\"\" grad_log_psi_out = grad_log_psi_apply ( params , jnp . reshape ( flat_x_in , x_shape )) return jnp . reshape ( grad_log_psi_out , ( - 1 ,)) def step_fn ( carry , unused ): del unused i = carry [ 0 ] primals , tangents = jax . jvp ( flattened_grad_log_psi_of_flat_x , ( flat_x ,), ( identity_mat [ i ],) ) return ( i + 1 , carry [ 1 ] + jnp . square ( primals [ i ]) + tangents [ i ]), None out , _ = jax . lax . scan ( step_fn , ( 0 , 0.0 ), xs = None , length = n ) return out [ 1 ]","title":"laplacian_psi_over_psi()"},{"location":"api/physics/core/#vmcnet.physics.core.get_statistics_from_local_energy","text":"Collectively reduce local energies to an average energy and variance. Parameters: Name Type Description Default local_energies Array local energies of shape (nchains,), possibly distributed across multiple devices via utils.distribute.pmap. required nchains int total number of chains across all devices, used to compute a sample variance estimate of the local energy required nan_safe bool flag which controls if jnp.nanmean is used instead of jnp.mean. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. True Returns: Type Description (jnp.float32, jnp.float32) local energy average, local energy (sample) variance Source code in vmcnet/physics/core.py def get_statistics_from_local_energy ( local_energies : Array , nchains : int , nan_safe : bool = True ) -> Tuple [ jnp . float32 , jnp . float32 ]: \"\"\"Collectively reduce local energies to an average energy and variance. Args: local_energies (Array): local energies of shape (nchains,), possibly distributed across multiple devices via utils.distribute.pmap. nchains (int): total number of chains across all devices, used to compute a sample variance estimate of the local energy nan_safe (bool, optional): flag which controls if jnp.nanmean is used instead of jnp.mean. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. Returns: (jnp.float32, jnp.float32): local energy average, local energy (sample) variance \"\"\" # TODO(Jeffmin) might be worth investigating the numerical stability of the XLA # compiled version of these two computations, since the quality of the gradients # is fairly crucial to the success of the algorithm if nan_safe : allreduce_mean = utils . distribute . nanmean_all_local_devices else : allreduce_mean = utils . distribute . mean_all_local_devices energy = allreduce_mean ( local_energies ) variance = ( allreduce_mean ( jnp . square ( local_energies - energy )) * nchains / ( nchains - 1 ) ) # adjust by n / (n - 1) to get an unbiased estimator return energy , variance","title":"get_statistics_from_local_energy()"},{"location":"api/physics/core/#vmcnet.physics.core.get_default_energy_bwd","text":"Use a standard variance reduction formula to get the bwd pass of the energy. The formula is 2 * E_p[(local_e - E_p[local_e]) * grad_log_psi], where the symbol E_p[] refers to the expectation over the probability distribution p defined by p(x) = |psi(x)|^2 / . This is an unbiased estimator of the gradient of E_p[local_e], and has a lower variance than directly differentiating E_p[local_e] with respect to the parameters. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required mean_grad_fn Callable function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. required Returns: Type Description Callable function which computes the backward pass in the custom vjp of the total energy. Has the signature (res, cotangents) -> (gradients, None) Source code in vmcnet/physics/core.py def get_default_energy_bwd ( log_psi_apply : ModelApply [ P ], mean_grad_fn : Callable [[ Array ], Array ], ): \"\"\"Use a standard variance reduction formula to get the bwd pass of the energy. The formula is 2 * E_p[(local_e - E_p[local_e]) * grad_log_psi], where the symbol E_p[] refers to the expectation over the probability distribution p defined by p(x) = |psi(x)|^2 / <psi | psi>. This is an unbiased estimator of the gradient of E_p[local_e], and has a lower variance than directly differentiating E_p[local_e] with respect to the parameters. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| mean_grad_fn (Callable): function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. Returns: Callable: function which computes the backward pass in the custom vjp of the total energy. Has the signature (res, cotangents) -> (gradients, None) \"\"\" def scaled_by_local_e ( params : P , positions : Array , centered_local_energies : Array ) -> jnp . float32 : log_psi = log_psi_apply ( params , positions ) loss_functions . register_normal_predictive_distribution ( log_psi [:, None ]) return 2.0 * mean_grad_fn ( centered_local_energies * log_psi ) _get_energy_grad = jax . grad ( scaled_by_local_e , argnums = 0 ) def energy_bwd ( res , cotangents ) -> Tuple [ P , None ]: energy , local_energies , params , positions = res centered_local_energies = local_energies - energy gradient = _get_energy_grad ( params , positions , centered_local_energies ) return jax . tree_map ( lambda x : x * cotangents [ 0 ], gradient ), None return energy_bwd","title":"get_default_energy_bwd()"},{"location":"api/physics/core/#vmcnet.physics.core.create_value_and_grad_energy_fn","text":"Create a function which computes unbiased energy gradients. Due to the Hermiticity of the Hamiltonian, we can get an unbiased lower variance estimate of the gradient of the expected energy than the naive gradient of the mean of sampled local energies. Specifically, the gradient of the expected energy expect[E_L] takes the form 2 * expect[(E_L - expect[E_L]) * (grad_psi / psi)(x)], where E_L is the local energy and expect[] denotes the expectation with respect to the distribution |psi|^2. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required local_energy_fn Callable computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) required nchains int total number of chains across all devices, used to compute a sample variance estimate of the local energy required clipping_fn Callable post-processing function on the local energy, e.g. a function which clips the values to be within some multiple of the total variation from the median. The post-processed values are used for the gradient calculation, if available. Defaults to None. None nan_safe bool flag which controls if jnp.nanmean and jnp.nansum are used instead of jnp.mean and jnp.sum for the terms in the gradient calculation. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. True get_energy_bwd Callable function which returns a custom backward pass for the total energy calculation. Has the signature (log_psi_apply, mean_grad_fn) -> energy_bwd. Defaults to get_default_energy_bwd, which computes the formula above. <function get_default_energy_bwd at 0x7ff1729ab940> Returns: Type Description Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) Source code in vmcnet/physics/core.py def create_value_and_grad_energy_fn ( log_psi_apply : ModelApply [ P ], local_energy_fn : ModelApply [ P ], nchains : int , clipping_fn : Optional [ Callable [[ Array ], Array ]] = None , nan_safe : bool = True , get_energy_bwd : Callable = get_default_energy_bwd , ) -> ValueGradEnergyFn [ P ]: \"\"\"Create a function which computes unbiased energy gradients. Due to the Hermiticity of the Hamiltonian, we can get an unbiased lower variance estimate of the gradient of the expected energy than the naive gradient of the mean of sampled local energies. Specifically, the gradient of the expected energy expect[E_L] takes the form 2 * expect[(E_L - expect[E_L]) * (grad_psi / psi)(x)], where E_L is the local energy and expect[] denotes the expectation with respect to the distribution |psi|^2. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| local_energy_fn (Callable): computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) nchains (int): total number of chains across all devices, used to compute a sample variance estimate of the local energy clipping_fn (Callable, optional): post-processing function on the local energy, e.g. a function which clips the values to be within some multiple of the total variation from the median. The post-processed values are used for the gradient calculation, if available. Defaults to None. nan_safe (bool, optional): flag which controls if jnp.nanmean and jnp.nansum are used instead of jnp.mean and jnp.sum for the terms in the gradient calculation. Can be set to False when debugging if trying to find the source of unexpected nans. Defaults to True. get_energy_bwd (Callable): function which returns a custom backward pass for the total energy calculation. Has the signature (log_psi_apply, mean_grad_fn) -> energy_bwd. Defaults to get_default_energy_bwd, which computes the formula above. Returns: Callable: function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) \"\"\" @jax . custom_vjp def compute_energy_data ( params : P , positions : Array ) -> EnergyData : local_energies_noclip = local_energy_fn ( params , positions ) if clipping_fn is not None : local_energies = clipping_fn ( local_energies_noclip ) energy , variance = get_statistics_from_local_energy ( local_energies , nchains , nan_safe = nan_safe ) # For the unclipped metrics, which are not used in the gradient, don't # do these in a nan-safe way. This makes nans more visible and makes sure # the command-line checkpoint_if_nans flag will work properly. energy_noclip , variance_noclip = get_statistics_from_local_energy ( local_energies_noclip , nchains , nan_safe = False ) aux_data = ( variance , local_energies , energy_noclip , variance_noclip ) else : local_energies = local_energies_noclip energy , variance = get_statistics_from_local_energy ( local_energies , nchains , nan_safe = nan_safe ) # Even though there's no clipping function, still record noclip metrics # without nan-safety so that checkpointing epochs with nans can be # supported. energy_noclip , variance_noclip = get_statistics_from_local_energy ( local_energies , nchains , nan_safe = False ) aux_data = ( variance , local_energies , energy_noclip , variance_noclip ) return energy , aux_data def energy_fwd ( params : P , positions : Array ): output = compute_energy_data ( params , positions ) energy = output [ 0 ] local_energies = output [ 1 ][ 1 ] return output , ( energy , local_energies , params , positions ) mean_grad_fn = utils . distribute . get_mean_over_first_axis_fn ( nan_safe = nan_safe ) energy_bwd = get_energy_bwd ( log_psi_apply , mean_grad_fn ) compute_energy_data . defvjp ( energy_fwd , energy_bwd ) energy_data_val_and_grad = jax . value_and_grad ( compute_energy_data , argnums = 0 , has_aux = True ) return energy_data_val_and_grad","title":"create_value_and_grad_energy_fn()"},{"location":"api/physics/kinetic/","text":"Kinetic energy terms. create_continuous_kinetic_energy ( log_psi_apply ) Create the local kinetic energy fn (params, x) -> -0.5 (nabla^2 psi(x) / psi(x)). Parameters: Name Type Description Default log_psi_apply Callable a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| required Returns: Type Description Callable function which computes the local kinetic energy for continuous problems (as opposed to discrete/lattice problems), i.e. -0.5 nabla^2 psi / psi. Evaluates on batches due to the jax.vmap call, so it has signature (params, x) -> kinetic energy array with shape (x.shape[0],) Source code in vmcnet/physics/kinetic.py def create_continuous_kinetic_energy ( log_psi_apply : Callable [[ P , Array ], Union [ jnp . float32 , Array ]] ) -> ModelApply [ P ]: \"\"\"Create the local kinetic energy fn (params, x) -> -0.5 (nabla^2 psi(x) / psi(x)). Args: log_psi_apply (Callable): a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| Returns: Callable: function which computes the local kinetic energy for continuous problems (as opposed to discrete/lattice problems), i.e. -0.5 nabla^2 psi / psi. Evaluates on batches due to the jax.vmap call, so it has signature (params, x) -> kinetic energy array with shape (x.shape[0],) \"\"\" grad_log_psi_apply = jax . grad ( log_psi_apply , argnums = 1 ) def kinetic_energy_fn ( params : P , x : Array ) -> jnp . float32 : return - 0.5 * physics . core . laplacian_psi_over_psi ( grad_log_psi_apply , params , x ) return jax . vmap ( kinetic_energy_fn , in_axes = ( None , 0 ), out_axes = 0 )","title":"kinetic"},{"location":"api/physics/kinetic/#vmcnet.physics.kinetic.create_continuous_kinetic_energy","text":"Create the local kinetic energy fn (params, x) -> -0.5 (nabla^2 psi(x) / psi(x)). Parameters: Name Type Description Default log_psi_apply Callable a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| required Returns: Type Description Callable function which computes the local kinetic energy for continuous problems (as opposed to discrete/lattice problems), i.e. -0.5 nabla^2 psi / psi. Evaluates on batches due to the jax.vmap call, so it has signature (params, x) -> kinetic energy array with shape (x.shape[0],) Source code in vmcnet/physics/kinetic.py def create_continuous_kinetic_energy ( log_psi_apply : Callable [[ P , Array ], Union [ jnp . float32 , Array ]] ) -> ModelApply [ P ]: \"\"\"Create the local kinetic energy fn (params, x) -> -0.5 (nabla^2 psi(x) / psi(x)). Args: log_psi_apply (Callable): a function which computes log|psi(x)| for single inputs x. It is okay for it to produce batch outputs on batches of x as long as it produces a single number for single x. Has the signature (params, single_x_in) -> log|psi(single_x_in)| Returns: Callable: function which computes the local kinetic energy for continuous problems (as opposed to discrete/lattice problems), i.e. -0.5 nabla^2 psi / psi. Evaluates on batches due to the jax.vmap call, so it has signature (params, x) -> kinetic energy array with shape (x.shape[0],) \"\"\" grad_log_psi_apply = jax . grad ( log_psi_apply , argnums = 1 ) def kinetic_energy_fn ( params : P , x : Array ) -> jnp . float32 : return - 0.5 * physics . core . laplacian_psi_over_psi ( grad_log_psi_apply , params , x ) return jax . vmap ( kinetic_energy_fn , in_axes = ( None , 0 ), out_axes = 0 )","title":"create_continuous_kinetic_energy()"},{"location":"api/physics/potential/","text":"Potential energy terms. create_electron_ion_coulomb_potential ( ion_locations , ion_charges , strength = 1.0 , softening_term = 0.0 ) Computes the total coulomb potential attraction between electron and ion. Parameters: Name Type Description Default ion_locations Array an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in required ion_charges Array an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) required strength jnp.float32 amount to multiply the overall interaction by. Defaults to 1.0. 1.0 softening_term jnp.float32 this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. 0.0 Returns: Type Description Callable function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] Source code in vmcnet/physics/potential.py def create_electron_ion_coulomb_potential ( ion_locations : Array , ion_charges : Array , strength : jnp . float32 = 1.0 , softening_term : jnp . float32 = 0.0 , ) -> ModelApply [ ModelParams ]: \"\"\"Computes the total coulomb potential attraction between electron and ion. Args: ion_locations (Array): an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in ion_charges (Array): an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) strength (jnp.float32, optional): amount to multiply the overall interaction by. Defaults to 1.0. softening_term (jnp.float32, optional): this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. Returns: Callable: function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] \"\"\" def potential_fn ( params : ModelParams , x : Array ) -> Array : del params electron_ion_displacements = _compute_displacements ( x , ion_locations ) electron_ion_distances = _compute_soft_norm ( electron_ion_displacements , softening_term = softening_term ) coulomb_attraction = ion_charges / electron_ion_distances return - strength * jnp . sum ( coulomb_attraction , axis = ( - 1 , - 2 )) return potential_fn create_electron_electron_coulomb_potential ( strength = 1.0 , softening_term = 0.0 ) Computes the total coulomb potential repulsion between pairs of electrons. Parameters: Name Type Description Default strength jnp.float32 amount to multiply the overall interaction by. Defaults to 1.0. 1.0 softening_term jnp.float32 this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. 0.0 Returns: Type Description Callable function which computes the potential energy due to the repulsion between pairs of electrons. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] Source code in vmcnet/physics/potential.py def create_electron_electron_coulomb_potential ( strength : jnp . float32 = 1.0 , softening_term : jnp . float32 = 0.0 ) -> ModelApply [ ModelParams ]: \"\"\"Computes the total coulomb potential repulsion between pairs of electrons. Args: strength (jnp.float32, optional): amount to multiply the overall interaction by. Defaults to 1.0. softening_term (jnp.float32, optional): this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. Returns: Callable: function which computes the potential energy due to the repulsion between pairs of electrons. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] \"\"\" def potential_fn ( params : ModelParams , x : Array ) -> Array : del params electron_electron_displacements = _compute_displacements ( x , x ) electron_electron_distances = _compute_soft_norm ( electron_electron_displacements , softening_term = softening_term ) return jnp . sum ( jnp . triu ( strength / electron_electron_distances , k = 1 ), axis = ( - 1 , - 2 ) ) return potential_fn create_ion_ion_coulomb_potential ( ion_locations , ion_charges ) Computes the total coulomb potential repulsion between stationary ions. Parameters: Name Type Description Default ion_locations Array an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in required ion_charges Array an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) required Returns: Type Description Callable function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] Source code in vmcnet/physics/potential.py def create_ion_ion_coulomb_potential ( ion_locations : Array , ion_charges : Array ) -> ModelApply [ ModelParams ]: \"\"\"Computes the total coulomb potential repulsion between stationary ions. Args: ion_locations (Array): an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in ion_charges (Array): an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) Returns: Callable: function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] \"\"\" ion_ion_displacements , charge_charge_prods = _get_ion_ion_info ( ion_locations , ion_charges ) ion_ion_distances = _compute_soft_norm ( ion_ion_displacements ) constant_potential = jnp . sum ( jnp . triu ( charge_charge_prods / ion_ion_distances , k = 1 ), axis = ( - 1 , - 2 ) ) def potential_fn ( params : ModelParams , x : Array ) -> Array : del params , x return constant_potential return potential_fn","title":"potential"},{"location":"api/physics/potential/#vmcnet.physics.potential.create_electron_ion_coulomb_potential","text":"Computes the total coulomb potential attraction between electron and ion. Parameters: Name Type Description Default ion_locations Array an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in required ion_charges Array an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) required strength jnp.float32 amount to multiply the overall interaction by. Defaults to 1.0. 1.0 softening_term jnp.float32 this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. 0.0 Returns: Type Description Callable function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] Source code in vmcnet/physics/potential.py def create_electron_ion_coulomb_potential ( ion_locations : Array , ion_charges : Array , strength : jnp . float32 = 1.0 , softening_term : jnp . float32 = 0.0 , ) -> ModelApply [ ModelParams ]: \"\"\"Computes the total coulomb potential attraction between electron and ion. Args: ion_locations (Array): an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in ion_charges (Array): an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) strength (jnp.float32, optional): amount to multiply the overall interaction by. Defaults to 1.0. softening_term (jnp.float32, optional): this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. Returns: Callable: function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] \"\"\" def potential_fn ( params : ModelParams , x : Array ) -> Array : del params electron_ion_displacements = _compute_displacements ( x , ion_locations ) electron_ion_distances = _compute_soft_norm ( electron_ion_displacements , softening_term = softening_term ) coulomb_attraction = ion_charges / electron_ion_distances return - strength * jnp . sum ( coulomb_attraction , axis = ( - 1 , - 2 )) return potential_fn","title":"create_electron_ion_coulomb_potential()"},{"location":"api/physics/potential/#vmcnet.physics.potential.create_electron_electron_coulomb_potential","text":"Computes the total coulomb potential repulsion between pairs of electrons. Parameters: Name Type Description Default strength jnp.float32 amount to multiply the overall interaction by. Defaults to 1.0. 1.0 softening_term jnp.float32 this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. 0.0 Returns: Type Description Callable function which computes the potential energy due to the repulsion between pairs of electrons. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] Source code in vmcnet/physics/potential.py def create_electron_electron_coulomb_potential ( strength : jnp . float32 = 1.0 , softening_term : jnp . float32 = 0.0 ) -> ModelApply [ ModelParams ]: \"\"\"Computes the total coulomb potential repulsion between pairs of electrons. Args: strength (jnp.float32, optional): amount to multiply the overall interaction by. Defaults to 1.0. softening_term (jnp.float32, optional): this amount squared is added to sum_i x_i^2 before taking the sqrt in the norm calculation. When zero, the usual vector 2-norm is used to compute distance. Defaults to 0.0. Returns: Callable: function which computes the potential energy due to the repulsion between pairs of electrons. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] \"\"\" def potential_fn ( params : ModelParams , x : Array ) -> Array : del params electron_electron_displacements = _compute_displacements ( x , x ) electron_electron_distances = _compute_soft_norm ( electron_electron_displacements , softening_term = softening_term ) return jnp . sum ( jnp . triu ( strength / electron_electron_distances , k = 1 ), axis = ( - 1 , - 2 ) ) return potential_fn","title":"create_electron_electron_coulomb_potential()"},{"location":"api/physics/potential/#vmcnet.physics.potential.create_ion_ion_coulomb_potential","text":"Computes the total coulomb potential repulsion between stationary ions. Parameters: Name Type Description Default ion_locations Array an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in required ion_charges Array an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) required Returns: Type Description Callable function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] Source code in vmcnet/physics/potential.py def create_ion_ion_coulomb_potential ( ion_locations : Array , ion_charges : Array ) -> ModelApply [ ModelParams ]: \"\"\"Computes the total coulomb potential repulsion between stationary ions. Args: ion_locations (Array): an (n, d) array of ion positions, where n is the number of ion positions and d is the dimension of the space they live in ion_charges (Array): an (n,) array of ion charges, in units of one elementary charge (the charge of one electron) Returns: Callable: function which computes the potential energy due to the attraction between electrons and ion. Has the signature (params, electron_positions of shape (..., n_elec, d)) -> array of potential energies of shape electron_positions.shape[:-2] \"\"\" ion_ion_displacements , charge_charge_prods = _get_ion_ion_info ( ion_locations , ion_charges ) ion_ion_distances = _compute_soft_norm ( ion_ion_displacements ) constant_potential = jnp . sum ( jnp . triu ( charge_charge_prods / ion_ion_distances , k = 1 ), axis = ( - 1 , - 2 ) ) def potential_fn ( params : ModelParams , x : Array ) -> Array : del params , x return constant_potential return potential_fn","title":"create_ion_ion_coulomb_potential()"},{"location":"api/train/default_config/","text":"Create configuration of hyperparameters. get_default_reload_config () Make a default reload configuration (no logdir but valid defaults otherwise). Source code in vmcnet/train/default_config.py def get_default_reload_config () -> ConfigDict : \"\"\"Make a default reload configuration (no logdir but valid defaults otherwise).\"\"\" return ConfigDict ( { \"logdir\" : NO_RELOAD_LOG_DIR , \"use_config_file\" : True , \"config_relative_file_path\" : DEFAULT_CONFIG_FILE_NAME , \"use_checkpoint_file\" : True , \"checkpoint_relative_file_path\" : CHECKPOINT_FILE_NAME , } ) get_default_config () Make a default configuration (single det FermiNet on LiH). Source code in vmcnet/train/default_config.py def get_default_config () -> ConfigDict : \"\"\"Make a default configuration (single det FermiNet on LiH).\"\"\" config = ConfigDict ( _copy_all_dicts ( { \"problem\" : get_default_molecular_config (), \"model\" : get_default_model_config (), \"vmc\" : get_default_vmc_config (), \"eval\" : get_default_eval_config (), \"logdir\" : os . path . join ( os . curdir , # this will be relative to the calling script \"logs\" , ), # if save_to_current_datetime_subfolder=True, will log into a subfolder # named according to the datetime at start \"save_to_current_datetime_subfolder\" : True , \"logging_level\" : \"WARNING\" , \"dtype\" : \"float32\" , \"distribute\" : True , \"debug_nans\" : False , # If true, OVERRIDES config.distribute to be False \"initial_seed\" : 0 , } ) ) return config choose_model_type_in_model_config ( model_config ) Given a model config with a specified type, select the specified model. The default config contains architecture hyperparameters for several types of models (in order to support command-line overwriting via absl.flags), but only one needs to be retained after the model type is chosen at the beginning of a run, so this function returns a ConfigDict with only the hyperparams associated with the model in model_config.type. Source code in vmcnet/train/default_config.py def choose_model_type_in_model_config ( model_config ): \"\"\"Given a model config with a specified type, select the specified model. The default config contains architecture hyperparameters for several types of models (in order to support command-line overwriting via absl.flags), but only one needs to be retained after the model type is chosen at the beginning of a run, so this function returns a ConfigDict with only the hyperparams associated with the model in model_config.type. \"\"\" model_type = model_config . type model_config = model_config [ model_type ] model_config . type = model_type return model_config get_default_model_config () Get a default model configuration from a model type. Source code in vmcnet/train/default_config.py def get_default_model_config () -> Dict : \"\"\"Get a default model configuration from a model type.\"\"\" orthogonal_init = { \"type\" : \"orthogonal\" , \"scale\" : 1.0 } normal_init = { \"type\" : \"normal\" } # tie together the values of ferminet_backflow.cyclic_spins and # invariance.cyclic_spins cyclic_spins = FieldReference ( False ) input_streams = { \"include_2e_stream\" : True , \"include_ei_norm\" : True , \"include_ee_norm\" : True , } base_backflow_config = { \"kernel_init_unmixed\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"kernel_init_mixed\" : orthogonal_init , \"kernel_init_2e_1e_stream\" : orthogonal_init , \"kernel_init_2e_2e_stream\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_init_1e_stream\" : normal_init , \"bias_init_2e_stream\" : normal_init , \"activation_fn\" : \"tanh\" , \"use_bias\" : True , \"one_electron_skip\" : True , \"one_electron_skip_scale\" : 1.0 , \"two_electron_skip\" : True , \"two_electron_skip_scale\" : 1.0 , \"cyclic_spins\" : cyclic_spins , } ferminet_backflow = { \"ndense_list\" : (( 256 , 16 ), ( 256 , 16 ), ( 256 , 16 ), ( 256 ,)), ** base_backflow_config , } determinant_resnet = { \"ndense\" : 10 , \"nlayers\" : 3 , \"activation\" : \"gelu\" , \"kernel_init\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_init\" : normal_init , \"use_bias\" : True , \"register_kfac\" : False , \"mode\" : \"parallel_even\" , } base_ferminet_config = { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"ndeterminants\" : 1 , \"kernel_init_orbital_linear\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"kernel_init_envelope_dim\" : { \"type\" : \"ones\" }, \"kernel_init_envelope_ion\" : { \"type\" : \"ones\" }, \"bias_init_orbital_linear\" : normal_init , \"orbitals_use_bias\" : True , \"isotropic_decay\" : True , \"use_det_resnet\" : False , \"det_resnet\" : determinant_resnet , \"determinant_fn_mode\" : \"parallel_even\" , \"full_det\" : False , } invariance_for_antieq = { \"ndense_list\" : (( 32 ,), ( 32 ,), ( 1 ,)), ** base_backflow_config , } antieq_config = { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"kernel_init_orbital_linear\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"kernel_init_envelope_dim\" : { \"type\" : \"ones\" }, \"kernel_init_envelope_ion\" : { \"type\" : \"ones\" }, \"bias_init_orbital_linear\" : normal_init , \"orbitals_use_bias\" : True , \"isotropic_decay\" : True , \"use_products_covariance\" : True , \"invariance\" : invariance_for_antieq , \"products_covariance\" : { \"kernel_init\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"register_kfac\" : True , \"use_weights\" : False , }, \"multiply_by_eq_features\" : False , } config = { \"type\" : \"ferminet\" , \"ferminet\" : base_ferminet_config , \"embedded_particle_ferminet\" : { ** base_ferminet_config , \"nhidden_fermions_per_spin\" : ( 2 , 2 ), \"invariance\" : { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"kernel_initializer\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_initializer\" : normal_init , \"use_bias\" : True , \"register_kfac\" : True , }, }, \"extended_orbital_matrix_ferminet\" : { ** base_ferminet_config , \"nhidden_fermions_per_spin\" : ( 2 , 2 ), \"use_separate_invariance_backflow\" : False , \"invariance\" : { \"backflow\" : ferminet_backflow , \"kernel_initializer\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_initializer\" : normal_init , \"use_bias\" : True , \"register_kfac\" : True , }, }, # TODO (ggoldsh): these two should probably be subtypes of a single # \"antiequivariance\" model type \"orbital_cofactor_net\" : antieq_config , \"per_particle_dets_net\" : antieq_config , \"explicit_antisym\" : { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"antisym_type\" : \"generic\" , # factorized or generic \"rank\" : 1 , # Only relevant for antisym_type=factorized \"ndense_resnet\" : 64 , \"nlayers_resnet\" : 2 , \"kernel_init_resnet\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_init_resnet\" : normal_init , \"activation_fn_resnet\" : \"tanh\" , \"resnet_use_bias\" : True , \"jastrow\" : { # type must be a value in models.jastrow.VALID_JASTROW_TYPES \"type\" : \"backflow_based\" , \"one_body_decay\" : { \"kernel_init\" : { \"type\" : \"ones\" }}, \"two_body_decay\" : { \"init_ee_strength\" : 1.0 , \"trainable\" : True }, \"backflow_based\" : { \"use_separate_jastrow_backflow\" : True , \"backflow\" : { \"ndense_list\" : (( 256 , 16 ), ( 256 , 16 ), ( 256 , 16 ), ( 256 ,)), \"kernel_init_unmixed\" : orthogonal_init , \"kernel_init_mixed\" : orthogonal_init , \"kernel_init_2e_1e_stream\" : orthogonal_init , \"kernel_init_2e_2e_stream\" : orthogonal_init , \"bias_init_1e_stream\" : normal_init , \"bias_init_2e_stream\" : normal_init , \"activation_fn\" : \"gelu\" , \"use_bias\" : True , \"one_electron_skip\" : True , \"one_electron_skip_scale\" : 1.0 , \"two_electron_skip\" : True , \"two_electron_skip_scale\" : 1.0 , \"cyclic_spins\" : cyclic_spins , }, }, }, }, } return config get_default_molecular_config () Get a default molecular configuration (LiH). Source code in vmcnet/train/default_config.py def get_default_molecular_config () -> Dict : \"\"\"Get a default molecular configuration (LiH).\"\"\" problem_config = { \"ion_pos\" : (( 0.0 , 0.0 , - 1.5069621 ), ( 0.0 , 0.0 , 1.5069621 )), \"ion_charges\" : ( 1.0 , 3.0 ), \"nelec\" : ( 2 , 2 ), } return problem_config get_default_vmc_config () Get a default VMC training configuration. Source code in vmcnet/train/default_config.py def get_default_vmc_config () -> Dict : \"\"\"Get a default VMC training configuration.\"\"\" vmc_config = { \"nchains\" : 2000 , \"nepochs\" : 200000 , \"nburn\" : 5000 , \"nsteps_per_param_update\" : 10 , \"nmoves_per_width_update\" : 100 , \"std_move\" : 0.25 , \"checkpoint_every\" : 5000 , \"best_checkpoint_every\" : 100 , \"checkpoint_dir\" : \"checkpoints\" , \"checkpoint_variance_scale\" : 10 , \"checkpoint_if_nans\" : False , \"only_checkpoint_first_nans\" : True , \"nhistory_max\" : 200 , \"record_amplitudes\" : False , \"record_param_l1_norm\" : False , \"clip_threshold\" : 5.0 , \"nan_safe\" : True , \"optimizer_type\" : \"kfac\" , \"optimizer\" : { \"kfac\" : { \"l2_reg\" : 0.0 , \"norm_constraint\" : 0.001 , \"curvature_ema\" : 0.95 , \"inverse_update_period\" : 1 , \"min_damping\" : 1e-4 , \"register_only_generic\" : False , \"estimation_mode\" : \"fisher_exact\" , \"damping\" : 0.001 , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , \"learning_decay_rate\" : 1e-4 , }, \"adam\" : { \"b1\" : 0.9 , \"b2\" : 0.999 , \"eps\" : 1e-8 , \"eps_root\" : 0.0 , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , \"learning_decay_rate\" : 1e-4 , }, \"sgd\" : { \"momentum\" : 0.0 , \"nesterov\" : False , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , \"learning_decay_rate\" : 1e-4 , }, \"sr\" : { \"damping\" : 1.0 , # needs to be tuned with everything else \"maxiter\" : 10 , # when maxiter <= -1, uses default 10 * nparams \"descent_type\" : \"sgd\" , \"norm_constraint\" : 0.001 , \"mode\" : \"lazy\" , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , # needs to be tuned with everything else \"learning_decay_rate\" : 1e-4 , }, }, } return vmc_config get_default_eval_config () Get a default evaluation configuration. Source code in vmcnet/train/default_config.py def get_default_eval_config () -> Dict : \"\"\"Get a default evaluation configuration.\"\"\" eval_config = { \"nchains\" : 2000 , \"nburn\" : 5000 , \"nepochs\" : 20000 , \"nsteps_per_param_update\" : 10 , \"nmoves_per_width_update\" : 100 , \"record_amplitudes\" : False , \"std_move\" : 0.25 , # if use_data_from_training=True, nchains, nmoves_per_width_update, and # std_move are completely ignored, and the data output from training is # used as the initial positions instead \"use_data_from_training\" : False , \"record_local_energies\" : True , # save local energies and compute statistics \"nan_safe\" : False , } return eval_config","title":"default_config"},{"location":"api/train/default_config/#vmcnet.train.default_config.get_default_reload_config","text":"Make a default reload configuration (no logdir but valid defaults otherwise). Source code in vmcnet/train/default_config.py def get_default_reload_config () -> ConfigDict : \"\"\"Make a default reload configuration (no logdir but valid defaults otherwise).\"\"\" return ConfigDict ( { \"logdir\" : NO_RELOAD_LOG_DIR , \"use_config_file\" : True , \"config_relative_file_path\" : DEFAULT_CONFIG_FILE_NAME , \"use_checkpoint_file\" : True , \"checkpoint_relative_file_path\" : CHECKPOINT_FILE_NAME , } )","title":"get_default_reload_config()"},{"location":"api/train/default_config/#vmcnet.train.default_config.get_default_config","text":"Make a default configuration (single det FermiNet on LiH). Source code in vmcnet/train/default_config.py def get_default_config () -> ConfigDict : \"\"\"Make a default configuration (single det FermiNet on LiH).\"\"\" config = ConfigDict ( _copy_all_dicts ( { \"problem\" : get_default_molecular_config (), \"model\" : get_default_model_config (), \"vmc\" : get_default_vmc_config (), \"eval\" : get_default_eval_config (), \"logdir\" : os . path . join ( os . curdir , # this will be relative to the calling script \"logs\" , ), # if save_to_current_datetime_subfolder=True, will log into a subfolder # named according to the datetime at start \"save_to_current_datetime_subfolder\" : True , \"logging_level\" : \"WARNING\" , \"dtype\" : \"float32\" , \"distribute\" : True , \"debug_nans\" : False , # If true, OVERRIDES config.distribute to be False \"initial_seed\" : 0 , } ) ) return config","title":"get_default_config()"},{"location":"api/train/default_config/#vmcnet.train.default_config.choose_model_type_in_model_config","text":"Given a model config with a specified type, select the specified model. The default config contains architecture hyperparameters for several types of models (in order to support command-line overwriting via absl.flags), but only one needs to be retained after the model type is chosen at the beginning of a run, so this function returns a ConfigDict with only the hyperparams associated with the model in model_config.type. Source code in vmcnet/train/default_config.py def choose_model_type_in_model_config ( model_config ): \"\"\"Given a model config with a specified type, select the specified model. The default config contains architecture hyperparameters for several types of models (in order to support command-line overwriting via absl.flags), but only one needs to be retained after the model type is chosen at the beginning of a run, so this function returns a ConfigDict with only the hyperparams associated with the model in model_config.type. \"\"\" model_type = model_config . type model_config = model_config [ model_type ] model_config . type = model_type return model_config","title":"choose_model_type_in_model_config()"},{"location":"api/train/default_config/#vmcnet.train.default_config.get_default_model_config","text":"Get a default model configuration from a model type. Source code in vmcnet/train/default_config.py def get_default_model_config () -> Dict : \"\"\"Get a default model configuration from a model type.\"\"\" orthogonal_init = { \"type\" : \"orthogonal\" , \"scale\" : 1.0 } normal_init = { \"type\" : \"normal\" } # tie together the values of ferminet_backflow.cyclic_spins and # invariance.cyclic_spins cyclic_spins = FieldReference ( False ) input_streams = { \"include_2e_stream\" : True , \"include_ei_norm\" : True , \"include_ee_norm\" : True , } base_backflow_config = { \"kernel_init_unmixed\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"kernel_init_mixed\" : orthogonal_init , \"kernel_init_2e_1e_stream\" : orthogonal_init , \"kernel_init_2e_2e_stream\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_init_1e_stream\" : normal_init , \"bias_init_2e_stream\" : normal_init , \"activation_fn\" : \"tanh\" , \"use_bias\" : True , \"one_electron_skip\" : True , \"one_electron_skip_scale\" : 1.0 , \"two_electron_skip\" : True , \"two_electron_skip_scale\" : 1.0 , \"cyclic_spins\" : cyclic_spins , } ferminet_backflow = { \"ndense_list\" : (( 256 , 16 ), ( 256 , 16 ), ( 256 , 16 ), ( 256 ,)), ** base_backflow_config , } determinant_resnet = { \"ndense\" : 10 , \"nlayers\" : 3 , \"activation\" : \"gelu\" , \"kernel_init\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_init\" : normal_init , \"use_bias\" : True , \"register_kfac\" : False , \"mode\" : \"parallel_even\" , } base_ferminet_config = { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"ndeterminants\" : 1 , \"kernel_init_orbital_linear\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"kernel_init_envelope_dim\" : { \"type\" : \"ones\" }, \"kernel_init_envelope_ion\" : { \"type\" : \"ones\" }, \"bias_init_orbital_linear\" : normal_init , \"orbitals_use_bias\" : True , \"isotropic_decay\" : True , \"use_det_resnet\" : False , \"det_resnet\" : determinant_resnet , \"determinant_fn_mode\" : \"parallel_even\" , \"full_det\" : False , } invariance_for_antieq = { \"ndense_list\" : (( 32 ,), ( 32 ,), ( 1 ,)), ** base_backflow_config , } antieq_config = { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"kernel_init_orbital_linear\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"kernel_init_envelope_dim\" : { \"type\" : \"ones\" }, \"kernel_init_envelope_ion\" : { \"type\" : \"ones\" }, \"bias_init_orbital_linear\" : normal_init , \"orbitals_use_bias\" : True , \"isotropic_decay\" : True , \"use_products_covariance\" : True , \"invariance\" : invariance_for_antieq , \"products_covariance\" : { \"kernel_init\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"register_kfac\" : True , \"use_weights\" : False , }, \"multiply_by_eq_features\" : False , } config = { \"type\" : \"ferminet\" , \"ferminet\" : base_ferminet_config , \"embedded_particle_ferminet\" : { ** base_ferminet_config , \"nhidden_fermions_per_spin\" : ( 2 , 2 ), \"invariance\" : { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"kernel_initializer\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_initializer\" : normal_init , \"use_bias\" : True , \"register_kfac\" : True , }, }, \"extended_orbital_matrix_ferminet\" : { ** base_ferminet_config , \"nhidden_fermions_per_spin\" : ( 2 , 2 ), \"use_separate_invariance_backflow\" : False , \"invariance\" : { \"backflow\" : ferminet_backflow , \"kernel_initializer\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_initializer\" : normal_init , \"use_bias\" : True , \"register_kfac\" : True , }, }, # TODO (ggoldsh): these two should probably be subtypes of a single # \"antiequivariance\" model type \"orbital_cofactor_net\" : antieq_config , \"per_particle_dets_net\" : antieq_config , \"explicit_antisym\" : { \"input_streams\" : input_streams , \"backflow\" : ferminet_backflow , \"antisym_type\" : \"generic\" , # factorized or generic \"rank\" : 1 , # Only relevant for antisym_type=factorized \"ndense_resnet\" : 64 , \"nlayers_resnet\" : 2 , \"kernel_init_resnet\" : { \"type\" : \"orthogonal\" , \"scale\" : 2.0 }, \"bias_init_resnet\" : normal_init , \"activation_fn_resnet\" : \"tanh\" , \"resnet_use_bias\" : True , \"jastrow\" : { # type must be a value in models.jastrow.VALID_JASTROW_TYPES \"type\" : \"backflow_based\" , \"one_body_decay\" : { \"kernel_init\" : { \"type\" : \"ones\" }}, \"two_body_decay\" : { \"init_ee_strength\" : 1.0 , \"trainable\" : True }, \"backflow_based\" : { \"use_separate_jastrow_backflow\" : True , \"backflow\" : { \"ndense_list\" : (( 256 , 16 ), ( 256 , 16 ), ( 256 , 16 ), ( 256 ,)), \"kernel_init_unmixed\" : orthogonal_init , \"kernel_init_mixed\" : orthogonal_init , \"kernel_init_2e_1e_stream\" : orthogonal_init , \"kernel_init_2e_2e_stream\" : orthogonal_init , \"bias_init_1e_stream\" : normal_init , \"bias_init_2e_stream\" : normal_init , \"activation_fn\" : \"gelu\" , \"use_bias\" : True , \"one_electron_skip\" : True , \"one_electron_skip_scale\" : 1.0 , \"two_electron_skip\" : True , \"two_electron_skip_scale\" : 1.0 , \"cyclic_spins\" : cyclic_spins , }, }, }, }, } return config","title":"get_default_model_config()"},{"location":"api/train/default_config/#vmcnet.train.default_config.get_default_molecular_config","text":"Get a default molecular configuration (LiH). Source code in vmcnet/train/default_config.py def get_default_molecular_config () -> Dict : \"\"\"Get a default molecular configuration (LiH).\"\"\" problem_config = { \"ion_pos\" : (( 0.0 , 0.0 , - 1.5069621 ), ( 0.0 , 0.0 , 1.5069621 )), \"ion_charges\" : ( 1.0 , 3.0 ), \"nelec\" : ( 2 , 2 ), } return problem_config","title":"get_default_molecular_config()"},{"location":"api/train/default_config/#vmcnet.train.default_config.get_default_vmc_config","text":"Get a default VMC training configuration. Source code in vmcnet/train/default_config.py def get_default_vmc_config () -> Dict : \"\"\"Get a default VMC training configuration.\"\"\" vmc_config = { \"nchains\" : 2000 , \"nepochs\" : 200000 , \"nburn\" : 5000 , \"nsteps_per_param_update\" : 10 , \"nmoves_per_width_update\" : 100 , \"std_move\" : 0.25 , \"checkpoint_every\" : 5000 , \"best_checkpoint_every\" : 100 , \"checkpoint_dir\" : \"checkpoints\" , \"checkpoint_variance_scale\" : 10 , \"checkpoint_if_nans\" : False , \"only_checkpoint_first_nans\" : True , \"nhistory_max\" : 200 , \"record_amplitudes\" : False , \"record_param_l1_norm\" : False , \"clip_threshold\" : 5.0 , \"nan_safe\" : True , \"optimizer_type\" : \"kfac\" , \"optimizer\" : { \"kfac\" : { \"l2_reg\" : 0.0 , \"norm_constraint\" : 0.001 , \"curvature_ema\" : 0.95 , \"inverse_update_period\" : 1 , \"min_damping\" : 1e-4 , \"register_only_generic\" : False , \"estimation_mode\" : \"fisher_exact\" , \"damping\" : 0.001 , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , \"learning_decay_rate\" : 1e-4 , }, \"adam\" : { \"b1\" : 0.9 , \"b2\" : 0.999 , \"eps\" : 1e-8 , \"eps_root\" : 0.0 , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , \"learning_decay_rate\" : 1e-4 , }, \"sgd\" : { \"momentum\" : 0.0 , \"nesterov\" : False , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , \"learning_decay_rate\" : 1e-4 , }, \"sr\" : { \"damping\" : 1.0 , # needs to be tuned with everything else \"maxiter\" : 10 , # when maxiter <= -1, uses default 10 * nparams \"descent_type\" : \"sgd\" , \"norm_constraint\" : 0.001 , \"mode\" : \"lazy\" , \"schedule_type\" : \"inverse_time\" , # constant or inverse_time \"learning_rate\" : 5e-2 , # needs to be tuned with everything else \"learning_decay_rate\" : 1e-4 , }, }, } return vmc_config","title":"get_default_vmc_config()"},{"location":"api/train/default_config/#vmcnet.train.default_config.get_default_eval_config","text":"Get a default evaluation configuration. Source code in vmcnet/train/default_config.py def get_default_eval_config () -> Dict : \"\"\"Get a default evaluation configuration.\"\"\" eval_config = { \"nchains\" : 2000 , \"nburn\" : 5000 , \"nepochs\" : 20000 , \"nsteps_per_param_update\" : 10 , \"nmoves_per_width_update\" : 100 , \"record_amplitudes\" : False , \"std_move\" : 0.25 , # if use_data_from_training=True, nchains, nmoves_per_width_update, and # std_move are completely ignored, and the data output from training is # used as the initial positions instead \"use_data_from_training\" : False , \"record_local_energies\" : True , # save local energies and compute statistics \"nan_safe\" : False , } return eval_config","title":"get_default_eval_config()"},{"location":"api/train/parse_config_flags/","text":"Logic for parsing command line flags into a ConfigDict. parse_flags ( flag_values ) Parse command line flags into ConfigDicts. Supports two cases. In the first, a global default ConfigDict is used as a starting point, and changed only where the user overrides it via command line flags such as --config.vmc.nburn=100 . In the second, the default ConfigDict is loaded from a previous run by specifying the log directory for that run, as well as several other options. These options are provided using --reload.. , for example --reload.logdir=./logs . The user can still override settings from the previous run by providing regular command-line flags via --config.. , as described above. However, to override model-related flags, some care must be taken since the structure of the ConfigDict loaded from the json snapshot is not identical to the structure of the default ConfigDict. The difference is due to :func: ~vmcnet.train.choose_model_type_in_model_config . Parameters: Name Type Description Default flag_values FlagValues a FlagValues object used to manage the command line flags. Should generally use the global flags.FLAGS, but it's useful to be able to override this for testing, since an error will be thrown if multiple tests define configs for the same FlagValues object. required Returns: Type Description (reload_config, config) Two ConfigDicts. The first holds settings for the case where configurations or checkpoints are reloaded from a previous run. The second holds all other settings. Source code in vmcnet/train/parse_config_flags.py def parse_flags ( flag_values : flags . FlagValues ) -> Tuple [ ConfigDict , ConfigDict ]: \"\"\"Parse command line flags into ConfigDicts. Supports two cases. In the first, a global default ConfigDict is used as a starting point, and changed only where the user overrides it via command line flags such as `--config.vmc.nburn=100`. In the second, the default ConfigDict is loaded from a previous run by specifying the log directory for that run, as well as several other options. These options are provided using `--reload..`, for example `--reload.logdir=./logs`. The user can still override settings from the previous run by providing regular command-line flags via `--config..`, as described above. However, to override model-related flags, some care must be taken since the structure of the ConfigDict loaded from the json snapshot is not identical to the structure of the default ConfigDict. The difference is due to :func:`~vmcnet.train.choose_model_type_in_model_config`. Args: flag_values (FlagValues): a FlagValues object used to manage the command line flags. Should generally use the global flags.FLAGS, but it's useful to be able to override this for testing, since an error will be thrown if multiple tests define configs for the same FlagValues object. Returns: (reload_config, config): Two ConfigDicts. The first holds settings for the case where configurations or checkpoints are reloaded from a previous run. The second holds all other settings. \"\"\" config_flags . DEFINE_config_dict ( \"reload\" , train . default_config . get_default_reload_config (), lock_config = True , flag_values = flag_values , ) flag_values ( sys . argv , True ) reload_config = flag_values . reload if ( reload_config . logdir != train . default_config . NO_RELOAD_LOG_DIR and reload_config . use_config_file ): config = _get_config_from_reload ( reload_config , flag_values ) else : config = _get_config_from_default_config ( flag_values ) if config . debug_nans : config . distribute = False jax . config . update ( \"jax_debug_nans\" , True ) return reload_config , config","title":"parse_config_flags"},{"location":"api/train/parse_config_flags/#vmcnet.train.parse_config_flags.parse_flags","text":"Parse command line flags into ConfigDicts. Supports two cases. In the first, a global default ConfigDict is used as a starting point, and changed only where the user overrides it via command line flags such as --config.vmc.nburn=100 . In the second, the default ConfigDict is loaded from a previous run by specifying the log directory for that run, as well as several other options. These options are provided using --reload.. , for example --reload.logdir=./logs . The user can still override settings from the previous run by providing regular command-line flags via --config.. , as described above. However, to override model-related flags, some care must be taken since the structure of the ConfigDict loaded from the json snapshot is not identical to the structure of the default ConfigDict. The difference is due to :func: ~vmcnet.train.choose_model_type_in_model_config . Parameters: Name Type Description Default flag_values FlagValues a FlagValues object used to manage the command line flags. Should generally use the global flags.FLAGS, but it's useful to be able to override this for testing, since an error will be thrown if multiple tests define configs for the same FlagValues object. required Returns: Type Description (reload_config, config) Two ConfigDicts. The first holds settings for the case where configurations or checkpoints are reloaded from a previous run. The second holds all other settings. Source code in vmcnet/train/parse_config_flags.py def parse_flags ( flag_values : flags . FlagValues ) -> Tuple [ ConfigDict , ConfigDict ]: \"\"\"Parse command line flags into ConfigDicts. Supports two cases. In the first, a global default ConfigDict is used as a starting point, and changed only where the user overrides it via command line flags such as `--config.vmc.nburn=100`. In the second, the default ConfigDict is loaded from a previous run by specifying the log directory for that run, as well as several other options. These options are provided using `--reload..`, for example `--reload.logdir=./logs`. The user can still override settings from the previous run by providing regular command-line flags via `--config..`, as described above. However, to override model-related flags, some care must be taken since the structure of the ConfigDict loaded from the json snapshot is not identical to the structure of the default ConfigDict. The difference is due to :func:`~vmcnet.train.choose_model_type_in_model_config`. Args: flag_values (FlagValues): a FlagValues object used to manage the command line flags. Should generally use the global flags.FLAGS, but it's useful to be able to override this for testing, since an error will be thrown if multiple tests define configs for the same FlagValues object. Returns: (reload_config, config): Two ConfigDicts. The first holds settings for the case where configurations or checkpoints are reloaded from a previous run. The second holds all other settings. \"\"\" config_flags . DEFINE_config_dict ( \"reload\" , train . default_config . get_default_reload_config (), lock_config = True , flag_values = flag_values , ) flag_values ( sys . argv , True ) reload_config = flag_values . reload if ( reload_config . logdir != train . default_config . NO_RELOAD_LOG_DIR and reload_config . use_config_file ): config = _get_config_from_reload ( reload_config , flag_values ) else : config = _get_config_from_default_config ( flag_values ) if config . debug_nans : config . distribute = False jax . config . update ( \"jax_debug_nans\" , True ) return reload_config , config","title":"parse_flags()"},{"location":"api/train/runners/","text":"Entry points for running standard jobs. total_variation_clipping_fn ( local_energies , threshold = 5.0 ) Clip local energy to within a multiple of the total variation from the median. Source code in vmcnet/train/runners.py def total_variation_clipping_fn ( local_energies , threshold = 5.0 ): \"\"\"Clip local energy to within a multiple of the total variation from the median.\"\"\" median_local_e = jnp . nanmedian ( local_energies ) total_variation = jnp . nanmean ( jnp . abs ( local_energies - median_local_e )) clipped_local_e = jnp . clip ( local_energies , median_local_e - threshold * total_variation , median_local_e + threshold * total_variation , ) return clipped_local_e run_molecule () Run VMC on a molecule. Source code in vmcnet/train/runners.py def run_molecule () -> None : \"\"\"Run VMC on a molecule.\"\"\" reload_config , config = train . parse_config_flags . parse_flags ( FLAGS ) root_logger = logging . getLogger () root_logger . setLevel ( config . logging_level ) logdir = _get_logdir_and_save_config ( reload_config , config ) dtype_to_use = _get_dtype ( config ) ion_pos , ion_charges , nelec = _get_electron_ion_config_as_arrays ( config , dtype = dtype_to_use ) key = jax . random . PRNGKey ( config . initial_seed ) ( log_psi_apply , burning_step , walker_fn , local_energy_fn , update_param_fn , get_amplitude_fn , params , data , optimizer_state , key , ) = _setup_vmc ( config , ion_pos , ion_charges , nelec , key , dtype = dtype_to_use , apply_pmap = config . distribute , ) reload_from_checkpoint = ( reload_config . logdir != train . default_config . NO_RELOAD_LOG_DIR and reload_config . use_checkpoint_file ) if reload_from_checkpoint : checkpoint_file_path = os . path . join ( reload_config . logdir , reload_config . checkpoint_relative_file_path ) directory , filename = os . path . split ( checkpoint_file_path ) _ , data , params , optimizer_state , key = utils . io . reload_vmc_state ( directory , filename ) ( data , params , optimizer_state , key , ) = utils . distribute . distribute_vmc_state_from_checkpoint ( data , params , optimizer_state , key ) params , optimizer_state , data , key = _burn_and_run_vmc ( config . vmc , logdir , params , optimizer_state , data , burning_step , walker_fn , update_param_fn , get_amplitude_fn , key , should_checkpoint = True , ) logging . info ( \"Completed VMC! Evaluating\" ) # TODO: integrate the stuff in mcmc/statistics and write out an evaluation summary # (energy, var, overall mean acceptance ratio, std error, iac) to eval_logdir, post # evaluation eval_logdir = os . path . join ( logdir , \"eval\" ) eval_update_param_fn , eval_burning_step , eval_walker_fn = _setup_eval ( config , log_psi_apply , local_energy_fn , pacore . get_position_from_data , apply_pmap = config . distribute , ) optimizer_state = None eval_and_vmc_nchains_match = config . vmc . nchains == config . eval . nchains if not config . eval . use_data_from_training or not eval_and_vmc_nchains_match : key , data = _make_new_data_for_eval ( config , log_psi_apply , params , ion_pos , ion_charges , nelec , key , dtype = dtype_to_use , ) _burn_and_run_vmc ( config . eval , eval_logdir , params , optimizer_state , data , eval_burning_step , eval_walker_fn , eval_update_param_fn , get_amplitude_fn , key , should_checkpoint = False , ) # need to check for local_energy.txt because when config.eval.nepochs=0 the file is # not created regardless of config.eval.record_local_energies local_es_were_recorded = os . path . exists ( os . path . join ( eval_logdir , \"local_energies.txt\" ) ) if config . eval . record_local_energies and local_es_were_recorded : local_energies_filepath = os . path . join ( eval_logdir , \"local_energies.txt\" ) _compute_and_save_energy_statistics ( local_energies_filepath , eval_logdir , \"statistics\" ) vmc_statistics () Calculate statistics from a VMC evaluation run and write them to disc. Source code in vmcnet/train/runners.py def vmc_statistics () -> None : \"\"\"Calculate statistics from a VMC evaluation run and write them to disc.\"\"\" parser = argparse . ArgumentParser ( description = \"Calculate statistics from a VMC evaluation run and write them \" \"to disc.\" ) parser . add_argument ( \"local_energies_file_path\" , type = str , help = \"File path to load local energies from\" , ) parser . add_argument ( \"output_file_path\" , type = str , help = \"File path to which to write the output statistics. The '.json' suffix \" \"will be appended to the supplied path.\" , ) args = parser . parse_args () output_dir , output_filename = os . path . split ( os . path . abspath ( args . output_file_path )) _compute_and_save_energy_statistics ( args . local_energies_file_path , output_dir , output_filename )","title":"runners"},{"location":"api/train/runners/#vmcnet.train.runners.total_variation_clipping_fn","text":"Clip local energy to within a multiple of the total variation from the median. Source code in vmcnet/train/runners.py def total_variation_clipping_fn ( local_energies , threshold = 5.0 ): \"\"\"Clip local energy to within a multiple of the total variation from the median.\"\"\" median_local_e = jnp . nanmedian ( local_energies ) total_variation = jnp . nanmean ( jnp . abs ( local_energies - median_local_e )) clipped_local_e = jnp . clip ( local_energies , median_local_e - threshold * total_variation , median_local_e + threshold * total_variation , ) return clipped_local_e","title":"total_variation_clipping_fn()"},{"location":"api/train/runners/#vmcnet.train.runners.run_molecule","text":"Run VMC on a molecule. Source code in vmcnet/train/runners.py def run_molecule () -> None : \"\"\"Run VMC on a molecule.\"\"\" reload_config , config = train . parse_config_flags . parse_flags ( FLAGS ) root_logger = logging . getLogger () root_logger . setLevel ( config . logging_level ) logdir = _get_logdir_and_save_config ( reload_config , config ) dtype_to_use = _get_dtype ( config ) ion_pos , ion_charges , nelec = _get_electron_ion_config_as_arrays ( config , dtype = dtype_to_use ) key = jax . random . PRNGKey ( config . initial_seed ) ( log_psi_apply , burning_step , walker_fn , local_energy_fn , update_param_fn , get_amplitude_fn , params , data , optimizer_state , key , ) = _setup_vmc ( config , ion_pos , ion_charges , nelec , key , dtype = dtype_to_use , apply_pmap = config . distribute , ) reload_from_checkpoint = ( reload_config . logdir != train . default_config . NO_RELOAD_LOG_DIR and reload_config . use_checkpoint_file ) if reload_from_checkpoint : checkpoint_file_path = os . path . join ( reload_config . logdir , reload_config . checkpoint_relative_file_path ) directory , filename = os . path . split ( checkpoint_file_path ) _ , data , params , optimizer_state , key = utils . io . reload_vmc_state ( directory , filename ) ( data , params , optimizer_state , key , ) = utils . distribute . distribute_vmc_state_from_checkpoint ( data , params , optimizer_state , key ) params , optimizer_state , data , key = _burn_and_run_vmc ( config . vmc , logdir , params , optimizer_state , data , burning_step , walker_fn , update_param_fn , get_amplitude_fn , key , should_checkpoint = True , ) logging . info ( \"Completed VMC! Evaluating\" ) # TODO: integrate the stuff in mcmc/statistics and write out an evaluation summary # (energy, var, overall mean acceptance ratio, std error, iac) to eval_logdir, post # evaluation eval_logdir = os . path . join ( logdir , \"eval\" ) eval_update_param_fn , eval_burning_step , eval_walker_fn = _setup_eval ( config , log_psi_apply , local_energy_fn , pacore . get_position_from_data , apply_pmap = config . distribute , ) optimizer_state = None eval_and_vmc_nchains_match = config . vmc . nchains == config . eval . nchains if not config . eval . use_data_from_training or not eval_and_vmc_nchains_match : key , data = _make_new_data_for_eval ( config , log_psi_apply , params , ion_pos , ion_charges , nelec , key , dtype = dtype_to_use , ) _burn_and_run_vmc ( config . eval , eval_logdir , params , optimizer_state , data , eval_burning_step , eval_walker_fn , eval_update_param_fn , get_amplitude_fn , key , should_checkpoint = False , ) # need to check for local_energy.txt because when config.eval.nepochs=0 the file is # not created regardless of config.eval.record_local_energies local_es_were_recorded = os . path . exists ( os . path . join ( eval_logdir , \"local_energies.txt\" ) ) if config . eval . record_local_energies and local_es_were_recorded : local_energies_filepath = os . path . join ( eval_logdir , \"local_energies.txt\" ) _compute_and_save_energy_statistics ( local_energies_filepath , eval_logdir , \"statistics\" )","title":"run_molecule()"},{"location":"api/train/runners/#vmcnet.train.runners.vmc_statistics","text":"Calculate statistics from a VMC evaluation run and write them to disc. Source code in vmcnet/train/runners.py def vmc_statistics () -> None : \"\"\"Calculate statistics from a VMC evaluation run and write them to disc.\"\"\" parser = argparse . ArgumentParser ( description = \"Calculate statistics from a VMC evaluation run and write them \" \"to disc.\" ) parser . add_argument ( \"local_energies_file_path\" , type = str , help = \"File path to load local energies from\" , ) parser . add_argument ( \"output_file_path\" , type = str , help = \"File path to which to write the output statistics. The '.json' suffix \" \"will be appended to the supplied path.\" , ) args = parser . parse_args () output_dir , output_filename = os . path . split ( os . path . abspath ( args . output_file_path )) _compute_and_save_energy_statistics ( args . local_energies_file_path , output_dir , output_filename )","title":"vmc_statistics()"},{"location":"api/train/vmc/","text":"Main VMC loop. vmc_loop ( params , optimizer_state , data , nchains , nepochs , walker_fn , update_param_fn , key , logdir = None , checkpoint_every = 1000 , best_checkpoint_every = 100 , checkpoint_dir = 'checkpoints' , checkpoint_variance_scale = 10.0 , checkpoint_if_nans = False , only_checkpoint_first_nans = True , record_amplitudes = False , get_amplitude_fn = None , nhistory_max = 200 ) Main Variational Monte Carlo loop routine. Variational Monte Carlo (VMC) can be generically viewed as minimizing a parameterized variational loss stochastically by sampling over a data distribution via Monte Carlo sampling. This function implements this idea at a high level, using a walker_fn to sample the data distribution, and passing the optimization step to a generic function update_param_fn . Parameters: Name Type Description Default params pytree-like model parameters which are trained required optimizer_state pytree-like initial state of the optimizer required data pytree-like initial data required nchains int number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. required nepochs int number of parameter updates to do required walker_fn Callable function which does a number of walker steps between each parameter update. Has the signature (data, params, key) -> (mean accept prob, new data, new key) required update_param_fn Callable function which updates the parameters. Has signature (data, params, optimizer_state, key) -> (new_params, optimizer_state, dict: metrics, key). If metrics is not None, it is required to have the entries \"energy\" and \"variance\" at a minimum. If metrics is None, no checkpointing is done. required key PRNGKey an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn required logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. None checkpoint_every int how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to 1000. 1000 best_checkpoint_every int limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of best_checkpoint_every , we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. 100 checkpoint_dir str name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". 'checkpoints' checkpoint_variance_scale float scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func: ~vmctrain.train.vmc.get_checkpoint_metric . Defaults to 10.0. 10.0 checkpoint_if_nans bool whether to save checkpoints when nan energy values are recorded. Defaults to False. False only_checkpoint_first_nans bool whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. True nhistory_max int How much history to keep in the running histories of the energy and variance. Defaults to 200. 200 Returns: Type Description Tuple[~P, ~S, ~D, jax._src.prng.PRNGKeyArray] A tuple of (trained parameters, final optimizer state, final data, final key). These are the same structure as (params, optimizer_state, initial_data, key). Source code in vmcnet/train/vmc.py def vmc_loop ( params : P , optimizer_state : S , data : D , nchains : int , nepochs : int , walker_fn : WalkerFn [ P , D ], update_param_fn : UpdateParamFn [ P , D , S ], key : PRNGKey , logdir : str = None , checkpoint_every : Optional [ int ] = 1000 , best_checkpoint_every : Optional [ int ] = 100 , checkpoint_dir : str = \"checkpoints\" , checkpoint_variance_scale : float = 10.0 , checkpoint_if_nans : bool = False , only_checkpoint_first_nans : bool = True , record_amplitudes : bool = False , get_amplitude_fn : Optional [ GetAmplitudeFromData [ D ]] = None , nhistory_max : int = 200 , ) -> Tuple [ P , S , D , PRNGKey ]: \"\"\"Main Variational Monte Carlo loop routine. Variational Monte Carlo (VMC) can be generically viewed as minimizing a parameterized variational loss stochastically by sampling over a data distribution via Monte Carlo sampling. This function implements this idea at a high level, using a walker_fn to sample the data distribution, and passing the optimization step to a generic function `update_param_fn`. Args: params (pytree-like): model parameters which are trained optimizer_state (pytree-like): initial state of the optimizer data (pytree-like): initial data nchains (int): number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. nepochs (int): number of parameter updates to do walker_fn (Callable): function which does a number of walker steps between each parameter update. Has the signature (data, params, key) -> (mean accept prob, new data, new key) update_param_fn (Callable): function which updates the parameters. Has signature (data, params, optimizer_state, key) -> (new_params, optimizer_state, dict: metrics, key). If metrics is not None, it is required to have the entries \"energy\" and \"variance\" at a minimum. If metrics is None, no checkpointing is done. key (PRNGKey): an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn logdir (str, optional): name of parent log directory. If None, no checkpointing is done. Defaults to None. checkpoint_every (int, optional): how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to 1000. best_checkpoint_every (int, optional): limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of `best_checkpoint_every`, we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. checkpoint_dir (str, optional): name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". checkpoint_variance_scale (float, optional): scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func:`~vmctrain.train.vmc.get_checkpoint_metric`. Defaults to 10.0. checkpoint_if_nans (bool, optional): whether to save checkpoints when nan energy values are recorded. Defaults to False. only_checkpoint_first_nans (bool, optional): whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. nhistory_max (int, optional): How much history to keep in the running histories of the energy and variance. Defaults to 200. Returns: A tuple of (trained parameters, final optimizer state, final data, final key). These are the same structure as (params, optimizer_state, initial_data, key). \"\"\" ( checkpoint_dir , checkpoint_metric , running_energy_and_variance , best_checkpoint_data , saved_nans_checkpoint , ) = utils . checkpoint . initialize_checkpointing ( checkpoint_dir , nhistory_max , logdir , checkpoint_every ) with CheckpointWriter () as checkpoint_writer , MetricsWriter () as metrics_writer : for epoch in range ( nepochs ): # Save state for checkpointing at the start of the epoch for two reasons: # 1. To save the model that generates the best energy and variance metrics, # rather than the model one parameter UPDATE after the best metrics. # 2. To ensure a fully consistent state can be reloaded from a checkpoint, & # the exact subsequent behavior can be reproduced (if run on same machine). old_params = params old_state = optimizer_state old_data = data old_key = key accept_ratio , data , key = walker_fn ( params , data , key ) params , optimizer_state , metrics , key = update_param_fn ( params , data , optimizer_state , key ) if metrics is None : # don't checkpoint if no metrics to checkpoint continue metrics [ \"accept_ratio\" ] = accept_ratio ( checkpoint_metric , checkpoint_str , best_checkpoint_data , saved_nans_checkpoint , ) = utils . checkpoint . save_metrics_and_handle_checkpoints ( epoch , old_params , params , old_state , old_data , data , old_key , metrics , nchains , running_energy_and_variance , checkpoint_writer , metrics_writer , checkpoint_metric , logdir = logdir , variance_scale = checkpoint_variance_scale , checkpoint_every = checkpoint_every , best_checkpoint_every = best_checkpoint_every , best_checkpoint_data = best_checkpoint_data , checkpoint_dir = checkpoint_dir , checkpoint_if_nans = checkpoint_if_nans , only_checkpoint_first_nans = only_checkpoint_first_nans , saved_nans_checkpoint = saved_nans_checkpoint , record_amplitudes = record_amplitudes , get_amplitude_fn = get_amplitude_fn , ) utils . checkpoint . log_vmc_loop_state ( epoch , metrics , checkpoint_str ) # TODO: add flag which gives a way to break out of the VMC loop when the # first nan has been hit, to keep jobs from running past useful output in # some cases utils . checkpoint . finish_checkpointing ( checkpoint_writer , best_checkpoint_data , logdir ) return params , optimizer_state , data , key","title":"vmc"},{"location":"api/train/vmc/#vmcnet.train.vmc.vmc_loop","text":"Main Variational Monte Carlo loop routine. Variational Monte Carlo (VMC) can be generically viewed as minimizing a parameterized variational loss stochastically by sampling over a data distribution via Monte Carlo sampling. This function implements this idea at a high level, using a walker_fn to sample the data distribution, and passing the optimization step to a generic function update_param_fn . Parameters: Name Type Description Default params pytree-like model parameters which are trained required optimizer_state pytree-like initial state of the optimizer required data pytree-like initial data required nchains int number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. required nepochs int number of parameter updates to do required walker_fn Callable function which does a number of walker steps between each parameter update. Has the signature (data, params, key) -> (mean accept prob, new data, new key) required update_param_fn Callable function which updates the parameters. Has signature (data, params, optimizer_state, key) -> (new_params, optimizer_state, dict: metrics, key). If metrics is not None, it is required to have the entries \"energy\" and \"variance\" at a minimum. If metrics is None, no checkpointing is done. required key PRNGKey an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn required logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. None checkpoint_every int how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to 1000. 1000 best_checkpoint_every int limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of best_checkpoint_every , we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. 100 checkpoint_dir str name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". 'checkpoints' checkpoint_variance_scale float scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func: ~vmctrain.train.vmc.get_checkpoint_metric . Defaults to 10.0. 10.0 checkpoint_if_nans bool whether to save checkpoints when nan energy values are recorded. Defaults to False. False only_checkpoint_first_nans bool whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. True nhistory_max int How much history to keep in the running histories of the energy and variance. Defaults to 200. 200 Returns: Type Description Tuple[~P, ~S, ~D, jax._src.prng.PRNGKeyArray] A tuple of (trained parameters, final optimizer state, final data, final key). These are the same structure as (params, optimizer_state, initial_data, key). Source code in vmcnet/train/vmc.py def vmc_loop ( params : P , optimizer_state : S , data : D , nchains : int , nepochs : int , walker_fn : WalkerFn [ P , D ], update_param_fn : UpdateParamFn [ P , D , S ], key : PRNGKey , logdir : str = None , checkpoint_every : Optional [ int ] = 1000 , best_checkpoint_every : Optional [ int ] = 100 , checkpoint_dir : str = \"checkpoints\" , checkpoint_variance_scale : float = 10.0 , checkpoint_if_nans : bool = False , only_checkpoint_first_nans : bool = True , record_amplitudes : bool = False , get_amplitude_fn : Optional [ GetAmplitudeFromData [ D ]] = None , nhistory_max : int = 200 , ) -> Tuple [ P , S , D , PRNGKey ]: \"\"\"Main Variational Monte Carlo loop routine. Variational Monte Carlo (VMC) can be generically viewed as minimizing a parameterized variational loss stochastically by sampling over a data distribution via Monte Carlo sampling. This function implements this idea at a high level, using a walker_fn to sample the data distribution, and passing the optimization step to a generic function `update_param_fn`. Args: params (pytree-like): model parameters which are trained optimizer_state (pytree-like): initial state of the optimizer data (pytree-like): initial data nchains (int): number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. nepochs (int): number of parameter updates to do walker_fn (Callable): function which does a number of walker steps between each parameter update. Has the signature (data, params, key) -> (mean accept prob, new data, new key) update_param_fn (Callable): function which updates the parameters. Has signature (data, params, optimizer_state, key) -> (new_params, optimizer_state, dict: metrics, key). If metrics is not None, it is required to have the entries \"energy\" and \"variance\" at a minimum. If metrics is None, no checkpointing is done. key (PRNGKey): an array with shape (2,) representing a jax PRNG key passed to proposal_fn and used to randomly accept proposals with probabilities output by acceptance_fn logdir (str, optional): name of parent log directory. If None, no checkpointing is done. Defaults to None. checkpoint_every (int, optional): how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to 1000. best_checkpoint_every (int, optional): limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of `best_checkpoint_every`, we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. checkpoint_dir (str, optional): name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". checkpoint_variance_scale (float, optional): scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func:`~vmctrain.train.vmc.get_checkpoint_metric`. Defaults to 10.0. checkpoint_if_nans (bool, optional): whether to save checkpoints when nan energy values are recorded. Defaults to False. only_checkpoint_first_nans (bool, optional): whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. nhistory_max (int, optional): How much history to keep in the running histories of the energy and variance. Defaults to 200. Returns: A tuple of (trained parameters, final optimizer state, final data, final key). These are the same structure as (params, optimizer_state, initial_data, key). \"\"\" ( checkpoint_dir , checkpoint_metric , running_energy_and_variance , best_checkpoint_data , saved_nans_checkpoint , ) = utils . checkpoint . initialize_checkpointing ( checkpoint_dir , nhistory_max , logdir , checkpoint_every ) with CheckpointWriter () as checkpoint_writer , MetricsWriter () as metrics_writer : for epoch in range ( nepochs ): # Save state for checkpointing at the start of the epoch for two reasons: # 1. To save the model that generates the best energy and variance metrics, # rather than the model one parameter UPDATE after the best metrics. # 2. To ensure a fully consistent state can be reloaded from a checkpoint, & # the exact subsequent behavior can be reproduced (if run on same machine). old_params = params old_state = optimizer_state old_data = data old_key = key accept_ratio , data , key = walker_fn ( params , data , key ) params , optimizer_state , metrics , key = update_param_fn ( params , data , optimizer_state , key ) if metrics is None : # don't checkpoint if no metrics to checkpoint continue metrics [ \"accept_ratio\" ] = accept_ratio ( checkpoint_metric , checkpoint_str , best_checkpoint_data , saved_nans_checkpoint , ) = utils . checkpoint . save_metrics_and_handle_checkpoints ( epoch , old_params , params , old_state , old_data , data , old_key , metrics , nchains , running_energy_and_variance , checkpoint_writer , metrics_writer , checkpoint_metric , logdir = logdir , variance_scale = checkpoint_variance_scale , checkpoint_every = checkpoint_every , best_checkpoint_every = best_checkpoint_every , best_checkpoint_data = best_checkpoint_data , checkpoint_dir = checkpoint_dir , checkpoint_if_nans = checkpoint_if_nans , only_checkpoint_first_nans = only_checkpoint_first_nans , saved_nans_checkpoint = saved_nans_checkpoint , record_amplitudes = record_amplitudes , get_amplitude_fn = get_amplitude_fn , ) utils . checkpoint . log_vmc_loop_state ( epoch , metrics , checkpoint_str ) # TODO: add flag which gives a way to break out of the VMC loop when the # first nan has been hit, to keep jobs from running past useful output in # some cases utils . checkpoint . finish_checkpointing ( checkpoint_writer , best_checkpoint_data , logdir ) return params , optimizer_state , data , key","title":"vmc_loop()"},{"location":"api/updates/params/","text":"Routines which handle model parameter updating. create_grad_energy_update_param_fn ( energy_data_val_and_grad , optimizer_apply , get_position_fn , apply_pmap = True , record_param_l1_norm = False ) Create the update_param_fn based on the gradient of the total energy. See :func: ~vmcnet.train.vmc.vmc_loop for its usage. Parameters: Name Type Description Default energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required optimizer_apply Callable applies an update to the parameters. Has signature (grad_energy, params, optimizer_state) -> (new_params, new_optimizer_state). required get_position_fn GetPositionFromData gets the walker positions from the MCMC data. required apply_pmap bool whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. True Returns: Type Description Callable function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) The function is pmapped if apply_pmap is True, and jitted if apply_pmap is False. Source code in vmcnet/updates/params.py def create_grad_energy_update_param_fn ( energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], optimizer_apply : Callable [[ P , P , S , D ], Tuple [ P , S ]], get_position_fn : GetPositionFromData [ D ], apply_pmap : bool = True , record_param_l1_norm : bool = False , ) -> UpdateParamFn [ P , D , S ]: \"\"\"Create the `update_param_fn` based on the gradient of the total energy. See :func:`~vmcnet.train.vmc.vmc_loop` for its usage. Args: energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) optimizer_apply (Callable): applies an update to the parameters. Has signature (grad_energy, params, optimizer_state) -> (new_params, new_optimizer_state). get_position_fn (GetPositionFromData): gets the walker positions from the MCMC data. apply_pmap (bool, optional): whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. Returns: Callable: function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) The function is pmapped if apply_pmap is True, and jitted if apply_pmap is False. \"\"\" def update_param_fn ( params , data , optimizer_state , key ): position = get_position_fn ( data ) energy_data , grad_energy = energy_data_val_and_grad ( params , position ) energy , aux_energy_data = energy_data grad_energy = utils . distribute . pmean_if_pmap ( grad_energy ) params , optimizer_state = optimizer_apply ( grad_energy , params , optimizer_state , data ) metrics = { \"energy\" : energy , \"variance\" : aux_energy_data [ 0 ]} metrics = _update_metrics_with_noclip ( aux_energy_data [ 2 ], aux_energy_data [ 3 ], metrics ) if record_param_l1_norm : metrics . update ({ \"param_l1_norm\" : tree_reduce_l1 ( params )}) return params , optimizer_state , metrics , key traced_fn = _make_traced_fn_with_single_metrics ( update_param_fn , apply_pmap ) return traced_fn create_kfac_update_param_fn ( optimizer , damping , get_position_fn , record_param_l1_norm = False ) Create momentum-less KFAC update step function. Parameters: Name Type Description Default optimizer kfac_ferminet_alpha.Optimizer instance of the Optimizer class from kfac_ferminet_alpha required damping jnp.float32 damping coefficient required get_position_fn GetPositionFromData function which gets the walker positions from the data. Has signature data -> Array required Returns: Type Description Callable function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) Source code in vmcnet/updates/params.py def create_kfac_update_param_fn ( optimizer : kfac_ferminet_alpha . Optimizer , damping : jnp . float32 , get_position_fn : GetPositionFromData [ D ], record_param_l1_norm : bool = False , ) -> UpdateParamFn [ kfac_opt . Parameters , D , kfac_opt . State ]: \"\"\"Create momentum-less KFAC update step function. Args: optimizer (kfac_ferminet_alpha.Optimizer): instance of the Optimizer class from kfac_ferminet_alpha damping (jnp.float32): damping coefficient get_position_fn (GetPositionFromData): function which gets the walker positions from the data. Has signature data -> Array Returns: Callable: function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) \"\"\" momentum = jnp . asarray ( 0.0 ) damping = jnp . asarray ( damping ) if optimizer . multi_device : momentum = utils . distribute . replicate_all_local_devices ( momentum ) damping = utils . distribute . replicate_all_local_devices ( damping ) traced_compute_param_norm = _get_traced_compute_param_norm ( optimizer . multi_device ) def update_param_fn ( params , data , optimizer_state , key ): key , subkey = utils . distribute . split_or_psplit_key ( key , optimizer . multi_device ) params , optimizer_state , stats = optimizer . step ( params = params , state = optimizer_state , rng = subkey , data_iterator = iter ([ get_position_fn ( data )]), momentum = momentum , damping = damping , ) energy = stats [ \"loss\" ] variance = stats [ \"aux\" ][ 0 ] energy_noclip = stats [ \"aux\" ][ 2 ] variance_noclip = stats [ \"aux\" ][ 3 ] picked_stats = ( energy , variance , energy_noclip , variance_noclip ) if record_param_l1_norm : param_l1_norm = traced_compute_param_norm ( params ) picked_stats = picked_stats + ( param_l1_norm ,) stats_to_save = picked_stats if optimizer . multi_device : stats_to_save = [ utils . distribute . get_first ( stat ) for stat in picked_stats ] metrics = { \"energy\" : stats_to_save [ 0 ], \"variance\" : stats_to_save [ 1 ]} metrics = _update_metrics_with_noclip ( stats_to_save [ 2 ], stats_to_save [ 3 ], metrics ) if record_param_l1_norm : metrics . update ({ \"param_l1_norm\" : stats_to_save [ 4 ]}) return params , optimizer_state , metrics , key return update_param_fn create_eval_update_param_fn ( local_energy_fn , nchains , get_position_fn , apply_pmap = True , record_local_energies = True , nan_safe = False ) No update/clipping/grad function which simply evaluates the local energies. Can be used to do simple unclipped MCMC with :func: ~vmcnet.train.vmc.vmc_loop . Parameters: Name Type Description Default local_energy_fn Callable computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) required nchains int total number of chains across all devices, used to compute a sample variance estimate of the local energy required get_position_fn GetPositionFromData gets the walker positions from the MCMC data. required nan_safe bool whether or not to mask local energy nans in the evaluation process. This option should not be used under normal circumstances, as the energy estimates are of unclear validity if nans are masked. However, it can be used to get a coarse estimate of the energy of a wavefunction even if a few walkers are returning nans for their local energies. False Returns: Type Description Callable function which evaluates the local energies and averages them, without updating the parameters Source code in vmcnet/updates/params.py def create_eval_update_param_fn ( local_energy_fn : ModelApply [ P ], nchains : int , get_position_fn : GetPositionFromData [ D ], apply_pmap : bool = True , record_local_energies : bool = True , nan_safe : bool = False , ) -> UpdateParamFn [ P , D , OptimizerState ]: \"\"\"No update/clipping/grad function which simply evaluates the local energies. Can be used to do simple unclipped MCMC with :func:`~vmcnet.train.vmc.vmc_loop`. Arguments: local_energy_fn (Callable): computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) nchains (int): total number of chains across all devices, used to compute a sample variance estimate of the local energy get_position_fn (GetPositionFromData): gets the walker positions from the MCMC data. nan_safe (bool): whether or not to mask local energy nans in the evaluation process. This option should not be used under normal circumstances, as the energy estimates are of unclear validity if nans are masked. However, it can be used to get a coarse estimate of the energy of a wavefunction even if a few walkers are returning nans for their local energies. Returns: Callable: function which evaluates the local energies and averages them, without updating the parameters \"\"\" def eval_update_param_fn ( params , data , optimizer_state , key ): local_energies = local_energy_fn ( params , get_position_fn ( data )) energy , variance = physics . core . get_statistics_from_local_energy ( local_energies , nchains , nan_safe = nan_safe ) metrics = { \"energy\" : energy , \"variance\" : variance } if record_local_energies : metrics . update ({ \"local_energies\" : local_energies }) return params , optimizer_state , metrics , key traced_fn = _make_traced_fn_with_single_metrics ( eval_update_param_fn , apply_pmap , { \"energy\" , \"variance\" } ) return traced_fn constrain_norm ( grads , preconditioned_grads , learning_rate , norm_constraint = 0.001 ) Constrains the preconditioned norm of the update, adapted from KFAC. Source code in vmcnet/updates/params.py def constrain_norm ( grads : P , preconditioned_grads : P , learning_rate : jnp . float32 , norm_constraint : jnp . float32 = 0.001 , ) -> P : \"\"\"Constrains the preconditioned norm of the update, adapted from KFAC.\"\"\" sq_norm_grads = tree_inner_product ( preconditioned_grads , grads ) sq_norm_scaled_grads = sq_norm_grads * learning_rate ** 2 # Sync the norms here, see: # https://github.com/deepmind/deepmind-research/blob/30799687edb1abca4953aec507be87ebe63e432d/kfac_ferminet_alpha/optimizer.py#L585 sq_norm_scaled_grads = utils . distribute . pmean_if_pmap ( sq_norm_scaled_grads ) max_coefficient = jnp . sqrt ( norm_constraint / sq_norm_scaled_grads ) coefficient = jnp . minimum ( max_coefficient , 1 ) constrained_grads = multiply_tree_by_scalar ( preconditioned_grads , coefficient ) return constrained_grads","title":"params"},{"location":"api/updates/params/#vmcnet.updates.params.create_grad_energy_update_param_fn","text":"Create the update_param_fn based on the gradient of the total energy. See :func: ~vmcnet.train.vmc.vmc_loop for its usage. Parameters: Name Type Description Default energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required optimizer_apply Callable applies an update to the parameters. Has signature (grad_energy, params, optimizer_state) -> (new_params, new_optimizer_state). required get_position_fn GetPositionFromData gets the walker positions from the MCMC data. required apply_pmap bool whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. True Returns: Type Description Callable function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) The function is pmapped if apply_pmap is True, and jitted if apply_pmap is False. Source code in vmcnet/updates/params.py def create_grad_energy_update_param_fn ( energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], optimizer_apply : Callable [[ P , P , S , D ], Tuple [ P , S ]], get_position_fn : GetPositionFromData [ D ], apply_pmap : bool = True , record_param_l1_norm : bool = False , ) -> UpdateParamFn [ P , D , S ]: \"\"\"Create the `update_param_fn` based on the gradient of the total energy. See :func:`~vmcnet.train.vmc.vmc_loop` for its usage. Args: energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) optimizer_apply (Callable): applies an update to the parameters. Has signature (grad_energy, params, optimizer_state) -> (new_params, new_optimizer_state). get_position_fn (GetPositionFromData): gets the walker positions from the MCMC data. apply_pmap (bool, optional): whether to apply jax.pmap to the walker function. If False, applies jax.jit. Defaults to True. Returns: Callable: function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) The function is pmapped if apply_pmap is True, and jitted if apply_pmap is False. \"\"\" def update_param_fn ( params , data , optimizer_state , key ): position = get_position_fn ( data ) energy_data , grad_energy = energy_data_val_and_grad ( params , position ) energy , aux_energy_data = energy_data grad_energy = utils . distribute . pmean_if_pmap ( grad_energy ) params , optimizer_state = optimizer_apply ( grad_energy , params , optimizer_state , data ) metrics = { \"energy\" : energy , \"variance\" : aux_energy_data [ 0 ]} metrics = _update_metrics_with_noclip ( aux_energy_data [ 2 ], aux_energy_data [ 3 ], metrics ) if record_param_l1_norm : metrics . update ({ \"param_l1_norm\" : tree_reduce_l1 ( params )}) return params , optimizer_state , metrics , key traced_fn = _make_traced_fn_with_single_metrics ( update_param_fn , apply_pmap ) return traced_fn","title":"create_grad_energy_update_param_fn()"},{"location":"api/updates/params/#vmcnet.updates.params.create_kfac_update_param_fn","text":"Create momentum-less KFAC update step function. Parameters: Name Type Description Default optimizer kfac_ferminet_alpha.Optimizer instance of the Optimizer class from kfac_ferminet_alpha required damping jnp.float32 damping coefficient required get_position_fn GetPositionFromData function which gets the walker positions from the data. Has signature data -> Array required Returns: Type Description Callable function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) Source code in vmcnet/updates/params.py def create_kfac_update_param_fn ( optimizer : kfac_ferminet_alpha . Optimizer , damping : jnp . float32 , get_position_fn : GetPositionFromData [ D ], record_param_l1_norm : bool = False , ) -> UpdateParamFn [ kfac_opt . Parameters , D , kfac_opt . State ]: \"\"\"Create momentum-less KFAC update step function. Args: optimizer (kfac_ferminet_alpha.Optimizer): instance of the Optimizer class from kfac_ferminet_alpha damping (jnp.float32): damping coefficient get_position_fn (GetPositionFromData): function which gets the walker positions from the data. Has signature data -> Array Returns: Callable: function which updates the parameters given the current data, params, and optimizer state. The signature of this function is (data, params, optimizer_state, key) -> (new_params, new_optimizer_state, metrics, key) \"\"\" momentum = jnp . asarray ( 0.0 ) damping = jnp . asarray ( damping ) if optimizer . multi_device : momentum = utils . distribute . replicate_all_local_devices ( momentum ) damping = utils . distribute . replicate_all_local_devices ( damping ) traced_compute_param_norm = _get_traced_compute_param_norm ( optimizer . multi_device ) def update_param_fn ( params , data , optimizer_state , key ): key , subkey = utils . distribute . split_or_psplit_key ( key , optimizer . multi_device ) params , optimizer_state , stats = optimizer . step ( params = params , state = optimizer_state , rng = subkey , data_iterator = iter ([ get_position_fn ( data )]), momentum = momentum , damping = damping , ) energy = stats [ \"loss\" ] variance = stats [ \"aux\" ][ 0 ] energy_noclip = stats [ \"aux\" ][ 2 ] variance_noclip = stats [ \"aux\" ][ 3 ] picked_stats = ( energy , variance , energy_noclip , variance_noclip ) if record_param_l1_norm : param_l1_norm = traced_compute_param_norm ( params ) picked_stats = picked_stats + ( param_l1_norm ,) stats_to_save = picked_stats if optimizer . multi_device : stats_to_save = [ utils . distribute . get_first ( stat ) for stat in picked_stats ] metrics = { \"energy\" : stats_to_save [ 0 ], \"variance\" : stats_to_save [ 1 ]} metrics = _update_metrics_with_noclip ( stats_to_save [ 2 ], stats_to_save [ 3 ], metrics ) if record_param_l1_norm : metrics . update ({ \"param_l1_norm\" : stats_to_save [ 4 ]}) return params , optimizer_state , metrics , key return update_param_fn","title":"create_kfac_update_param_fn()"},{"location":"api/updates/params/#vmcnet.updates.params.create_eval_update_param_fn","text":"No update/clipping/grad function which simply evaluates the local energies. Can be used to do simple unclipped MCMC with :func: ~vmcnet.train.vmc.vmc_loop . Parameters: Name Type Description Default local_energy_fn Callable computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) required nchains int total number of chains across all devices, used to compute a sample variance estimate of the local energy required get_position_fn GetPositionFromData gets the walker positions from the MCMC data. required nan_safe bool whether or not to mask local energy nans in the evaluation process. This option should not be used under normal circumstances, as the energy estimates are of unclear validity if nans are masked. However, it can be used to get a coarse estimate of the energy of a wavefunction even if a few walkers are returning nans for their local energies. False Returns: Type Description Callable function which evaluates the local energies and averages them, without updating the parameters Source code in vmcnet/updates/params.py def create_eval_update_param_fn ( local_energy_fn : ModelApply [ P ], nchains : int , get_position_fn : GetPositionFromData [ D ], apply_pmap : bool = True , record_local_energies : bool = True , nan_safe : bool = False , ) -> UpdateParamFn [ P , D , OptimizerState ]: \"\"\"No update/clipping/grad function which simply evaluates the local energies. Can be used to do simple unclipped MCMC with :func:`~vmcnet.train.vmc.vmc_loop`. Arguments: local_energy_fn (Callable): computes local energies Hpsi / psi. Has signature (params, x) -> (Hpsi / psi)(x) nchains (int): total number of chains across all devices, used to compute a sample variance estimate of the local energy get_position_fn (GetPositionFromData): gets the walker positions from the MCMC data. nan_safe (bool): whether or not to mask local energy nans in the evaluation process. This option should not be used under normal circumstances, as the energy estimates are of unclear validity if nans are masked. However, it can be used to get a coarse estimate of the energy of a wavefunction even if a few walkers are returning nans for their local energies. Returns: Callable: function which evaluates the local energies and averages them, without updating the parameters \"\"\" def eval_update_param_fn ( params , data , optimizer_state , key ): local_energies = local_energy_fn ( params , get_position_fn ( data )) energy , variance = physics . core . get_statistics_from_local_energy ( local_energies , nchains , nan_safe = nan_safe ) metrics = { \"energy\" : energy , \"variance\" : variance } if record_local_energies : metrics . update ({ \"local_energies\" : local_energies }) return params , optimizer_state , metrics , key traced_fn = _make_traced_fn_with_single_metrics ( eval_update_param_fn , apply_pmap , { \"energy\" , \"variance\" } ) return traced_fn","title":"create_eval_update_param_fn()"},{"location":"api/updates/params/#vmcnet.updates.params.constrain_norm","text":"Constrains the preconditioned norm of the update, adapted from KFAC. Source code in vmcnet/updates/params.py def constrain_norm ( grads : P , preconditioned_grads : P , learning_rate : jnp . float32 , norm_constraint : jnp . float32 = 0.001 , ) -> P : \"\"\"Constrains the preconditioned norm of the update, adapted from KFAC.\"\"\" sq_norm_grads = tree_inner_product ( preconditioned_grads , grads ) sq_norm_scaled_grads = sq_norm_grads * learning_rate ** 2 # Sync the norms here, see: # https://github.com/deepmind/deepmind-research/blob/30799687edb1abca4953aec507be87ebe63e432d/kfac_ferminet_alpha/optimizer.py#L585 sq_norm_scaled_grads = utils . distribute . pmean_if_pmap ( sq_norm_scaled_grads ) max_coefficient = jnp . sqrt ( norm_constraint / sq_norm_scaled_grads ) coefficient = jnp . minimum ( max_coefficient , 1 ) constrained_grads = multiply_tree_by_scalar ( preconditioned_grads , coefficient ) return constrained_grads","title":"constrain_norm()"},{"location":"api/updates/parse_config/","text":"Get update functions from ConfigDicts. get_update_fn_and_init_optimizer ( log_psi_apply , vmc_config , params , data , get_position_fn , energy_data_val_and_grad , key , apply_pmap = True ) Get an update function and initialize optimizer state from the vmc configuration. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required vmc_config ConfigDict configuration for VMC required params pytree params with which to initialize optimizer state required data pytree data with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required key PRNGKey PRNGKey with which to initialize optimizer state required apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Exceptions: Type Description ValueError A non-supported optimizer type is requested. Currently, KFAC, Adam, SGD, and SR (with either Adam or SGD) is supported. Returns: Type Description (UpdateParamFn, OptimizerState, PRNGKey) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey Source code in vmcnet/updates/parse_config.py def get_update_fn_and_init_optimizer ( log_psi_apply : ModelApply [ P ], vmc_config : ConfigDict , params : P , data : D , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], key : PRNGKey , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , OptimizerState ], OptimizerState , PRNGKey ]: \"\"\"Get an update function and initialize optimizer state from the vmc configuration. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| vmc_config (ConfigDict): configuration for VMC params (pytree): params with which to initialize optimizer state data (pytree): data with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) key (PRNGKey): PRNGKey with which to initialize optimizer state apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Raises: ValueError: A non-supported optimizer type is requested. Currently, KFAC, Adam, SGD, and SR (with either Adam or SGD) is supported. Returns: (UpdateParamFn, OptimizerState, PRNGKey): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey \"\"\" learning_rate_schedule = _get_learning_rate_schedule ( vmc_config . optimizer [ vmc_config . optimizer_type ] ) if vmc_config . optimizer_type == \"kfac\" : return get_kfac_update_fn_and_state ( params , data , get_position_fn , energy_data_val_and_grad , key , learning_rate_schedule , vmc_config . optimizer . kfac , vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , ) elif vmc_config . optimizer_type == \"sgd\" : ( update_param_fn , optimizer_state ,) = get_sgd_update_fn_and_state ( params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , vmc_config . optimizer . sgd , vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , ) return update_param_fn , optimizer_state , key elif vmc_config . optimizer_type == \"adam\" : ( update_param_fn , optimizer_state ,) = get_adam_update_fn_and_state ( params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , vmc_config . optimizer . adam , vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , ) return update_param_fn , optimizer_state , key elif vmc_config . optimizer_type == \"sr\" : ( update_param_fn , optimizer_state ,) = get_sr_update_fn_and_state ( log_psi_apply , params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , vmc_config . optimizer . sr , vmc_config . optimizer [ vmc_config . optimizer . sr . descent_type ], vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , nan_safe = vmc_config . nan_safe , ) return update_param_fn , optimizer_state , key else : raise ValueError ( \"Requested optimizer type not supported; {} was requested\" . format ( vmc_config . optimizer_type ) ) get_kfac_update_fn_and_state ( params , data , get_position_fn , energy_data_val_and_grad , key , learning_rate_schedule , optimizer_config , record_param_l1_norm = False , apply_pmap = True ) Get an update param function, initial state, and key for KFAC. Parameters: Name Type Description Default params pytree params with which to initialize optimizer state required data pytree data with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required key PRNGKey PRNGKey with which to initialize optimizer state required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for KFAC required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Returns: Type Description (UpdateParamFn, kfac_opt.State, PRNGKey) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey Source code in vmcnet/updates/parse_config.py def get_kfac_update_fn_and_state ( params : P , data : D , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], key : PRNGKey , learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , kfac_opt . State ], kfac_opt . State , PRNGKey ]: \"\"\"Get an update param function, initial state, and key for KFAC. Args: params (pytree): params with which to initialize optimizer state data (pytree): data with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) key (PRNGKey): PRNGKey with which to initialize optimizer state learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for KFAC record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Returns: (UpdateParamFn, kfac_opt.State, PRNGKey): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey \"\"\" optimizer = kfac_ferminet_alpha . Optimizer ( energy_data_val_and_grad , l2_reg = optimizer_config . l2_reg , norm_constraint = optimizer_config . norm_constraint , value_func_has_aux = True , learning_rate_schedule = learning_rate_schedule , curvature_ema = optimizer_config . curvature_ema , inverse_update_period = optimizer_config . inverse_update_period , min_damping = optimizer_config . min_damping , num_burnin_steps = 0 , register_only_generic = optimizer_config . register_only_generic , estimation_mode = optimizer_config . estimation_mode , multi_device = apply_pmap , pmap_axis_name = utils . distribute . PMAP_AXIS_NAME , ) key , subkey = utils . distribute . split_or_psplit_key ( key , apply_pmap ) optimizer_state = optimizer . init ( params , subkey , get_position_fn ( data )) update_param_fn = create_kfac_update_param_fn ( optimizer , optimizer_config . damping , pacore . get_position_from_data , record_param_l1_norm = record_param_l1_norm , ) return update_param_fn , optimizer_state , key get_adam_update_fn_and_state ( params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , optimizer_config , record_param_l1_norm = False , apply_pmap = True ) Get an update param function and initial state for Adam. Parameters: Name Type Description Default params pytree params with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for Adam required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Returns: Type Description (UpdateParamFn, optax.OptState) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state Source code in vmcnet/updates/parse_config.py def get_adam_update_fn_and_state ( params : P , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , optax . OptState ], optax . OptState ]: \"\"\"Get an update param function and initial state for Adam. Args: params (pytree): params with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for Adam record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Returns: (UpdateParamFn, optax.OptState): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state \"\"\" optimizer = _get_adam_optax_optimizer ( learning_rate_schedule , optimizer_config ) return _get_optax_update_fn_and_state ( optimizer , params , get_position_fn , energy_data_val_and_grad , record_param_l1_norm , apply_pmap , ) get_sgd_update_fn_and_state ( params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , optimizer_config , record_param_l1_norm = False , apply_pmap = True ) Get an update param function and initial state for SGD. Parameters: Name Type Description Default params pytree params with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for SGD required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Returns: Type Description (UpdateParamFn, optax.OptState) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state Source code in vmcnet/updates/parse_config.py def get_sgd_update_fn_and_state ( params : P , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , optax . OptState ], optax . OptState ]: \"\"\"Get an update param function and initial state for SGD. Args: params (pytree): params with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for SGD record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Returns: (UpdateParamFn, optax.OptState): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state \"\"\" optimizer = _get_sgd_optax_optimizer ( learning_rate_schedule , optimizer_config ) return _get_optax_update_fn_and_state ( optimizer , params , get_position_fn , energy_data_val_and_grad , record_param_l1_norm , apply_pmap , ) get_sr_update_fn_and_state ( log_psi_apply , params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , optimizer_config , descent_config , record_param_l1_norm = False , apply_pmap = True , nan_safe = True ) Get an update param function and initial state for stochastic reconfiguration. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required params pytree params with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for stochastic reconfiguration required descent_config ConfigDict configuration for the gradient descent-like method used to apply the preconditioned updates required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True nan_safe bool whether the mean function used when centering the Jacobian of log|psi(x)| during the Fisher matvec is nan-safe. Defaults to True. True Exceptions: Type Description ValueError A non-supported descent type is requested. Currently only Adam and SGD are supported. Returns: Type Description (UpdateParamFn, optax.OptState) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state Source code in vmcnet/updates/parse_config.py def get_sr_update_fn_and_state ( log_psi_apply : ModelApply [ P ], params : P , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , descent_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , nan_safe : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , optax . OptState ], optax . OptState ]: \"\"\"Get an update param function and initial state for stochastic reconfiguration. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| params (pytree): params with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for stochastic reconfiguration descent_config (ConfigDict): configuration for the gradient descent-like method used to apply the preconditioned updates record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. nan_safe (bool, optional): whether the mean function used when centering the Jacobian of log|psi(x)| during the Fisher matvec is nan-safe. Defaults to True. Raises: ValueError: A non-supported descent type is requested. Currently only Adam and SGD are supported. Returns: (UpdateParamFn, optax.OptState): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state \"\"\" maxiter = optimizer_config . maxiter if optimizer_config . maxiter >= 0 else None mean_grad_fn = utils . distribute . get_mean_over_first_axis_fn ( nan_safe = nan_safe ) precondition_grad_fn = get_fisher_inverse_fn ( log_psi_apply , mean_grad_fn , damping = optimizer_config . damping , maxiter = maxiter , mode = SRMode [ optimizer_config . mode . upper ()], ) if optimizer_config . descent_type == \"adam\" : descent_optimizer = _get_adam_optax_optimizer ( learning_rate_schedule , descent_config ) elif optimizer_config . descent_type == \"sgd\" : descent_optimizer = _get_sgd_optax_optimizer ( learning_rate_schedule , descent_config ) else : raise ValueError ( \"Requested descent type not supported; {} was requested\" . format ( optimizer_config . descent_type ) ) def get_optimizer_step_count ( optimizer_state ): return optimizer_state [ 1 ] . count def optimizer_apply ( grad , params , optimizer_state , data ): preconditioned_grad = precondition_grad_fn ( grad , params , get_position_fn ( data )) step_count = get_optimizer_step_count ( optimizer_state ) learning_rate = learning_rate_schedule ( step_count ) constrained_grad = constrain_norm ( grad , preconditioned_grad , learning_rate , optimizer_config . norm_constraint ) updates , optimizer_state = descent_optimizer . update ( constrained_grad , optimizer_state , params ) params = optax . apply_updates ( params , updates ) return params , optimizer_state update_param_fn = create_grad_energy_update_param_fn ( energy_data_val_and_grad , optimizer_apply , get_position_fn = get_position_fn , record_param_l1_norm = record_param_l1_norm , apply_pmap = apply_pmap , ) optimizer_state = _init_optax_optimizer ( descent_optimizer , params , apply_pmap = apply_pmap ) return update_param_fn , optimizer_state","title":"parse_config"},{"location":"api/updates/parse_config/#vmcnet.updates.parse_config.get_update_fn_and_init_optimizer","text":"Get an update function and initialize optimizer state from the vmc configuration. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required vmc_config ConfigDict configuration for VMC required params pytree params with which to initialize optimizer state required data pytree data with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required key PRNGKey PRNGKey with which to initialize optimizer state required apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Exceptions: Type Description ValueError A non-supported optimizer type is requested. Currently, KFAC, Adam, SGD, and SR (with either Adam or SGD) is supported. Returns: Type Description (UpdateParamFn, OptimizerState, PRNGKey) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey Source code in vmcnet/updates/parse_config.py def get_update_fn_and_init_optimizer ( log_psi_apply : ModelApply [ P ], vmc_config : ConfigDict , params : P , data : D , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], key : PRNGKey , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , OptimizerState ], OptimizerState , PRNGKey ]: \"\"\"Get an update function and initialize optimizer state from the vmc configuration. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| vmc_config (ConfigDict): configuration for VMC params (pytree): params with which to initialize optimizer state data (pytree): data with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) key (PRNGKey): PRNGKey with which to initialize optimizer state apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Raises: ValueError: A non-supported optimizer type is requested. Currently, KFAC, Adam, SGD, and SR (with either Adam or SGD) is supported. Returns: (UpdateParamFn, OptimizerState, PRNGKey): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey \"\"\" learning_rate_schedule = _get_learning_rate_schedule ( vmc_config . optimizer [ vmc_config . optimizer_type ] ) if vmc_config . optimizer_type == \"kfac\" : return get_kfac_update_fn_and_state ( params , data , get_position_fn , energy_data_val_and_grad , key , learning_rate_schedule , vmc_config . optimizer . kfac , vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , ) elif vmc_config . optimizer_type == \"sgd\" : ( update_param_fn , optimizer_state ,) = get_sgd_update_fn_and_state ( params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , vmc_config . optimizer . sgd , vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , ) return update_param_fn , optimizer_state , key elif vmc_config . optimizer_type == \"adam\" : ( update_param_fn , optimizer_state ,) = get_adam_update_fn_and_state ( params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , vmc_config . optimizer . adam , vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , ) return update_param_fn , optimizer_state , key elif vmc_config . optimizer_type == \"sr\" : ( update_param_fn , optimizer_state ,) = get_sr_update_fn_and_state ( log_psi_apply , params , get_position_fn , energy_data_val_and_grad , learning_rate_schedule , vmc_config . optimizer . sr , vmc_config . optimizer [ vmc_config . optimizer . sr . descent_type ], vmc_config . record_param_l1_norm , apply_pmap = apply_pmap , nan_safe = vmc_config . nan_safe , ) return update_param_fn , optimizer_state , key else : raise ValueError ( \"Requested optimizer type not supported; {} was requested\" . format ( vmc_config . optimizer_type ) )","title":"get_update_fn_and_init_optimizer()"},{"location":"api/updates/parse_config/#vmcnet.updates.parse_config.get_kfac_update_fn_and_state","text":"Get an update param function, initial state, and key for KFAC. Parameters: Name Type Description Default params pytree params with which to initialize optimizer state required data pytree data with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required key PRNGKey PRNGKey with which to initialize optimizer state required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for KFAC required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Returns: Type Description (UpdateParamFn, kfac_opt.State, PRNGKey) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey Source code in vmcnet/updates/parse_config.py def get_kfac_update_fn_and_state ( params : P , data : D , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], key : PRNGKey , learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , kfac_opt . State ], kfac_opt . State , PRNGKey ]: \"\"\"Get an update param function, initial state, and key for KFAC. Args: params (pytree): params with which to initialize optimizer state data (pytree): data with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) key (PRNGKey): PRNGKey with which to initialize optimizer state learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for KFAC record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Returns: (UpdateParamFn, kfac_opt.State, PRNGKey): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), initial optimizer state, and PRNGKey \"\"\" optimizer = kfac_ferminet_alpha . Optimizer ( energy_data_val_and_grad , l2_reg = optimizer_config . l2_reg , norm_constraint = optimizer_config . norm_constraint , value_func_has_aux = True , learning_rate_schedule = learning_rate_schedule , curvature_ema = optimizer_config . curvature_ema , inverse_update_period = optimizer_config . inverse_update_period , min_damping = optimizer_config . min_damping , num_burnin_steps = 0 , register_only_generic = optimizer_config . register_only_generic , estimation_mode = optimizer_config . estimation_mode , multi_device = apply_pmap , pmap_axis_name = utils . distribute . PMAP_AXIS_NAME , ) key , subkey = utils . distribute . split_or_psplit_key ( key , apply_pmap ) optimizer_state = optimizer . init ( params , subkey , get_position_fn ( data )) update_param_fn = create_kfac_update_param_fn ( optimizer , optimizer_config . damping , pacore . get_position_from_data , record_param_l1_norm = record_param_l1_norm , ) return update_param_fn , optimizer_state , key","title":"get_kfac_update_fn_and_state()"},{"location":"api/updates/parse_config/#vmcnet.updates.parse_config.get_adam_update_fn_and_state","text":"Get an update param function and initial state for Adam. Parameters: Name Type Description Default params pytree params with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for Adam required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Returns: Type Description (UpdateParamFn, optax.OptState) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state Source code in vmcnet/updates/parse_config.py def get_adam_update_fn_and_state ( params : P , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , optax . OptState ], optax . OptState ]: \"\"\"Get an update param function and initial state for Adam. Args: params (pytree): params with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for Adam record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Returns: (UpdateParamFn, optax.OptState): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state \"\"\" optimizer = _get_adam_optax_optimizer ( learning_rate_schedule , optimizer_config ) return _get_optax_update_fn_and_state ( optimizer , params , get_position_fn , energy_data_val_and_grad , record_param_l1_norm , apply_pmap , )","title":"get_adam_update_fn_and_state()"},{"location":"api/updates/parse_config/#vmcnet.updates.parse_config.get_sgd_update_fn_and_state","text":"Get an update param function and initial state for SGD. Parameters: Name Type Description Default params pytree params with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for SGD required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True Returns: Type Description (UpdateParamFn, optax.OptState) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state Source code in vmcnet/updates/parse_config.py def get_sgd_update_fn_and_state ( params : P , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , optax . OptState ], optax . OptState ]: \"\"\"Get an update param function and initial state for SGD. Args: params (pytree): params with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for SGD record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. Returns: (UpdateParamFn, optax.OptState): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state \"\"\" optimizer = _get_sgd_optax_optimizer ( learning_rate_schedule , optimizer_config ) return _get_optax_update_fn_and_state ( optimizer , params , get_position_fn , energy_data_val_and_grad , record_param_l1_norm , apply_pmap , )","title":"get_sgd_update_fn_and_state()"},{"location":"api/updates/parse_config/#vmcnet.updates.parse_config.get_sr_update_fn_and_state","text":"Get an update param function and initial state for stochastic reconfiguration. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required params pytree params with which to initialize optimizer state required get_position_fn Callable function which gets the position array from the data required energy_data_val_and_grad Callable function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) required learning_rate_schedule Callable function which returns a learning rate from epoch number. Has signature epoch -> learning_rate required optimizer_config ConfigDict configuration for stochastic reconfiguration required descent_config ConfigDict configuration for the gradient descent-like method used to apply the preconditioned updates required record_param_l1_norm bool whether to record the L1 norm of the parameters in the metrics. Defaults to False. False apply_pmap bool whether to pmap the optimizer steps. Defaults to True. True nan_safe bool whether the mean function used when centering the Jacobian of log|psi(x)| during the Fisher matvec is nan-safe. Defaults to True. True Exceptions: Type Description ValueError A non-supported descent type is requested. Currently only Adam and SGD are supported. Returns: Type Description (UpdateParamFn, optax.OptState) update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state Source code in vmcnet/updates/parse_config.py def get_sr_update_fn_and_state ( log_psi_apply : ModelApply [ P ], params : P , get_position_fn : GetPositionFromData [ D ], energy_data_val_and_grad : physics . core . ValueGradEnergyFn [ P ], learning_rate_schedule : Callable [[ int ], jnp . float32 ], optimizer_config : ConfigDict , descent_config : ConfigDict , record_param_l1_norm : bool = False , apply_pmap : bool = True , nan_safe : bool = True , ) -> Tuple [ UpdateParamFn [ P , D , optax . OptState ], optax . OptState ]: \"\"\"Get an update param function and initial state for stochastic reconfiguration. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| params (pytree): params with which to initialize optimizer state get_position_fn (Callable): function which gets the position array from the data energy_data_val_and_grad (Callable): function which computes the clipped energy value and gradient. Has the signature (params, x) -> ((expected_energy, auxiliary_energy_data), grad_energy), where auxiliary_energy_data is the tuple (expected_variance, local_energies, unclipped_energy, unclipped_variance) learning_rate_schedule (Callable): function which returns a learning rate from epoch number. Has signature epoch -> learning_rate optimizer_config (ConfigDict): configuration for stochastic reconfiguration descent_config (ConfigDict): configuration for the gradient descent-like method used to apply the preconditioned updates record_param_l1_norm (bool, optional): whether to record the L1 norm of the parameters in the metrics. Defaults to False. apply_pmap (bool, optional): whether to pmap the optimizer steps. Defaults to True. nan_safe (bool, optional): whether the mean function used when centering the Jacobian of log|psi(x)| during the Fisher matvec is nan-safe. Defaults to True. Raises: ValueError: A non-supported descent type is requested. Currently only Adam and SGD are supported. Returns: (UpdateParamFn, optax.OptState): update param function with signature (params, data, optimizer_state, key) -> (new params, new state, metrics, new key), and initial optimizer state \"\"\" maxiter = optimizer_config . maxiter if optimizer_config . maxiter >= 0 else None mean_grad_fn = utils . distribute . get_mean_over_first_axis_fn ( nan_safe = nan_safe ) precondition_grad_fn = get_fisher_inverse_fn ( log_psi_apply , mean_grad_fn , damping = optimizer_config . damping , maxiter = maxiter , mode = SRMode [ optimizer_config . mode . upper ()], ) if optimizer_config . descent_type == \"adam\" : descent_optimizer = _get_adam_optax_optimizer ( learning_rate_schedule , descent_config ) elif optimizer_config . descent_type == \"sgd\" : descent_optimizer = _get_sgd_optax_optimizer ( learning_rate_schedule , descent_config ) else : raise ValueError ( \"Requested descent type not supported; {} was requested\" . format ( optimizer_config . descent_type ) ) def get_optimizer_step_count ( optimizer_state ): return optimizer_state [ 1 ] . count def optimizer_apply ( grad , params , optimizer_state , data ): preconditioned_grad = precondition_grad_fn ( grad , params , get_position_fn ( data )) step_count = get_optimizer_step_count ( optimizer_state ) learning_rate = learning_rate_schedule ( step_count ) constrained_grad = constrain_norm ( grad , preconditioned_grad , learning_rate , optimizer_config . norm_constraint ) updates , optimizer_state = descent_optimizer . update ( constrained_grad , optimizer_state , params ) params = optax . apply_updates ( params , updates ) return params , optimizer_state update_param_fn = create_grad_energy_update_param_fn ( energy_data_val_and_grad , optimizer_apply , get_position_fn = get_position_fn , record_param_l1_norm = record_param_l1_norm , apply_pmap = apply_pmap , ) optimizer_state = _init_optax_optimizer ( descent_optimizer , params , apply_pmap = apply_pmap ) return update_param_fn , optimizer_state","title":"get_sr_update_fn_and_state()"},{"location":"api/updates/sr/","text":"Stochastic reconfiguration (SR) routine. SRMode ( Enum ) Modes for computing the preconditioning by the Fisher inverse during SR. If LAZY, then uses composed jvp and vjp calls to lazily compute the various Jacobian-vector products. This is more computationally and memory-efficient. If DEBUG, then directly computes the Jacobian (per-example gradients) and uses jnp.matmul to compute the Jacobian-vector products. Defaults to LAZY. get_fisher_inverse_fn ( log_psi_apply , mean_grad_fn , damping = 0.001 , maxiter = None , mode =< SRMode . LAZY : 1 > ) Get a Fisher-preconditioned update. Given a gradient update grad_E, the function returned here approximates (0.25 * F + damping * I)^{-1} * grad_E, where F is the Fisher information matrix. The inversion is approximated via the conjugate gradient algorithm (possibly truncated to a finite number of iterations). This preconditioned gradient update, when used as-is, is also known as the stochastic reconfiguration algorithm. See https://arxiv.org/pdf/1909.02487.pdf, Appendix C for the connection between natural gradient descent and stochastic reconfiguration. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required mean_grad_fn Callable function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. required damping float multiple of the identity to add to the Fisher before inverting. Without this term, the approximation to the Fisher will always be less than full rank when nchains < nparams, and so CG will fail to converge. This should be tuned together with the learning rate. Defaults to 0.001. 0.001 maxiter int maximum number of CG iterations to do when computing the inverse application of the Fisher. Defaults to None, which uses maxiter equal to 10 * number of params. None mode SRMode mode of computing the forward Fisher-vector products. If LAZY, then uses composed jvp and vjp calls to lazily compute the various Jacobian-vector products. This is more computationally and memory-efficient. If DEBUG, then directly computes the Jacobian (per-example gradients) and uses jnp.matmul to compute the Jacobian-vector products. Defaults to LAZY. <SRMode.LAZY: 1> Returns: Type Description Callable function which computes the gradient preconditioned with the inverse of the Fisher information matrix. Has the signature (energy_grad, params, positions) -> preconditioned_grad Source code in vmcnet/updates/sr.py def get_fisher_inverse_fn ( log_psi_apply : ModelApply [ P ], mean_grad_fn : Callable [[ Array ], Array ], damping : float = 0.001 , maxiter : Optional [ int ] = None , mode : SRMode = SRMode . LAZY , ): \"\"\"Get a Fisher-preconditioned update. Given a gradient update grad_E, the function returned here approximates (0.25 * F + damping * I)^{-1} * grad_E, where F is the Fisher information matrix. The inversion is approximated via the conjugate gradient algorithm (possibly truncated to a finite number of iterations). This preconditioned gradient update, when used as-is, is also known as the stochastic reconfiguration algorithm. See https://arxiv.org/pdf/1909.02487.pdf, Appendix C for the connection between natural gradient descent and stochastic reconfiguration. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| mean_grad_fn (Callable): function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. damping (float, optional): multiple of the identity to add to the Fisher before inverting. Without this term, the approximation to the Fisher will always be less than full rank when nchains < nparams, and so CG will fail to converge. This should be tuned together with the learning rate. Defaults to 0.001. maxiter (int, optional): maximum number of CG iterations to do when computing the inverse application of the Fisher. Defaults to None, which uses maxiter equal to 10 * number of params. mode (SRMode, optional): mode of computing the forward Fisher-vector products. If LAZY, then uses composed jvp and vjp calls to lazily compute the various Jacobian-vector products. This is more computationally and memory-efficient. If DEBUG, then directly computes the Jacobian (per-example gradients) and uses jnp.matmul to compute the Jacobian-vector products. Defaults to LAZY. Returns: Callable: function which computes the gradient preconditioned with the inverse of the Fisher information matrix. Has the signature (energy_grad, params, positions) -> preconditioned_grad \"\"\" # TODO(Jeffmin): explore preconditioners for speeding up convergence and to provide # more stability # TODO(Jeffmin): investigate damping scheduling and possibly adaptive damping if mode == SRMode . DEBUG : def raveled_log_psi_grad ( params : P , positions : Array ) -> Array : log_grads = jax . grad ( log_psi_apply )( params , positions ) return jax . flatten_util . ravel_pytree ( log_grads )[ 0 ] batch_raveled_log_psi_grad = jax . vmap ( raveled_log_psi_grad , in_axes = ( None , 0 )) def precondition_grad_with_fisher ( energy_grad : P , params : P , positions : Array ) -> P : raveled_energy_grad , unravel_fn = jax . flatten_util . ravel_pytree ( energy_grad ) log_psi_grads = batch_raveled_log_psi_grad ( params , positions ) mean_log_psi_grads = mean_grad_fn ( log_psi_grads ) centered_log_psi_grads = ( log_psi_grads - mean_log_psi_grads ) # shape (nchains, nparams) def fisher_apply ( x : Array ) -> Array : # x is shape (nparams,) nchains_local = centered_log_psi_grads . shape [ 0 ] centered_jacobian_vector_prod = jnp . matmul ( centered_log_psi_grads , x ) local_fisher_times_x = ( jnp . matmul ( jnp . transpose ( centered_log_psi_grads ), centered_jacobian_vector_prod , ) / nchains_local ) fisher_times_x = pmean_if_pmap ( local_fisher_times_x ) return fisher_times_x + damping * x sr_grad , _ = jscp . sparse . linalg . cg ( fisher_apply , raveled_energy_grad , x0 = raveled_energy_grad , maxiter = maxiter , ) return unravel_fn ( sr_grad ) elif mode == SRMode . LAZY : def precondition_grad_with_fisher ( energy_grad : P , params : P , positions : Array ) -> P : def partial_log_psi_apply ( params : P ) -> Array : return log_psi_apply ( params , positions ) _ , vjp_fn = jax . vjp ( partial_log_psi_apply , params ) def fisher_apply ( x : Array ) -> Array : # x is a pytree with same structure as params nchains_local = positions . shape [ 0 ] _ , jacobian_vector_prod = jax . jvp ( partial_log_psi_apply , ( params ,), ( x ,) ) mean_jacobian_vector_prod = mean_grad_fn ( jacobian_vector_prod ) centered_jacobian_vector_prod = ( jacobian_vector_prod - mean_jacobian_vector_prod ) local_device_fisher_times_x = multiply_tree_by_scalar ( vjp_fn ( centered_jacobian_vector_prod )[ 0 ], 1.0 / nchains_local ) fisher_times_x = pmean_if_pmap ( local_device_fisher_times_x ) return tree_sum ( fisher_times_x , multiply_tree_by_scalar ( x , damping )) sr_grad , _ = jscp . sparse . linalg . cg ( fisher_apply , energy_grad , x0 = energy_grad , maxiter = maxiter , ) return sr_grad else : raise ValueError ( \"Requested Fisher apply mode not supported; only {} are supported, \" \"but {} was requested.\" . format ( \", \" . join ( SRMode . __members__ . keys ()), mode ) ) return precondition_grad_with_fisher","title":"sr"},{"location":"api/updates/sr/#vmcnet.updates.sr.SRMode","text":"Modes for computing the preconditioning by the Fisher inverse during SR. If LAZY, then uses composed jvp and vjp calls to lazily compute the various Jacobian-vector products. This is more computationally and memory-efficient. If DEBUG, then directly computes the Jacobian (per-example gradients) and uses jnp.matmul to compute the Jacobian-vector products. Defaults to LAZY.","title":"SRMode"},{"location":"api/updates/sr/#vmcnet.updates.sr.get_fisher_inverse_fn","text":"Get a Fisher-preconditioned update. Given a gradient update grad_E, the function returned here approximates (0.25 * F + damping * I)^{-1} * grad_E, where F is the Fisher information matrix. The inversion is approximated via the conjugate gradient algorithm (possibly truncated to a finite number of iterations). This preconditioned gradient update, when used as-is, is also known as the stochastic reconfiguration algorithm. See https://arxiv.org/pdf/1909.02487.pdf, Appendix C for the connection between natural gradient descent and stochastic reconfiguration. Parameters: Name Type Description Default log_psi_apply Callable computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| required mean_grad_fn Callable function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. required damping float multiple of the identity to add to the Fisher before inverting. Without this term, the approximation to the Fisher will always be less than full rank when nchains < nparams, and so CG will fail to converge. This should be tuned together with the learning rate. Defaults to 0.001. 0.001 maxiter int maximum number of CG iterations to do when computing the inverse application of the Fisher. Defaults to None, which uses maxiter equal to 10 * number of params. None mode SRMode mode of computing the forward Fisher-vector products. If LAZY, then uses composed jvp and vjp calls to lazily compute the various Jacobian-vector products. This is more computationally and memory-efficient. If DEBUG, then directly computes the Jacobian (per-example gradients) and uses jnp.matmul to compute the Jacobian-vector products. Defaults to LAZY. <SRMode.LAZY: 1> Returns: Type Description Callable function which computes the gradient preconditioned with the inverse of the Fisher information matrix. Has the signature (energy_grad, params, positions) -> preconditioned_grad Source code in vmcnet/updates/sr.py def get_fisher_inverse_fn ( log_psi_apply : ModelApply [ P ], mean_grad_fn : Callable [[ Array ], Array ], damping : float = 0.001 , maxiter : Optional [ int ] = None , mode : SRMode = SRMode . LAZY , ): \"\"\"Get a Fisher-preconditioned update. Given a gradient update grad_E, the function returned here approximates (0.25 * F + damping * I)^{-1} * grad_E, where F is the Fisher information matrix. The inversion is approximated via the conjugate gradient algorithm (possibly truncated to a finite number of iterations). This preconditioned gradient update, when used as-is, is also known as the stochastic reconfiguration algorithm. See https://arxiv.org/pdf/1909.02487.pdf, Appendix C for the connection between natural gradient descent and stochastic reconfiguration. Args: log_psi_apply (Callable): computes log|psi(x)|, where the signature of this function is (params, x) -> log|psi(x)| mean_grad_fn (Callable): function which is used to average the local gradient terms over all local devices. Has the signature local_grads -> avg_grad / 2, and should only average over the batch axis 0. damping (float, optional): multiple of the identity to add to the Fisher before inverting. Without this term, the approximation to the Fisher will always be less than full rank when nchains < nparams, and so CG will fail to converge. This should be tuned together with the learning rate. Defaults to 0.001. maxiter (int, optional): maximum number of CG iterations to do when computing the inverse application of the Fisher. Defaults to None, which uses maxiter equal to 10 * number of params. mode (SRMode, optional): mode of computing the forward Fisher-vector products. If LAZY, then uses composed jvp and vjp calls to lazily compute the various Jacobian-vector products. This is more computationally and memory-efficient. If DEBUG, then directly computes the Jacobian (per-example gradients) and uses jnp.matmul to compute the Jacobian-vector products. Defaults to LAZY. Returns: Callable: function which computes the gradient preconditioned with the inverse of the Fisher information matrix. Has the signature (energy_grad, params, positions) -> preconditioned_grad \"\"\" # TODO(Jeffmin): explore preconditioners for speeding up convergence and to provide # more stability # TODO(Jeffmin): investigate damping scheduling and possibly adaptive damping if mode == SRMode . DEBUG : def raveled_log_psi_grad ( params : P , positions : Array ) -> Array : log_grads = jax . grad ( log_psi_apply )( params , positions ) return jax . flatten_util . ravel_pytree ( log_grads )[ 0 ] batch_raveled_log_psi_grad = jax . vmap ( raveled_log_psi_grad , in_axes = ( None , 0 )) def precondition_grad_with_fisher ( energy_grad : P , params : P , positions : Array ) -> P : raveled_energy_grad , unravel_fn = jax . flatten_util . ravel_pytree ( energy_grad ) log_psi_grads = batch_raveled_log_psi_grad ( params , positions ) mean_log_psi_grads = mean_grad_fn ( log_psi_grads ) centered_log_psi_grads = ( log_psi_grads - mean_log_psi_grads ) # shape (nchains, nparams) def fisher_apply ( x : Array ) -> Array : # x is shape (nparams,) nchains_local = centered_log_psi_grads . shape [ 0 ] centered_jacobian_vector_prod = jnp . matmul ( centered_log_psi_grads , x ) local_fisher_times_x = ( jnp . matmul ( jnp . transpose ( centered_log_psi_grads ), centered_jacobian_vector_prod , ) / nchains_local ) fisher_times_x = pmean_if_pmap ( local_fisher_times_x ) return fisher_times_x + damping * x sr_grad , _ = jscp . sparse . linalg . cg ( fisher_apply , raveled_energy_grad , x0 = raveled_energy_grad , maxiter = maxiter , ) return unravel_fn ( sr_grad ) elif mode == SRMode . LAZY : def precondition_grad_with_fisher ( energy_grad : P , params : P , positions : Array ) -> P : def partial_log_psi_apply ( params : P ) -> Array : return log_psi_apply ( params , positions ) _ , vjp_fn = jax . vjp ( partial_log_psi_apply , params ) def fisher_apply ( x : Array ) -> Array : # x is a pytree with same structure as params nchains_local = positions . shape [ 0 ] _ , jacobian_vector_prod = jax . jvp ( partial_log_psi_apply , ( params ,), ( x ,) ) mean_jacobian_vector_prod = mean_grad_fn ( jacobian_vector_prod ) centered_jacobian_vector_prod = ( jacobian_vector_prod - mean_jacobian_vector_prod ) local_device_fisher_times_x = multiply_tree_by_scalar ( vjp_fn ( centered_jacobian_vector_prod )[ 0 ], 1.0 / nchains_local ) fisher_times_x = pmean_if_pmap ( local_device_fisher_times_x ) return tree_sum ( fisher_times_x , multiply_tree_by_scalar ( x , damping )) sr_grad , _ = jscp . sparse . linalg . cg ( fisher_apply , energy_grad , x0 = energy_grad , maxiter = maxiter , ) return sr_grad else : raise ValueError ( \"Requested Fisher apply mode not supported; only {} are supported, \" \"but {} was requested.\" . format ( \", \" . join ( SRMode . __members__ . keys ()), mode ) ) return precondition_grad_with_fisher","title":"get_fisher_inverse_fn()"},{"location":"api/utils/checkpoint/","text":"Utilities for checkpointing and logging the VMC loop. Running queues of energy and variance histories are tracked, along with their averages. Unlike many of the other routines in this package, these are not pure functions, as they modify the RunningMetrics inside RunningEnergyVariance. CheckpointWriter ( ThreadedWriter ) A ThreadedWriter for saving checkpoints during training. write_out_data ( self , directory , name , checkpoint_data ) Save checkpoint data. Parameters: Name Type Description Default directory str directory in which to write the checkpoint required name str filename for the checkpoint required checkpoint_data CheckpointData checkpoint data which contains: epoch (int): epoch at which checkpoint is being saved data (pytree or Array): walker data to save params (pytree): model parameters to save optimizer_state (pytree): optimizer state to save key (PRNGKey): RNG key, used to reproduce exact behavior from checkpoint required Source code in vmcnet/utils/checkpoint.py def write_out_data ( self , directory : str , name : str , checkpoint_data : CheckpointData ): \"\"\"Save checkpoint data. Args: directory (str): directory in which to write the checkpoint name (str): filename for the checkpoint checkpoint_data (CheckpointData): checkpoint data which contains: epoch (int): epoch at which checkpoint is being saved data (pytree or Array): walker data to save params (pytree): model parameters to save optimizer_state (pytree): optimizer state to save key (PRNGKey): RNG key, used to reproduce exact behavior from checkpoint \"\"\" io . save_vmc_state ( directory , name , checkpoint_data ) save_data ( self , directory , name , checkpoint_data ) Queue up checkpoint data to be written to disc. Source code in vmcnet/utils/checkpoint.py def save_data ( self , directory : str , name : str , checkpoint_data : CheckpointData ): \"\"\"Queue up checkpoint data to be written to disc.\"\"\" checkpoint_data = io . process_checkpoint_data_for_saving ( checkpoint_data ) # Move data to CPU to avoid clogging up GPU memory with queued checkpoints checkpoint_data = jax . device_put ( checkpoint_data , jax . devices ( \"cpu\" )[ 0 ]) super () . save_data ( directory , name , checkpoint_data ) MetricsWriter ( ThreadedWriter ) A ThreadedWriter for saving metrics during training. write_out_data ( self , directory , name , metrics ) Save metrics to individual text files. Source code in vmcnet/utils/checkpoint.py def write_out_data ( self , directory : str , name : str , metrics : Dict ): \"\"\"Save metrics to individual text files.\"\"\" del name # unused, each metric gets its own file for metric , metric_val in metrics . items (): io . append_metric_to_file ( metric_val , directory , metric ) RunningEnergyVariance ( tuple ) Running energy history and variance history, packaged together. __new__ ( _cls , energy , variance ) special staticmethod Create new instance of RunningEnergyVariance(energy, variance) __repr__ ( self ) special Return a nicely formatted representation string Source code in vmcnet/utils/checkpoint.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in vmcnet/utils/checkpoint.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) RunningMetric dataclass Running history and average of a metric for checkpointing purposes. Attributes: Name Type Description nhistory_max int maximum length of the running history to keep when adding new values avg jnp.float32 the running average, should be equal to jnp.mean(self.history). Stored here to avoid recomputation when new values are added history deque[jnp.float32] the running history of the metric move_history_window ( self , new_value ) Append new value to running history, remove oldest if length > nhistory_max. Parameters: Name Type Description Default new_value jnp.float32 new value to insert into the history required Source code in vmcnet/utils/checkpoint.py def move_history_window ( self , new_value : jnp . float32 ): \"\"\"Append new value to running history, remove oldest if length > nhistory_max. Args: new_value (jnp.float32): new value to insert into the history \"\"\" if self . nhistory_max <= 0 : return history_length = len ( self . history ) self . history . append ( new_value ) self_sum = history_length * self . avg self_sum += new_value history_length += 1 if history_length >= self . nhistory_max : oldest_value = self . history . popleft () self_sum -= oldest_value history_length -= 1 self . avg = self_sum / history_length ThreadedWriter ( Generic ) A simple asynchronous writer to handle file io during training. Spins up a thread for the file IO so that it does not block the main line of the training procedure. While Python threads do not provide true parallelism of CPU computations across cores, they do allow us to write to files and run Jax computations simultaneously. __init__ ( self ) special Create a new ThreadedWriter. Source code in vmcnet/utils/checkpoint.py def __init__ ( self ): \"\"\"Create a new ThreadedWriter.\"\"\" self . _thread = threading . Thread ( target = self . _run_thread ) self . _done = False self . _queue = queue . Queue () write_out_data ( self , directory , name , data_to_save ) Abstract method which saves a piece of data pulled from the queue. Parameters: Name Type Description Default directory str directory in which to write the checkpoint required name str filename for the checkpoint required data_to_save Any data to save required Source code in vmcnet/utils/checkpoint.py @abstractmethod def write_out_data ( self , directory : str , name : str , data_to_save : T ): \"\"\"Abstract method which saves a piece of data pulled from the queue. Args: directory (str): directory in which to write the checkpoint name (str): filename for the checkpoint data_to_save (Any): data to save \"\"\" pass initialize ( self ) Initialize the ThreadedWriter by starting its internal thread. Source code in vmcnet/utils/checkpoint.py def initialize ( self ): \"\"\"Initialize the ThreadedWriter by starting its internal thread.\"\"\" self . _thread . start () save_data ( self , directory , name , data_to_save ) Queue up data to be written to disc. Source code in vmcnet/utils/checkpoint.py def save_data ( self , directory : str , name : str , data_to_save : T ): \"\"\"Queue up data to be written to disc.\"\"\" self . _queue . put (( directory , name , data_to_save )) close_and_await ( self ) Stop the thread by setting a flag, and return once it gets the message. Source code in vmcnet/utils/checkpoint.py def close_and_await ( self ): \"\"\"Stop the thread by setting a flag, and return once it gets the message.\"\"\" self . _done = True self . _thread . join () __enter__ ( self ) special Enter a ThreadedWriter's context, starting up a thread. Source code in vmcnet/utils/checkpoint.py def __enter__ ( self ): \"\"\"Enter a ThreadedWriter's context, starting up a thread.\"\"\" self . initialize () return self __exit__ ( self , exc_type , exc_value , traceback ) special Wait for the thread to finish, then leave the ThreadedWriter's context. Source code in vmcnet/utils/checkpoint.py def __exit__ ( self , exc_type , exc_value , traceback ): \"\"\"Wait for the thread to finish, then leave the ThreadedWriter's context.\"\"\" self . close_and_await () initialize_checkpointing ( checkpoint_dir , nhistory_max , logdir = None , checkpoint_every = None ) Initialize checkpointing objects. A suffix is added to the checkpointing directory if one with the same name already exists in the logdir. The checkpointing metric (error-adjusted running energy average) is initialized to infinity, and empty arrays are initialized in running_energy_and_variance. The best checkpoint data is initialized to None, and saved_nan_checkpoint is initialized to False. Source code in vmcnet/utils/checkpoint.py def initialize_checkpointing ( checkpoint_dir : str , nhistory_max : int , logdir : str = None , checkpoint_every : int = None , ) -> Tuple [ str , jnp . float32 , RunningEnergyVariance , Optional [ CheckpointData ], bool ]: \"\"\"Initialize checkpointing objects. A suffix is added to the checkpointing directory if one with the same name already exists in the logdir. The checkpointing metric (error-adjusted running energy average) is initialized to infinity, and empty arrays are initialized in running_energy_and_variance. The best checkpoint data is initialized to None, and saved_nan_checkpoint is initialized to False. \"\"\" if logdir is not None : logging . info ( \"Saving to %s \" , logdir ) os . makedirs ( logdir , exist_ok = True ) if checkpoint_every is not None : checkpoint_dir = io . add_suffix_for_uniqueness ( checkpoint_dir , logdir ) os . makedirs ( os . path . join ( logdir , checkpoint_dir ), exist_ok = False ) checkpoint_metric = jnp . inf running_energy_and_variance = RunningEnergyVariance ( RunningMetric ( nhistory_max ), RunningMetric ( nhistory_max ) ) best_checkpoint_data = None saved_nan_checkpoint = False return ( checkpoint_dir , checkpoint_metric , running_energy_and_variance , best_checkpoint_data , saved_nan_checkpoint , ) finish_checkpointing ( checkpoint_writer , best_checkpoint_data = None , logdir = None ) Save any final checkpoint data to the CheckpointWriter. Source code in vmcnet/utils/checkpoint.py def finish_checkpointing ( checkpoint_writer : CheckpointWriter , best_checkpoint_data : CheckpointData = None , logdir : str = None , ): \"\"\"Save any final checkpoint data to the CheckpointWriter.\"\"\" if logdir is not None and best_checkpoint_data is not None : checkpoint_writer . save_data ( logdir , CHECKPOINT_FILE_NAME , best_checkpoint_data ) get_checkpoint_metric ( energy_running_avg , variance_running_avg , nsamples , variance_scale ) Get an error-adjusted running average of the energy for checkpointing. The parameter variance_scale can be tuned and probably should scale linearly with some estimate of the integrated autocorrelation. Higher means more allergic to high variance, lower means more allergic to high energies. Parameters: Name Type Description Default energy_running_avg jnp.float32 running average of the energy required variance_running_avg jnp.float32 running average of the variance required nsamples int total number of samples reflected in the running averages, equal to the number of parallel chains times the length of the history required variance_scale float weight of the variance part of the checkpointing metric. The final effect on the variance part is to scale it by jnp.sqrt(variance_scale), i.e. to treat it like the integrated autocorrelation. required Returns: Type Description jnp.float32 error adjusted running average of the energy Source code in vmcnet/utils/checkpoint.py def get_checkpoint_metric ( energy_running_avg : jnp . float32 , variance_running_avg : jnp . float32 , nsamples : int , variance_scale : float , ) -> jnp . float32 : \"\"\"Get an error-adjusted running average of the energy for checkpointing. The parameter variance_scale can be tuned and probably should scale linearly with some estimate of the integrated autocorrelation. Higher means more allergic to high variance, lower means more allergic to high energies. Args: energy_running_avg (jnp.float32): running average of the energy variance_running_avg (jnp.float32): running average of the variance nsamples (int): total number of samples reflected in the running averages, equal to the number of parallel chains times the length of the history variance_scale (float): weight of the variance part of the checkpointing metric. The final effect on the variance part is to scale it by jnp.sqrt(variance_scale), i.e. to treat it like the integrated autocorrelation. Returns: jnp.float32: error adjusted running average of the energy \"\"\" # TODO(Jeffmin): eventually maybe put in some cheap best guess at the IAC? if variance_scale <= 0.0 or nsamples <= 0 : return energy_running_avg effective_nsamples = nsamples / variance_scale return energy_running_avg + jnp . sqrt ( variance_running_avg / effective_nsamples ) save_metrics_and_handle_checkpoints ( epoch , old_params , new_params , optimizer_state , old_data , new_data , key , metrics , nchains , running_energy_and_variance , checkpoint_writer , metrics_writer , checkpoint_metric , logdir = None , variance_scale = 10.0 , checkpoint_every = None , best_checkpoint_every = None , best_checkpoint_data = None , checkpoint_dir = 'checkpoints' , checkpoint_if_nans = False , only_checkpoint_first_nans = True , saved_nans_checkpoint = False , record_amplitudes = False , get_amplitude_fn = None ) Checkpoint the current state of the VMC loop. There are two situations to checkpoint: 1) Regularly, every x epochs, to handle job preemption and track parameters/metrics/state over time, and 2) Whenever a checkpoint metric improves, i.e. the error adjusted running average of the energy. This is not a pure function, as it modifies the running energy and variance history. Parameters: Name Type Description Default epoch int current epoch number required old_params pytree-like model parameters, from before the update function. Needs to be serializable via np.savez . required new_params pytree-like model parameters, from after the update function. required optimizer_state pytree-like running state of the optimizer other than the trainable parameters. Needs to be serialiable via np.savez required old_data pytree-like previous mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required new_data pytree-like new mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required metrics dict dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func: utils.io.write_metric_to_file . required nchains int number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. required running_energy_and_variance RunningEnergyVariance running history of energies and variances required checkpoint_metric jnp.float32 current best error adjusted running average of the energy history required best_checkpoint_every int limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of best_checkpoint_every , we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. None logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. None variance_scale float scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func: ~vmctrain.train.vmc.get_checkpoint_metric . Defaults to 10.0. 10.0 checkpoint_every int how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to None. None best_checkpoint_data CheckpointData the data needed to save a checkpoint for the best energy observed so far. None checkpoint_dir str name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". 'checkpoints' checkpoint_if_nans bool whether to save checkpoints when nan energy values are recorded. Defaults to False. False only_checkpoint_first_nans bool whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. True saved_nans_checkpoint bool whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. False Returns: Type Description (jnp.float32, str, CheckpointData, bool) best error-adjusted energy average, then string indicating if checkpointing has been done, then new best checkpoint data (or None), then the updated value of saved_nans_checkpoint. Source code in vmcnet/utils/checkpoint.py def save_metrics_and_handle_checkpoints ( epoch : int , old_params : P , new_params : P , optimizer_state : S , old_data : D , new_data : D , key : PRNGKey , metrics : Dict , nchains : int , running_energy_and_variance : RunningEnergyVariance , checkpoint_writer : CheckpointWriter , metrics_writer : MetricsWriter , checkpoint_metric : jnp . float32 , logdir : Optional [ str ] = None , variance_scale : float = 10.0 , checkpoint_every : Optional [ int ] = None , best_checkpoint_every : Optional [ int ] = None , best_checkpoint_data : Optional [ CheckpointData [ D , P , S ]] = None , checkpoint_dir : str = \"checkpoints\" , checkpoint_if_nans : bool = False , only_checkpoint_first_nans : bool = True , saved_nans_checkpoint : bool = False , record_amplitudes : bool = False , get_amplitude_fn : Optional [ GetAmplitudeFromData [ D ]] = None , ) -> Tuple [ jnp . float32 , str , Optional [ CheckpointData [ D , P , S ]], bool ]: \"\"\"Checkpoint the current state of the VMC loop. There are two situations to checkpoint: 1) Regularly, every x epochs, to handle job preemption and track parameters/metrics/state over time, and 2) Whenever a checkpoint metric improves, i.e. the error adjusted running average of the energy. This is not a pure function, as it modifies the running energy and variance history. Args: epoch (int): current epoch number old_params (pytree-like): model parameters, from before the update function. Needs to be serializable via `np.savez`. new_params (pytree-like): model parameters, from after the update function. optimizer_state (pytree-like): running state of the optimizer other than the trainable parameters. Needs to be serialiable via `np.savez` old_data (pytree-like): previous mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` new_data (pytree-like): new mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` metrics (dict): dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func:`utils.io.write_metric_to_file`. nchains (int): number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. running_energy_and_variance (RunningEnergyVariance): running history of energies and variances checkpoint_metric (jnp.float32): current best error adjusted running average of the energy history best_checkpoint_every (int): limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of `best_checkpoint_every`, we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. logdir (str, optional): name of parent log directory. If None, no checkpointing is done. Defaults to None. variance_scale (float, optional): scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func:`~vmctrain.train.vmc.get_checkpoint_metric`. Defaults to 10.0. checkpoint_every (int, optional): how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to None. best_checkpoint_data (CheckpointData, optional): the data needed to save a checkpoint for the best energy observed so far. checkpoint_dir (str, optional): name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". checkpoint_if_nans (bool, optional): whether to save checkpoints when nan energy values are recorded. Defaults to False. only_checkpoint_first_nans (bool, optional): whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. saved_nans_checkpoint (bool, optional): whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. Returns: (jnp.float32, str, CheckpointData, bool): best error-adjusted energy average, then string indicating if checkpointing has been done, then new best checkpoint data (or None), then the updated value of saved_nans_checkpoint. \"\"\" checkpoint_str = \"\" if logdir is None or metrics is None : # do nothing return ( checkpoint_metric , checkpoint_str , best_checkpoint_data , saved_nans_checkpoint , ) _add_amplitude_to_metrics_if_requested ( metrics , new_data , record_amplitudes , get_amplitude_fn ) checkpoint_str , saved_nans_checkpoint = save_metrics_and_regular_checkpoint ( epoch , old_params , new_params , optimizer_state , old_data , key , metrics , logdir , checkpoint_writer , metrics_writer , checkpoint_dir , checkpoint_str , checkpoint_every , checkpoint_if_nans = checkpoint_if_nans , only_checkpoint_first_nans = only_checkpoint_first_nans , saved_nans_checkpoint = saved_nans_checkpoint , ) ( checkpoint_str , error_adjusted_running_avg , new_best_checkpoint_data , ) = track_and_save_best_checkpoint ( epoch , old_params , optimizer_state , old_data , key , metrics , nchains , running_energy_and_variance , checkpoint_writer , checkpoint_metric , logdir , variance_scale , checkpoint_str , best_checkpoint_every , best_checkpoint_data , ) return ( jnp . minimum ( error_adjusted_running_avg , checkpoint_metric ), checkpoint_str , new_best_checkpoint_data , saved_nans_checkpoint , ) track_and_save_best_checkpoint ( epoch , old_params , optimizer_state , data , key , metrics , nchains , running_energy_and_variance , checkpoint_writer , checkpoint_metric , logdir , variance_scale , checkpoint_str , best_checkpoint_every = None , best_checkpoint_data = None ) Update running avgs and checkpoint if the error-adjusted energy avg improves. Parameters: Name Type Description Default epoch int current epoch number required old_params pytree-like model parameters, from before the update function. Needs to be serializable via np.savez . required optimizer_state pytree-like running state of the optimizer other than the trainable parameters. Needs to be serialiable via np.savez required data pytree-like current mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required metrics dict dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func: utils.io.write_metric_to_file . required nchains int number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. required running_energy_and_variance RunningEnergyVariance running history of energies and variances required checkpoint_metric jnp.float32 current best error adjusted running average of the energy history required logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. required variance_scale float scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func: ~vmctrain.train.vmc.get_checkpoint_metric . required checkpoint_str str string indicating whether checkpointing has previously occurred required best_checkpoint_every int limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of best_checkpoint_every , we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. None best_checkpoint_data CheckpointData the data needed to save a checkpoint for the best energy observed so far. None Returns: Type Description (str, jnp.float32, CheckpointData) previous checkpointing string with additional info if this function did checkpointing, then best error-adjusted energy average, then new best checkpoint data, or None. Source code in vmcnet/utils/checkpoint.py def track_and_save_best_checkpoint ( epoch : int , old_params : P , optimizer_state : S , data : D , key : PRNGKey , metrics : Dict , nchains : int , running_energy_and_variance : RunningEnergyVariance , checkpoint_writer : CheckpointWriter , checkpoint_metric : jnp . float32 , logdir : str , variance_scale : float , checkpoint_str : str , best_checkpoint_every : Optional [ int ] = None , best_checkpoint_data : Optional [ CheckpointData [ D , P , S ]] = None , ) -> Tuple [ str , jnp . float32 , Optional [ CheckpointData [ D , P , S ]]]: \"\"\"Update running avgs and checkpoint if the error-adjusted energy avg improves. Args: epoch (int): current epoch number old_params (pytree-like): model parameters, from before the update function. Needs to be serializable via `np.savez`. optimizer_state (pytree-like): running state of the optimizer other than the trainable parameters. Needs to be serialiable via `np.savez` data (pytree-like): current mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` metrics (dict): dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func:`utils.io.write_metric_to_file`. nchains (int): number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. running_energy_and_variance (RunningEnergyVariance): running history of energies and variances checkpoint_metric (jnp.float32): current best error adjusted running average of the energy history logdir (str): name of parent log directory. If None, no checkpointing is done. Defaults to None. variance_scale (float): scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func:`~vmctrain.train.vmc.get_checkpoint_metric`. checkpoint_str (str): string indicating whether checkpointing has previously occurred best_checkpoint_every (int, optional): limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of `best_checkpoint_every`, we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. best_checkpoint_data (CheckpointData, optional): the data needed to save a checkpoint for the best energy observed so far. Returns: (str, jnp.float32, CheckpointData): previous checkpointing string with additional info if this function did checkpointing, then best error-adjusted energy average, then new best checkpoint data, or None. \"\"\" if best_checkpoint_every is not None : energy , variance = running_energy_and_variance energy . move_history_window ( metrics [ \"energy\" ]) variance . move_history_window ( metrics [ \"variance\" ]) error_adjusted_running_avg = get_checkpoint_metric ( energy . avg , variance . avg , nchains * len ( energy . history ), variance_scale ) if error_adjusted_running_avg < checkpoint_metric : best_checkpoint_data = ( epoch , data , old_params , optimizer_state , key , ) should_save_best_checkpoint = ( epoch + 1 ) % best_checkpoint_every == 0 if should_save_best_checkpoint and best_checkpoint_data is not None : checkpoint_writer . save_data ( logdir , CHECKPOINT_FILE_NAME , best_checkpoint_data ) checkpoint_str = checkpoint_str + \", best weights saved\" best_checkpoint_data = None else : error_adjusted_running_avg = checkpoint_metric return checkpoint_str , error_adjusted_running_avg , best_checkpoint_data save_metrics_and_regular_checkpoint ( epoch , old_params , new_params , optimizer_state , data , key , metrics , logdir , checkpoint_writer , metrics_writer , checkpoint_dir , checkpoint_str , checkpoint_every = None , checkpoint_if_nans = False , only_checkpoint_first_nans = True , saved_nans_checkpoint = False ) Save current metrics to file, and save model state regularly. This currently touches the disk repeatedly, once for each metric, which is probably fairly inefficient, especially if called every epoch (as it currently is in :func: ~vmcnet.train.vmc.vmc_loop ). Parameters: Name Type Description Default epoch int current epoch number required old_params pytree-like model parameters, from before the update function. Needs to be serializable via np.savez . required new_params pytree-like model parameters, from after the update function. required optimizer_state pytree-like running state of the optimizer other than the trainable parameters. Needs to be serialiable via np.savez required data pytree-like current mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required metrics dict dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func: utils.io.write_metric_to_file . required checkpoint_str str string indicating whether checkpointing has previously occurred required logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. required checkpoint_dir str name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". required checkpoint_every int how often to regularly save checkpoints. If None, this function doesn't save the model state. Defaults to None. None checkpoint_if_nans bool whether to save checkpoints when nan energy values are recorded. Defaults to False. False only_checkpoint_first_nans bool whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. True saved_nans_checkpoint bool whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. False Returns: Type Description (str, bool) previous checkpointing string, with additional info if this function did checkpointing; followed by updated value of saved_nans_checkpoint. Source code in vmcnet/utils/checkpoint.py def save_metrics_and_regular_checkpoint ( epoch : int , old_params : P , new_params : P , optimizer_state : S , data : D , key : PRNGKey , metrics : Dict , logdir : str , checkpoint_writer : CheckpointWriter , metrics_writer : MetricsWriter , checkpoint_dir : str , checkpoint_str : str , checkpoint_every : int = None , checkpoint_if_nans : bool = False , only_checkpoint_first_nans : bool = True , saved_nans_checkpoint : bool = False , ) -> Tuple [ str , bool ]: \"\"\"Save current metrics to file, and save model state regularly. This currently touches the disk repeatedly, once for each metric, which is probably fairly inefficient, especially if called every epoch (as it currently is in :func:`~vmcnet.train.vmc.vmc_loop`). Args: epoch (int): current epoch number old_params (pytree-like): model parameters, from before the update function. Needs to be serializable via `np.savez`. new_params (pytree-like): model parameters, from after the update function. optimizer_state (pytree-like): running state of the optimizer other than the trainable parameters. Needs to be serialiable via `np.savez` data (pytree-like): current mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` metrics (dict): dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func:`utils.io.write_metric_to_file`. checkpoint_str (str): string indicating whether checkpointing has previously occurred logdir (str): name of parent log directory. If None, no checkpointing is done. Defaults to None. checkpoint_dir (str): name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". checkpoint_every (int, optional): how often to regularly save checkpoints. If None, this function doesn't save the model state. Defaults to None. checkpoint_if_nans (bool, optional): whether to save checkpoints when nan energy values are recorded. Defaults to False. only_checkpoint_first_nans (bool, optional): whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. saved_nans_checkpoint (bool, optional): whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. Returns: (str, bool): previous checkpointing string, with additional info if this function did checkpointing; followed by updated value of saved_nans_checkpoint. \"\"\" metrics_writer . save_data ( logdir , \"\" , metrics ) checkpoint_data = ( epoch , data , old_params , optimizer_state , key ) if checkpoint_every is not None : if ( epoch + 1 ) % checkpoint_every == 0 : checkpoint_writer . save_data ( os . path . join ( logdir , checkpoint_dir ), str ( epoch + 1 ) + \".npz\" , checkpoint_data , ) checkpoint_str = checkpoint_str + \", regular ckpt saved\" save_nans_checkpoint = _should_save_nans_checkpoint ( metrics , new_params , checkpoint_if_nans , only_checkpoint_first_nans , saved_nans_checkpoint , ) if save_nans_checkpoint : checkpoint_writer . save_data ( os . path . join ( logdir , checkpoint_dir ), \"nans_\" + str ( epoch + 1 ) + \".npz\" , checkpoint_data , ) checkpoint_str = checkpoint_str + \", nans ckpt saved\" saved_nans_checkpoint = True return checkpoint_str , saved_nans_checkpoint log_vmc_loop_state ( epoch , metrics , checkpoint_str ) Log current energy, variance, and accept ratio, w/ optional unclipped values. Source code in vmcnet/utils/checkpoint.py def log_vmc_loop_state ( epoch : int , metrics : Dict , checkpoint_str : str ) -> None : \"\"\"Log current energy, variance, and accept ratio, w/ optional unclipped values.\"\"\" epoch_str = \"Epoch %(epoch)5d \" energy_str = \"Energy: %(energy).5e \" variance_str = \"Variance: %(variance).5e \" accept_ratio_str = \"Accept ratio: %(accept_ratio).5f \" amplitude_str = \"\" if \"energy_noclip\" in metrics : energy_str = energy_str + \" ( %(energy_noclip).5e )\" if \"variance_noclip\" in metrics : variance_str = variance_str + \" ( %(variance_noclip).5e )\" if \"amplitude_min\" in metrics : amplitude_str = \"Min/max amplitude: %(amplitude_min).2f / %(amplitude_max).2f \" info_out = \", \" . join ( [ epoch_str , energy_str , variance_str , accept_ratio_str , amplitude_str ] ) info_out = info_out + checkpoint_str logged_metrics = { \"epoch\" : epoch + 1 } logged_metrics . update ( metrics ) logging . info ( info_out , logged_metrics )","title":"checkpoint"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.CheckpointWriter","text":"A ThreadedWriter for saving checkpoints during training.","title":"CheckpointWriter"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.CheckpointWriter.write_out_data","text":"Save checkpoint data. Parameters: Name Type Description Default directory str directory in which to write the checkpoint required name str filename for the checkpoint required checkpoint_data CheckpointData checkpoint data which contains: epoch (int): epoch at which checkpoint is being saved data (pytree or Array): walker data to save params (pytree): model parameters to save optimizer_state (pytree): optimizer state to save key (PRNGKey): RNG key, used to reproduce exact behavior from checkpoint required Source code in vmcnet/utils/checkpoint.py def write_out_data ( self , directory : str , name : str , checkpoint_data : CheckpointData ): \"\"\"Save checkpoint data. Args: directory (str): directory in which to write the checkpoint name (str): filename for the checkpoint checkpoint_data (CheckpointData): checkpoint data which contains: epoch (int): epoch at which checkpoint is being saved data (pytree or Array): walker data to save params (pytree): model parameters to save optimizer_state (pytree): optimizer state to save key (PRNGKey): RNG key, used to reproduce exact behavior from checkpoint \"\"\" io . save_vmc_state ( directory , name , checkpoint_data )","title":"write_out_data()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.CheckpointWriter.save_data","text":"Queue up checkpoint data to be written to disc. Source code in vmcnet/utils/checkpoint.py def save_data ( self , directory : str , name : str , checkpoint_data : CheckpointData ): \"\"\"Queue up checkpoint data to be written to disc.\"\"\" checkpoint_data = io . process_checkpoint_data_for_saving ( checkpoint_data ) # Move data to CPU to avoid clogging up GPU memory with queued checkpoints checkpoint_data = jax . device_put ( checkpoint_data , jax . devices ( \"cpu\" )[ 0 ]) super () . save_data ( directory , name , checkpoint_data )","title":"save_data()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.MetricsWriter","text":"A ThreadedWriter for saving metrics during training.","title":"MetricsWriter"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.MetricsWriter.write_out_data","text":"Save metrics to individual text files. Source code in vmcnet/utils/checkpoint.py def write_out_data ( self , directory : str , name : str , metrics : Dict ): \"\"\"Save metrics to individual text files.\"\"\" del name # unused, each metric gets its own file for metric , metric_val in metrics . items (): io . append_metric_to_file ( metric_val , directory , metric )","title":"write_out_data()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.RunningEnergyVariance","text":"Running energy history and variance history, packaged together.","title":"RunningEnergyVariance"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.RunningEnergyVariance.__new__","text":"Create new instance of RunningEnergyVariance(energy, variance)","title":"__new__()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.RunningEnergyVariance.__repr__","text":"Return a nicely formatted representation string Source code in vmcnet/utils/checkpoint.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.RunningEnergyVariance.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in vmcnet/utils/checkpoint.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.RunningMetric","text":"Running history and average of a metric for checkpointing purposes. Attributes: Name Type Description nhistory_max int maximum length of the running history to keep when adding new values avg jnp.float32 the running average, should be equal to jnp.mean(self.history). Stored here to avoid recomputation when new values are added history deque[jnp.float32] the running history of the metric","title":"RunningMetric"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.RunningMetric.move_history_window","text":"Append new value to running history, remove oldest if length > nhistory_max. Parameters: Name Type Description Default new_value jnp.float32 new value to insert into the history required Source code in vmcnet/utils/checkpoint.py def move_history_window ( self , new_value : jnp . float32 ): \"\"\"Append new value to running history, remove oldest if length > nhistory_max. Args: new_value (jnp.float32): new value to insert into the history \"\"\" if self . nhistory_max <= 0 : return history_length = len ( self . history ) self . history . append ( new_value ) self_sum = history_length * self . avg self_sum += new_value history_length += 1 if history_length >= self . nhistory_max : oldest_value = self . history . popleft () self_sum -= oldest_value history_length -= 1 self . avg = self_sum / history_length","title":"move_history_window()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter","text":"A simple asynchronous writer to handle file io during training. Spins up a thread for the file IO so that it does not block the main line of the training procedure. While Python threads do not provide true parallelism of CPU computations across cores, they do allow us to write to files and run Jax computations simultaneously.","title":"ThreadedWriter"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter.__init__","text":"Create a new ThreadedWriter. Source code in vmcnet/utils/checkpoint.py def __init__ ( self ): \"\"\"Create a new ThreadedWriter.\"\"\" self . _thread = threading . Thread ( target = self . _run_thread ) self . _done = False self . _queue = queue . Queue ()","title":"__init__()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter.write_out_data","text":"Abstract method which saves a piece of data pulled from the queue. Parameters: Name Type Description Default directory str directory in which to write the checkpoint required name str filename for the checkpoint required data_to_save Any data to save required Source code in vmcnet/utils/checkpoint.py @abstractmethod def write_out_data ( self , directory : str , name : str , data_to_save : T ): \"\"\"Abstract method which saves a piece of data pulled from the queue. Args: directory (str): directory in which to write the checkpoint name (str): filename for the checkpoint data_to_save (Any): data to save \"\"\" pass","title":"write_out_data()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter.initialize","text":"Initialize the ThreadedWriter by starting its internal thread. Source code in vmcnet/utils/checkpoint.py def initialize ( self ): \"\"\"Initialize the ThreadedWriter by starting its internal thread.\"\"\" self . _thread . start ()","title":"initialize()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter.save_data","text":"Queue up data to be written to disc. Source code in vmcnet/utils/checkpoint.py def save_data ( self , directory : str , name : str , data_to_save : T ): \"\"\"Queue up data to be written to disc.\"\"\" self . _queue . put (( directory , name , data_to_save ))","title":"save_data()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter.close_and_await","text":"Stop the thread by setting a flag, and return once it gets the message. Source code in vmcnet/utils/checkpoint.py def close_and_await ( self ): \"\"\"Stop the thread by setting a flag, and return once it gets the message.\"\"\" self . _done = True self . _thread . join ()","title":"close_and_await()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter.__enter__","text":"Enter a ThreadedWriter's context, starting up a thread. Source code in vmcnet/utils/checkpoint.py def __enter__ ( self ): \"\"\"Enter a ThreadedWriter's context, starting up a thread.\"\"\" self . initialize () return self","title":"__enter__()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.ThreadedWriter.__exit__","text":"Wait for the thread to finish, then leave the ThreadedWriter's context. Source code in vmcnet/utils/checkpoint.py def __exit__ ( self , exc_type , exc_value , traceback ): \"\"\"Wait for the thread to finish, then leave the ThreadedWriter's context.\"\"\" self . close_and_await ()","title":"__exit__()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.initialize_checkpointing","text":"Initialize checkpointing objects. A suffix is added to the checkpointing directory if one with the same name already exists in the logdir. The checkpointing metric (error-adjusted running energy average) is initialized to infinity, and empty arrays are initialized in running_energy_and_variance. The best checkpoint data is initialized to None, and saved_nan_checkpoint is initialized to False. Source code in vmcnet/utils/checkpoint.py def initialize_checkpointing ( checkpoint_dir : str , nhistory_max : int , logdir : str = None , checkpoint_every : int = None , ) -> Tuple [ str , jnp . float32 , RunningEnergyVariance , Optional [ CheckpointData ], bool ]: \"\"\"Initialize checkpointing objects. A suffix is added to the checkpointing directory if one with the same name already exists in the logdir. The checkpointing metric (error-adjusted running energy average) is initialized to infinity, and empty arrays are initialized in running_energy_and_variance. The best checkpoint data is initialized to None, and saved_nan_checkpoint is initialized to False. \"\"\" if logdir is not None : logging . info ( \"Saving to %s \" , logdir ) os . makedirs ( logdir , exist_ok = True ) if checkpoint_every is not None : checkpoint_dir = io . add_suffix_for_uniqueness ( checkpoint_dir , logdir ) os . makedirs ( os . path . join ( logdir , checkpoint_dir ), exist_ok = False ) checkpoint_metric = jnp . inf running_energy_and_variance = RunningEnergyVariance ( RunningMetric ( nhistory_max ), RunningMetric ( nhistory_max ) ) best_checkpoint_data = None saved_nan_checkpoint = False return ( checkpoint_dir , checkpoint_metric , running_energy_and_variance , best_checkpoint_data , saved_nan_checkpoint , )","title":"initialize_checkpointing()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.finish_checkpointing","text":"Save any final checkpoint data to the CheckpointWriter. Source code in vmcnet/utils/checkpoint.py def finish_checkpointing ( checkpoint_writer : CheckpointWriter , best_checkpoint_data : CheckpointData = None , logdir : str = None , ): \"\"\"Save any final checkpoint data to the CheckpointWriter.\"\"\" if logdir is not None and best_checkpoint_data is not None : checkpoint_writer . save_data ( logdir , CHECKPOINT_FILE_NAME , best_checkpoint_data )","title":"finish_checkpointing()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.get_checkpoint_metric","text":"Get an error-adjusted running average of the energy for checkpointing. The parameter variance_scale can be tuned and probably should scale linearly with some estimate of the integrated autocorrelation. Higher means more allergic to high variance, lower means more allergic to high energies. Parameters: Name Type Description Default energy_running_avg jnp.float32 running average of the energy required variance_running_avg jnp.float32 running average of the variance required nsamples int total number of samples reflected in the running averages, equal to the number of parallel chains times the length of the history required variance_scale float weight of the variance part of the checkpointing metric. The final effect on the variance part is to scale it by jnp.sqrt(variance_scale), i.e. to treat it like the integrated autocorrelation. required Returns: Type Description jnp.float32 error adjusted running average of the energy Source code in vmcnet/utils/checkpoint.py def get_checkpoint_metric ( energy_running_avg : jnp . float32 , variance_running_avg : jnp . float32 , nsamples : int , variance_scale : float , ) -> jnp . float32 : \"\"\"Get an error-adjusted running average of the energy for checkpointing. The parameter variance_scale can be tuned and probably should scale linearly with some estimate of the integrated autocorrelation. Higher means more allergic to high variance, lower means more allergic to high energies. Args: energy_running_avg (jnp.float32): running average of the energy variance_running_avg (jnp.float32): running average of the variance nsamples (int): total number of samples reflected in the running averages, equal to the number of parallel chains times the length of the history variance_scale (float): weight of the variance part of the checkpointing metric. The final effect on the variance part is to scale it by jnp.sqrt(variance_scale), i.e. to treat it like the integrated autocorrelation. Returns: jnp.float32: error adjusted running average of the energy \"\"\" # TODO(Jeffmin): eventually maybe put in some cheap best guess at the IAC? if variance_scale <= 0.0 or nsamples <= 0 : return energy_running_avg effective_nsamples = nsamples / variance_scale return energy_running_avg + jnp . sqrt ( variance_running_avg / effective_nsamples )","title":"get_checkpoint_metric()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.save_metrics_and_handle_checkpoints","text":"Checkpoint the current state of the VMC loop. There are two situations to checkpoint: 1) Regularly, every x epochs, to handle job preemption and track parameters/metrics/state over time, and 2) Whenever a checkpoint metric improves, i.e. the error adjusted running average of the energy. This is not a pure function, as it modifies the running energy and variance history. Parameters: Name Type Description Default epoch int current epoch number required old_params pytree-like model parameters, from before the update function. Needs to be serializable via np.savez . required new_params pytree-like model parameters, from after the update function. required optimizer_state pytree-like running state of the optimizer other than the trainable parameters. Needs to be serialiable via np.savez required old_data pytree-like previous mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required new_data pytree-like new mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required metrics dict dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func: utils.io.write_metric_to_file . required nchains int number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. required running_energy_and_variance RunningEnergyVariance running history of energies and variances required checkpoint_metric jnp.float32 current best error adjusted running average of the energy history required best_checkpoint_every int limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of best_checkpoint_every , we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. None logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. None variance_scale float scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func: ~vmctrain.train.vmc.get_checkpoint_metric . Defaults to 10.0. 10.0 checkpoint_every int how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to None. None best_checkpoint_data CheckpointData the data needed to save a checkpoint for the best energy observed so far. None checkpoint_dir str name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". 'checkpoints' checkpoint_if_nans bool whether to save checkpoints when nan energy values are recorded. Defaults to False. False only_checkpoint_first_nans bool whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. True saved_nans_checkpoint bool whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. False Returns: Type Description (jnp.float32, str, CheckpointData, bool) best error-adjusted energy average, then string indicating if checkpointing has been done, then new best checkpoint data (or None), then the updated value of saved_nans_checkpoint. Source code in vmcnet/utils/checkpoint.py def save_metrics_and_handle_checkpoints ( epoch : int , old_params : P , new_params : P , optimizer_state : S , old_data : D , new_data : D , key : PRNGKey , metrics : Dict , nchains : int , running_energy_and_variance : RunningEnergyVariance , checkpoint_writer : CheckpointWriter , metrics_writer : MetricsWriter , checkpoint_metric : jnp . float32 , logdir : Optional [ str ] = None , variance_scale : float = 10.0 , checkpoint_every : Optional [ int ] = None , best_checkpoint_every : Optional [ int ] = None , best_checkpoint_data : Optional [ CheckpointData [ D , P , S ]] = None , checkpoint_dir : str = \"checkpoints\" , checkpoint_if_nans : bool = False , only_checkpoint_first_nans : bool = True , saved_nans_checkpoint : bool = False , record_amplitudes : bool = False , get_amplitude_fn : Optional [ GetAmplitudeFromData [ D ]] = None , ) -> Tuple [ jnp . float32 , str , Optional [ CheckpointData [ D , P , S ]], bool ]: \"\"\"Checkpoint the current state of the VMC loop. There are two situations to checkpoint: 1) Regularly, every x epochs, to handle job preemption and track parameters/metrics/state over time, and 2) Whenever a checkpoint metric improves, i.e. the error adjusted running average of the energy. This is not a pure function, as it modifies the running energy and variance history. Args: epoch (int): current epoch number old_params (pytree-like): model parameters, from before the update function. Needs to be serializable via `np.savez`. new_params (pytree-like): model parameters, from after the update function. optimizer_state (pytree-like): running state of the optimizer other than the trainable parameters. Needs to be serialiable via `np.savez` old_data (pytree-like): previous mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` new_data (pytree-like): new mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` metrics (dict): dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func:`utils.io.write_metric_to_file`. nchains (int): number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. running_energy_and_variance (RunningEnergyVariance): running history of energies and variances checkpoint_metric (jnp.float32): current best error adjusted running average of the energy history best_checkpoint_every (int): limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of `best_checkpoint_every`, we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. logdir (str, optional): name of parent log directory. If None, no checkpointing is done. Defaults to None. variance_scale (float, optional): scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func:`~vmctrain.train.vmc.get_checkpoint_metric`. Defaults to 10.0. checkpoint_every (int, optional): how often to regularly save checkpoints. If None, checkpoints are only saved when the error-adjusted running avg of the energy improves. Defaults to None. best_checkpoint_data (CheckpointData, optional): the data needed to save a checkpoint for the best energy observed so far. checkpoint_dir (str, optional): name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". checkpoint_if_nans (bool, optional): whether to save checkpoints when nan energy values are recorded. Defaults to False. only_checkpoint_first_nans (bool, optional): whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. saved_nans_checkpoint (bool, optional): whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. Returns: (jnp.float32, str, CheckpointData, bool): best error-adjusted energy average, then string indicating if checkpointing has been done, then new best checkpoint data (or None), then the updated value of saved_nans_checkpoint. \"\"\" checkpoint_str = \"\" if logdir is None or metrics is None : # do nothing return ( checkpoint_metric , checkpoint_str , best_checkpoint_data , saved_nans_checkpoint , ) _add_amplitude_to_metrics_if_requested ( metrics , new_data , record_amplitudes , get_amplitude_fn ) checkpoint_str , saved_nans_checkpoint = save_metrics_and_regular_checkpoint ( epoch , old_params , new_params , optimizer_state , old_data , key , metrics , logdir , checkpoint_writer , metrics_writer , checkpoint_dir , checkpoint_str , checkpoint_every , checkpoint_if_nans = checkpoint_if_nans , only_checkpoint_first_nans = only_checkpoint_first_nans , saved_nans_checkpoint = saved_nans_checkpoint , ) ( checkpoint_str , error_adjusted_running_avg , new_best_checkpoint_data , ) = track_and_save_best_checkpoint ( epoch , old_params , optimizer_state , old_data , key , metrics , nchains , running_energy_and_variance , checkpoint_writer , checkpoint_metric , logdir , variance_scale , checkpoint_str , best_checkpoint_every , best_checkpoint_data , ) return ( jnp . minimum ( error_adjusted_running_avg , checkpoint_metric ), checkpoint_str , new_best_checkpoint_data , saved_nans_checkpoint , )","title":"save_metrics_and_handle_checkpoints()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.track_and_save_best_checkpoint","text":"Update running avgs and checkpoint if the error-adjusted energy avg improves. Parameters: Name Type Description Default epoch int current epoch number required old_params pytree-like model parameters, from before the update function. Needs to be serializable via np.savez . required optimizer_state pytree-like running state of the optimizer other than the trainable parameters. Needs to be serialiable via np.savez required data pytree-like current mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required metrics dict dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func: utils.io.write_metric_to_file . required nchains int number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. required running_energy_and_variance RunningEnergyVariance running history of energies and variances required checkpoint_metric jnp.float32 current best error adjusted running average of the energy history required logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. required variance_scale float scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func: ~vmctrain.train.vmc.get_checkpoint_metric . required checkpoint_str str string indicating whether checkpointing has previously occurred required best_checkpoint_every int limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of best_checkpoint_every , we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. None best_checkpoint_data CheckpointData the data needed to save a checkpoint for the best energy observed so far. None Returns: Type Description (str, jnp.float32, CheckpointData) previous checkpointing string with additional info if this function did checkpointing, then best error-adjusted energy average, then new best checkpoint data, or None. Source code in vmcnet/utils/checkpoint.py def track_and_save_best_checkpoint ( epoch : int , old_params : P , optimizer_state : S , data : D , key : PRNGKey , metrics : Dict , nchains : int , running_energy_and_variance : RunningEnergyVariance , checkpoint_writer : CheckpointWriter , checkpoint_metric : jnp . float32 , logdir : str , variance_scale : float , checkpoint_str : str , best_checkpoint_every : Optional [ int ] = None , best_checkpoint_data : Optional [ CheckpointData [ D , P , S ]] = None , ) -> Tuple [ str , jnp . float32 , Optional [ CheckpointData [ D , P , S ]]]: \"\"\"Update running avgs and checkpoint if the error-adjusted energy avg improves. Args: epoch (int): current epoch number old_params (pytree-like): model parameters, from before the update function. Needs to be serializable via `np.savez`. optimizer_state (pytree-like): running state of the optimizer other than the trainable parameters. Needs to be serialiable via `np.savez` data (pytree-like): current mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` metrics (dict): dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func:`utils.io.write_metric_to_file`. nchains (int): number of parallel MCMC chains being run. This can be difficult to infer from data, depending on the structure of data, whether data has been pmapped, etc. running_energy_and_variance (RunningEnergyVariance): running history of energies and variances checkpoint_metric (jnp.float32): current best error adjusted running average of the energy history logdir (str): name of parent log directory. If None, no checkpointing is done. Defaults to None. variance_scale (float): scale of the variance term in the error-adjusted running avg of the energy. Higher means the variance is more important, and lower means the energy is more important. See :func:`~vmctrain.train.vmc.get_checkpoint_metric`. checkpoint_str (str): string indicating whether checkpointing has previously occurred best_checkpoint_every (int, optional): limit on how often to save best checkpoint, even if energy is improving. When the error-adjusted running avg of the energy improves, instead of immediately saving a checkpoint, we hold onto the data from that epoch in memory, and if it's still the best one when we hit an epoch which is a multiple of `best_checkpoint_every`, we save it then. This ensures we don't waste time saving best checkpoints too often when the energy is on a downward trajectory (as we hope it often is!). Defaults to 100. best_checkpoint_data (CheckpointData, optional): the data needed to save a checkpoint for the best energy observed so far. Returns: (str, jnp.float32, CheckpointData): previous checkpointing string with additional info if this function did checkpointing, then best error-adjusted energy average, then new best checkpoint data, or None. \"\"\" if best_checkpoint_every is not None : energy , variance = running_energy_and_variance energy . move_history_window ( metrics [ \"energy\" ]) variance . move_history_window ( metrics [ \"variance\" ]) error_adjusted_running_avg = get_checkpoint_metric ( energy . avg , variance . avg , nchains * len ( energy . history ), variance_scale ) if error_adjusted_running_avg < checkpoint_metric : best_checkpoint_data = ( epoch , data , old_params , optimizer_state , key , ) should_save_best_checkpoint = ( epoch + 1 ) % best_checkpoint_every == 0 if should_save_best_checkpoint and best_checkpoint_data is not None : checkpoint_writer . save_data ( logdir , CHECKPOINT_FILE_NAME , best_checkpoint_data ) checkpoint_str = checkpoint_str + \", best weights saved\" best_checkpoint_data = None else : error_adjusted_running_avg = checkpoint_metric return checkpoint_str , error_adjusted_running_avg , best_checkpoint_data","title":"track_and_save_best_checkpoint()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.save_metrics_and_regular_checkpoint","text":"Save current metrics to file, and save model state regularly. This currently touches the disk repeatedly, once for each metric, which is probably fairly inefficient, especially if called every epoch (as it currently is in :func: ~vmcnet.train.vmc.vmc_loop ). Parameters: Name Type Description Default epoch int current epoch number required old_params pytree-like model parameters, from before the update function. Needs to be serializable via np.savez . required new_params pytree-like model parameters, from after the update function. required optimizer_state pytree-like running state of the optimizer other than the trainable parameters. Needs to be serialiable via np.savez required data pytree-like current mcmc data (e.g. position and amplitude data). Needs to be serializable via np.savez required metrics dict dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func: utils.io.write_metric_to_file . required checkpoint_str str string indicating whether checkpointing has previously occurred required logdir str name of parent log directory. If None, no checkpointing is done. Defaults to None. required checkpoint_dir str name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". required checkpoint_every int how often to regularly save checkpoints. If None, this function doesn't save the model state. Defaults to None. None checkpoint_if_nans bool whether to save checkpoints when nan energy values are recorded. Defaults to False. False only_checkpoint_first_nans bool whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. True saved_nans_checkpoint bool whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. False Returns: Type Description (str, bool) previous checkpointing string, with additional info if this function did checkpointing; followed by updated value of saved_nans_checkpoint. Source code in vmcnet/utils/checkpoint.py def save_metrics_and_regular_checkpoint ( epoch : int , old_params : P , new_params : P , optimizer_state : S , data : D , key : PRNGKey , metrics : Dict , logdir : str , checkpoint_writer : CheckpointWriter , metrics_writer : MetricsWriter , checkpoint_dir : str , checkpoint_str : str , checkpoint_every : int = None , checkpoint_if_nans : bool = False , only_checkpoint_first_nans : bool = True , saved_nans_checkpoint : bool = False , ) -> Tuple [ str , bool ]: \"\"\"Save current metrics to file, and save model state regularly. This currently touches the disk repeatedly, once for each metric, which is probably fairly inefficient, especially if called every epoch (as it currently is in :func:`~vmcnet.train.vmc.vmc_loop`). Args: epoch (int): current epoch number old_params (pytree-like): model parameters, from before the update function. Needs to be serializable via `np.savez`. new_params (pytree-like): model parameters, from after the update function. optimizer_state (pytree-like): running state of the optimizer other than the trainable parameters. Needs to be serialiable via `np.savez` data (pytree-like): current mcmc data (e.g. position and amplitude data). Needs to be serializable via `np.savez` metrics (dict): dictionary of metrics. If this is not None, then it must include \"energy\" and \"variance\". Metrics are currently flattened and written to a row of a text file. See :func:`utils.io.write_metric_to_file`. checkpoint_str (str): string indicating whether checkpointing has previously occurred logdir (str): name of parent log directory. If None, no checkpointing is done. Defaults to None. checkpoint_dir (str): name of subdirectory to save the regular checkpoints. These are saved as \"logdir/checkpoint_dir/(epoch + 1).npz\". Defaults to \"checkpoints\". checkpoint_every (int, optional): how often to regularly save checkpoints. If None, this function doesn't save the model state. Defaults to None. checkpoint_if_nans (bool, optional): whether to save checkpoints when nan energy values are recorded. Defaults to False. only_checkpoint_first_nans (bool, optional): whether to checkpoint only the first time nans are encountered, or every time. Useful to capture a nan checkpoint without risking writing too many checkpoints if the optimization starts to hit nans most or every epoch after some point. Only relevant if checkpoint_if_nans is True. Defaults to True. saved_nans_checkpoint (bool, optional): whether a nans checkpoint has already been saved. Only relevant if checkpoint_if_nans and only_checkpoint_first_nans are both True, and used in that case to decide whether to save further nans checkpoints or not. Defaults to False. Returns: (str, bool): previous checkpointing string, with additional info if this function did checkpointing; followed by updated value of saved_nans_checkpoint. \"\"\" metrics_writer . save_data ( logdir , \"\" , metrics ) checkpoint_data = ( epoch , data , old_params , optimizer_state , key ) if checkpoint_every is not None : if ( epoch + 1 ) % checkpoint_every == 0 : checkpoint_writer . save_data ( os . path . join ( logdir , checkpoint_dir ), str ( epoch + 1 ) + \".npz\" , checkpoint_data , ) checkpoint_str = checkpoint_str + \", regular ckpt saved\" save_nans_checkpoint = _should_save_nans_checkpoint ( metrics , new_params , checkpoint_if_nans , only_checkpoint_first_nans , saved_nans_checkpoint , ) if save_nans_checkpoint : checkpoint_writer . save_data ( os . path . join ( logdir , checkpoint_dir ), \"nans_\" + str ( epoch + 1 ) + \".npz\" , checkpoint_data , ) checkpoint_str = checkpoint_str + \", nans ckpt saved\" saved_nans_checkpoint = True return checkpoint_str , saved_nans_checkpoint","title":"save_metrics_and_regular_checkpoint()"},{"location":"api/utils/checkpoint/#vmcnet.utils.checkpoint.log_vmc_loop_state","text":"Log current energy, variance, and accept ratio, w/ optional unclipped values. Source code in vmcnet/utils/checkpoint.py def log_vmc_loop_state ( epoch : int , metrics : Dict , checkpoint_str : str ) -> None : \"\"\"Log current energy, variance, and accept ratio, w/ optional unclipped values.\"\"\" epoch_str = \"Epoch %(epoch)5d \" energy_str = \"Energy: %(energy).5e \" variance_str = \"Variance: %(variance).5e \" accept_ratio_str = \"Accept ratio: %(accept_ratio).5f \" amplitude_str = \"\" if \"energy_noclip\" in metrics : energy_str = energy_str + \" ( %(energy_noclip).5e )\" if \"variance_noclip\" in metrics : variance_str = variance_str + \" ( %(variance_noclip).5e )\" if \"amplitude_min\" in metrics : amplitude_str = \"Min/max amplitude: %(amplitude_min).2f / %(amplitude_max).2f \" info_out = \", \" . join ( [ epoch_str , energy_str , variance_str , accept_ratio_str , amplitude_str ] ) info_out = info_out + checkpoint_str logged_metrics = { \"epoch\" : epoch + 1 } logged_metrics . update ( metrics ) logging . info ( info_out , logged_metrics )","title":"log_vmc_loop_state()"},{"location":"api/utils/distribute/","text":"Helper functions for distributing computation to multiple devices. wrap_if_pmap ( p_func ) Make a function run if in a pmapped context. Source code in vmcnet/utils/distribute.py def wrap_if_pmap ( p_func : Callable ) -> Callable : \"\"\"Make a function run if in a pmapped context.\"\"\" def p_func_if_pmap ( obj , axis_name ): try : core . axis_frame ( axis_name ) return p_func ( obj , axis_name ) except NameError : return obj return p_func_if_pmap replicate_all_local_devices ( obj ) Replicate a pytree on all local devices. Source code in vmcnet/utils/distribute.py def replicate_all_local_devices ( obj : T ) -> T : \"\"\"Replicate a pytree on all local devices.\"\"\" if obj is None : return None n = jax . local_device_count () obj_stacked = jax . tree_map ( lambda x : jnp . stack ([ x ] * n , axis = 0 ), obj ) return broadcast_all_local_devices ( obj_stacked ) make_different_rng_key_on_all_devices ( rng ) Split a PRNG key to all local devices. Source code in vmcnet/utils/distribute.py def make_different_rng_key_on_all_devices ( rng : PRNGKey ) -> PRNGKey : \"\"\"Split a PRNG key to all local devices.\"\"\" rng = jax . random . fold_in ( rng , jax . process_index ()) rng = jax . random . split ( rng , jax . local_device_count ()) return broadcast_all_local_devices ( rng ) get_first ( obj ) Get the first object in each leaf of a pytree. Can be used to grab the first instance of a replicated object on the first local device. Source code in vmcnet/utils/distribute.py def get_first ( obj : T ) -> T : \"\"\"Get the first object in each leaf of a pytree. Can be used to grab the first instance of a replicated object on the first local device. \"\"\" return jax . tree_map ( lambda x : x [ 0 ], obj ) mean_all_local_devices ( x ) Compute mean over all local devices if distributed, otherwise the usual mean. Source code in vmcnet/utils/distribute.py def mean_all_local_devices ( x : Array ) -> jnp . float32 : \"\"\"Compute mean over all local devices if distributed, otherwise the usual mean.\"\"\" return pmean_if_pmap ( jnp . mean ( x )) nanmean_all_local_devices ( x ) Compute a nan-safe mean over all local devices. Source code in vmcnet/utils/distribute.py def nanmean_all_local_devices ( x : Array ) -> jnp . float32 : \"\"\"Compute a nan-safe mean over all local devices.\"\"\" return pmean_if_pmap ( jnp . nanmean ( x )) get_mean_over_first_axis_fn ( nan_safe = True ) Get a function which averages over the first axis over all local devices. Parameters: Name Type Description Default nan_safe bool whether to use jnp.nanmean or jnp.mean in the local average computation. Defaults to True. True Returns: Type Description Callable function which averages an array over its first axis over all local devices. Source code in vmcnet/utils/distribute.py def get_mean_over_first_axis_fn ( nan_safe : bool = True , ) -> Callable [[ Array ], Array ]: \"\"\"Get a function which averages over the first axis over all local devices. Args: nan_safe (bool, optional): whether to use jnp.nanmean or jnp.mean in the local average computation. Defaults to True. Returns: Callable: function which averages an array over its first axis over all local devices. \"\"\" if nan_safe : local_mean_fn = functools . partial ( jnp . nanmean , axis = 0 ) else : local_mean_fn = functools . partial ( jnp . mean , axis = 0 ) def mean_fn ( x : Array ) -> Array : return pmean_if_pmap ( local_mean_fn ( x )) return mean_fn split_or_psplit_key ( key , multi_device = True ) Split PRNG key, potentially on multiple devices. Source code in vmcnet/utils/distribute.py def split_or_psplit_key ( key : PRNGKey , multi_device : bool = True ) -> PRNGKey : \"\"\"Split PRNG key, potentially on multiple devices.\"\"\" return p_split ( key ) if multi_device else jax . random . split ( key ) reshape_data_leaves_for_distribution ( data_leaf ) For a leaf of a pytree, reshape it for distributing to all local devices. Source code in vmcnet/utils/distribute.py def reshape_data_leaves_for_distribution ( data_leaf : Array ) -> Array : \"\"\"For a leaf of a pytree, reshape it for distributing to all local devices.\"\"\" num_devices = jax . local_device_count () nchains = data_leaf . shape [ 0 ] if nchains % num_devices != 0 : raise ValueError ( \"Number of chains must be divisible by number of devices, \" \"got nchains {} for {} devices.\" . format ( nchains , num_devices ) ) distributed_data_shape = ( num_devices , nchains // num_devices ) data = jnp . reshape ( data_leaf , distributed_data_shape + data_leaf . shape [ 1 :]) return data default_distribute_data ( data ) Split all data to all devices. The first axis must be divisible by ndevices. Source code in vmcnet/utils/distribute.py def default_distribute_data ( data : D ) -> D : \"\"\"Split all data to all devices. The first axis must be divisible by ndevices.\"\"\" data = jax . tree_map ( reshape_data_leaves_for_distribution , data ) data = broadcast_all_local_devices ( data ) return data distribute_vmc_state ( data , params , optimizer_state , key , distribute_data_fn =< function default_distribute_data at 0x7ff172b38430 > ) Split data, replicate params and opt state, and split PRNG key to all devices. Parameters: Name Type Description Default data ~D the MCMC data to distribute required params ~P model parameters required optimizer_state ~S optimizer state required key PRNGKeyArray RNG key required distribute_data_fn Callable[[~D], ~D] custom function for distributing the MCMC data, for the case where some of the data needs to be replicated instead of distributed across the devices. Default works if there is no data that requires replication. <function default_distribute_data at 0x7ff172b38430> Returns: Type Description (D, P, S, PRNGKey) tuple of data, params, optimizer_state, and key, each of which has been either distributed or replicated across all devices, as appopriate. Source code in vmcnet/utils/distribute.py def distribute_vmc_state ( data : D , params : P , optimizer_state : S , key : PRNGKey , distribute_data_fn : Callable [[ D ], D ] = default_distribute_data , ) -> Tuple [ D , P , S , PRNGKey ]: \"\"\"Split data, replicate params and opt state, and split PRNG key to all devices. Args: data: the MCMC data to distribute params: model parameters optimizer_state: optimizer state key: RNG key distribute_data_fn: custom function for distributing the MCMC data, for the case where some of the data needs to be replicated instead of distributed across the devices. Default works if there is no data that requires replication. Returns: (D, P, S, PRNGKey): tuple of data, params, optimizer_state, and key, each of which has been either distributed or replicated across all devices, as appopriate. \"\"\" data = distribute_data_fn ( data ) params = replicate_all_local_devices ( params ) optimizer_state = replicate_all_local_devices ( optimizer_state ) sharded_key = make_different_rng_key_on_all_devices ( key ) return data , params , optimizer_state , sharded_key distribute_vmc_state_from_checkpoint ( data , params , optimizer_state , key ) Distribute vmc state that was reloaded from a saved checkpoint. Data and key are saved independently for each device, so on reload we simply broadcast them back to the devices. Params and optimizer state are saved as a single copy, so on reload we replicate them to all devices. Source code in vmcnet/utils/distribute.py def distribute_vmc_state_from_checkpoint ( data : D , params : P , optimizer_state : S , key : PRNGKey , ) -> Tuple [ D , P , S , PRNGKey ]: \"\"\"Distribute vmc state that was reloaded from a saved checkpoint. Data and key are saved independently for each device, so on reload we simply broadcast them back to the devices. Params and optimizer state are saved as a single copy, so on reload we replicate them to all devices. \"\"\" data = broadcast_all_local_devices ( data ) params = replicate_all_local_devices ( params ) optimizer_state = replicate_all_local_devices ( optimizer_state ) key = broadcast_all_local_devices ( key ) return data , params , optimizer_state , key is_distributed ( data ) Tests whether given data has been distributed using pmap. Source code in vmcnet/utils/distribute.py def is_distributed ( data : PyTree ) -> bool : \"\"\"Tests whether given data has been distributed using pmap.\"\"\" return isinstance ( jax . tree_leaves ( data )[ 0 ], pxla . ShardedDeviceArray ) get_first_if_distributed ( data ) Gets single copy of input data, which may or may not be replicated. Source code in vmcnet/utils/distribute.py def get_first_if_distributed ( data : PyTree ) -> PyTree : \"\"\"Gets single copy of input data, which may or may not be replicated.\"\"\" if is_distributed ( data ): return get_first ( data ) return data","title":"distribute"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.wrap_if_pmap","text":"Make a function run if in a pmapped context. Source code in vmcnet/utils/distribute.py def wrap_if_pmap ( p_func : Callable ) -> Callable : \"\"\"Make a function run if in a pmapped context.\"\"\" def p_func_if_pmap ( obj , axis_name ): try : core . axis_frame ( axis_name ) return p_func ( obj , axis_name ) except NameError : return obj return p_func_if_pmap","title":"wrap_if_pmap()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.replicate_all_local_devices","text":"Replicate a pytree on all local devices. Source code in vmcnet/utils/distribute.py def replicate_all_local_devices ( obj : T ) -> T : \"\"\"Replicate a pytree on all local devices.\"\"\" if obj is None : return None n = jax . local_device_count () obj_stacked = jax . tree_map ( lambda x : jnp . stack ([ x ] * n , axis = 0 ), obj ) return broadcast_all_local_devices ( obj_stacked )","title":"replicate_all_local_devices()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.make_different_rng_key_on_all_devices","text":"Split a PRNG key to all local devices. Source code in vmcnet/utils/distribute.py def make_different_rng_key_on_all_devices ( rng : PRNGKey ) -> PRNGKey : \"\"\"Split a PRNG key to all local devices.\"\"\" rng = jax . random . fold_in ( rng , jax . process_index ()) rng = jax . random . split ( rng , jax . local_device_count ()) return broadcast_all_local_devices ( rng )","title":"make_different_rng_key_on_all_devices()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.get_first","text":"Get the first object in each leaf of a pytree. Can be used to grab the first instance of a replicated object on the first local device. Source code in vmcnet/utils/distribute.py def get_first ( obj : T ) -> T : \"\"\"Get the first object in each leaf of a pytree. Can be used to grab the first instance of a replicated object on the first local device. \"\"\" return jax . tree_map ( lambda x : x [ 0 ], obj )","title":"get_first()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.mean_all_local_devices","text":"Compute mean over all local devices if distributed, otherwise the usual mean. Source code in vmcnet/utils/distribute.py def mean_all_local_devices ( x : Array ) -> jnp . float32 : \"\"\"Compute mean over all local devices if distributed, otherwise the usual mean.\"\"\" return pmean_if_pmap ( jnp . mean ( x ))","title":"mean_all_local_devices()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.nanmean_all_local_devices","text":"Compute a nan-safe mean over all local devices. Source code in vmcnet/utils/distribute.py def nanmean_all_local_devices ( x : Array ) -> jnp . float32 : \"\"\"Compute a nan-safe mean over all local devices.\"\"\" return pmean_if_pmap ( jnp . nanmean ( x ))","title":"nanmean_all_local_devices()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.get_mean_over_first_axis_fn","text":"Get a function which averages over the first axis over all local devices. Parameters: Name Type Description Default nan_safe bool whether to use jnp.nanmean or jnp.mean in the local average computation. Defaults to True. True Returns: Type Description Callable function which averages an array over its first axis over all local devices. Source code in vmcnet/utils/distribute.py def get_mean_over_first_axis_fn ( nan_safe : bool = True , ) -> Callable [[ Array ], Array ]: \"\"\"Get a function which averages over the first axis over all local devices. Args: nan_safe (bool, optional): whether to use jnp.nanmean or jnp.mean in the local average computation. Defaults to True. Returns: Callable: function which averages an array over its first axis over all local devices. \"\"\" if nan_safe : local_mean_fn = functools . partial ( jnp . nanmean , axis = 0 ) else : local_mean_fn = functools . partial ( jnp . mean , axis = 0 ) def mean_fn ( x : Array ) -> Array : return pmean_if_pmap ( local_mean_fn ( x )) return mean_fn","title":"get_mean_over_first_axis_fn()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.split_or_psplit_key","text":"Split PRNG key, potentially on multiple devices. Source code in vmcnet/utils/distribute.py def split_or_psplit_key ( key : PRNGKey , multi_device : bool = True ) -> PRNGKey : \"\"\"Split PRNG key, potentially on multiple devices.\"\"\" return p_split ( key ) if multi_device else jax . random . split ( key )","title":"split_or_psplit_key()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.reshape_data_leaves_for_distribution","text":"For a leaf of a pytree, reshape it for distributing to all local devices. Source code in vmcnet/utils/distribute.py def reshape_data_leaves_for_distribution ( data_leaf : Array ) -> Array : \"\"\"For a leaf of a pytree, reshape it for distributing to all local devices.\"\"\" num_devices = jax . local_device_count () nchains = data_leaf . shape [ 0 ] if nchains % num_devices != 0 : raise ValueError ( \"Number of chains must be divisible by number of devices, \" \"got nchains {} for {} devices.\" . format ( nchains , num_devices ) ) distributed_data_shape = ( num_devices , nchains // num_devices ) data = jnp . reshape ( data_leaf , distributed_data_shape + data_leaf . shape [ 1 :]) return data","title":"reshape_data_leaves_for_distribution()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.default_distribute_data","text":"Split all data to all devices. The first axis must be divisible by ndevices. Source code in vmcnet/utils/distribute.py def default_distribute_data ( data : D ) -> D : \"\"\"Split all data to all devices. The first axis must be divisible by ndevices.\"\"\" data = jax . tree_map ( reshape_data_leaves_for_distribution , data ) data = broadcast_all_local_devices ( data ) return data","title":"default_distribute_data()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.distribute_vmc_state","text":"Split data, replicate params and opt state, and split PRNG key to all devices. Parameters: Name Type Description Default data ~D the MCMC data to distribute required params ~P model parameters required optimizer_state ~S optimizer state required key PRNGKeyArray RNG key required distribute_data_fn Callable[[~D], ~D] custom function for distributing the MCMC data, for the case where some of the data needs to be replicated instead of distributed across the devices. Default works if there is no data that requires replication. <function default_distribute_data at 0x7ff172b38430> Returns: Type Description (D, P, S, PRNGKey) tuple of data, params, optimizer_state, and key, each of which has been either distributed or replicated across all devices, as appopriate. Source code in vmcnet/utils/distribute.py def distribute_vmc_state ( data : D , params : P , optimizer_state : S , key : PRNGKey , distribute_data_fn : Callable [[ D ], D ] = default_distribute_data , ) -> Tuple [ D , P , S , PRNGKey ]: \"\"\"Split data, replicate params and opt state, and split PRNG key to all devices. Args: data: the MCMC data to distribute params: model parameters optimizer_state: optimizer state key: RNG key distribute_data_fn: custom function for distributing the MCMC data, for the case where some of the data needs to be replicated instead of distributed across the devices. Default works if there is no data that requires replication. Returns: (D, P, S, PRNGKey): tuple of data, params, optimizer_state, and key, each of which has been either distributed or replicated across all devices, as appopriate. \"\"\" data = distribute_data_fn ( data ) params = replicate_all_local_devices ( params ) optimizer_state = replicate_all_local_devices ( optimizer_state ) sharded_key = make_different_rng_key_on_all_devices ( key ) return data , params , optimizer_state , sharded_key","title":"distribute_vmc_state()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.distribute_vmc_state_from_checkpoint","text":"Distribute vmc state that was reloaded from a saved checkpoint. Data and key are saved independently for each device, so on reload we simply broadcast them back to the devices. Params and optimizer state are saved as a single copy, so on reload we replicate them to all devices. Source code in vmcnet/utils/distribute.py def distribute_vmc_state_from_checkpoint ( data : D , params : P , optimizer_state : S , key : PRNGKey , ) -> Tuple [ D , P , S , PRNGKey ]: \"\"\"Distribute vmc state that was reloaded from a saved checkpoint. Data and key are saved independently for each device, so on reload we simply broadcast them back to the devices. Params and optimizer state are saved as a single copy, so on reload we replicate them to all devices. \"\"\" data = broadcast_all_local_devices ( data ) params = replicate_all_local_devices ( params ) optimizer_state = replicate_all_local_devices ( optimizer_state ) key = broadcast_all_local_devices ( key ) return data , params , optimizer_state , key","title":"distribute_vmc_state_from_checkpoint()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.is_distributed","text":"Tests whether given data has been distributed using pmap. Source code in vmcnet/utils/distribute.py def is_distributed ( data : PyTree ) -> bool : \"\"\"Tests whether given data has been distributed using pmap.\"\"\" return isinstance ( jax . tree_leaves ( data )[ 0 ], pxla . ShardedDeviceArray )","title":"is_distributed()"},{"location":"api/utils/distribute/#vmcnet.utils.distribute.get_first_if_distributed","text":"Gets single copy of input data, which may or may not be replicated. Source code in vmcnet/utils/distribute.py def get_first_if_distributed ( data : PyTree ) -> PyTree : \"\"\"Gets single copy of input data, which may or may not be replicated.\"\"\" if is_distributed ( data ): return get_first ( data ) return data","title":"get_first_if_distributed()"},{"location":"api/utils/kfac/","text":"Utils for interfacing with DeepMind's KFAC implementation. These functions are taken directly from DeepMind's FermiNet jax branch, see https://github.com/deepmind/ferminet/blob/aade61b3d30883b3238d6b50c85404d0e8176155/ferminet/curvature_tags_and_blocks.py Some names are slightly modified (e.g. repeated_dense -> batch_dense). BatchDenseBlock ( DenseTwoKroneckerFactored ) Dense curvature block corresponding to inputs of shape (..., d). compute_extra_scale ( self ) Extra scale factor for the curvature block (relative to other blocks). Source code in vmcnet/utils/kfac.py def compute_extra_scale ( self ) -> Array : \"\"\"Extra scale factor for the curvature block (relative to other blocks).\"\"\" ( x_shape ,) = self . inputs_shapes return kfac_utils . product ( x_shape ) // ( x_shape [ 0 ] * x_shape [ - 1 ]) update_curvature_matrix_estimate ( self , info , batch_size , ema_old , ema_new , pmap_axis_name ) Satsify kfac_ferminet_alpha's assumption that the inputs are 2d. The inputs are generally of shape (batch_size, ..., d), and the optimizer expects that the input batch_size matches the first axis of info[\"inputs\"] . However, the Dense layer itself is batch applied only to the very last axis, with all of the other axes acting as batch axes. Thus to compute the correct curvature matrix estimate, we reshape the inputs and outputs to be (-1, shape[-1]) and provide to the superclass implementation a new batch_size equal to the product of the batch axes sizes. Source code in vmcnet/utils/kfac.py def update_curvature_matrix_estimate ( self , info : Mapping [ str , blocks . _Arrays ], batch_size : int , ema_old : Union [ float , Array ], ema_new : Union [ float , Array ], pmap_axis_name : str , ) -> None : \"\"\"Satsify kfac_ferminet_alpha's assumption that the inputs are 2d. The inputs are generally of shape (batch_size, ..., d), and the optimizer expects that the input `batch_size` matches the first axis of `info[\"inputs\"]`. However, the Dense layer itself is batch applied only to the very last axis, with all of the other axes acting as batch axes. Thus to compute the correct curvature matrix estimate, we reshape the inputs and outputs to be (-1, shape[-1]) and provide to the superclass implementation a new `batch_size` equal to the product of the batch axes sizes. \"\"\" info = dict ( ** info ) ( x ,), ( dy ,) = info [ \"inputs\" ], info [ \"outputs_tangent\" ] assert x . shape [ 0 ] == batch_size info [ \"inputs\" ] = ( x . reshape ([ - 1 , x . shape [ - 1 ]]),) info [ \"outputs_tangent\" ] = ( dy . reshape ([ - 1 , dy . shape [ - 1 ]]),) super () . update_curvature_matrix_estimate ( info , x . size // x . shape [ - 1 ], ema_old , ema_new , pmap_axis_name ) register_batch_dense ( y , x , w , b ) Register the weights of a dense layer. The dense layer performs y = wx + b. Source code in vmcnet/utils/kfac.py def register_batch_dense ( y , x , w , b ): \"\"\"Register the weights of a dense layer. The dense layer performs y = wx + b. \"\"\" if b is None : return batch_dense_tag . bind ( y , x , w ) return batch_dense_tag . bind ( y , x , w , b )","title":"kfac"},{"location":"api/utils/kfac/#vmcnet.utils.kfac.BatchDenseBlock","text":"Dense curvature block corresponding to inputs of shape (..., d).","title":"BatchDenseBlock"},{"location":"api/utils/kfac/#vmcnet.utils.kfac.BatchDenseBlock.compute_extra_scale","text":"Extra scale factor for the curvature block (relative to other blocks). Source code in vmcnet/utils/kfac.py def compute_extra_scale ( self ) -> Array : \"\"\"Extra scale factor for the curvature block (relative to other blocks).\"\"\" ( x_shape ,) = self . inputs_shapes return kfac_utils . product ( x_shape ) // ( x_shape [ 0 ] * x_shape [ - 1 ])","title":"compute_extra_scale()"},{"location":"api/utils/kfac/#vmcnet.utils.kfac.BatchDenseBlock.update_curvature_matrix_estimate","text":"Satsify kfac_ferminet_alpha's assumption that the inputs are 2d. The inputs are generally of shape (batch_size, ..., d), and the optimizer expects that the input batch_size matches the first axis of info[\"inputs\"] . However, the Dense layer itself is batch applied only to the very last axis, with all of the other axes acting as batch axes. Thus to compute the correct curvature matrix estimate, we reshape the inputs and outputs to be (-1, shape[-1]) and provide to the superclass implementation a new batch_size equal to the product of the batch axes sizes. Source code in vmcnet/utils/kfac.py def update_curvature_matrix_estimate ( self , info : Mapping [ str , blocks . _Arrays ], batch_size : int , ema_old : Union [ float , Array ], ema_new : Union [ float , Array ], pmap_axis_name : str , ) -> None : \"\"\"Satsify kfac_ferminet_alpha's assumption that the inputs are 2d. The inputs are generally of shape (batch_size, ..., d), and the optimizer expects that the input `batch_size` matches the first axis of `info[\"inputs\"]`. However, the Dense layer itself is batch applied only to the very last axis, with all of the other axes acting as batch axes. Thus to compute the correct curvature matrix estimate, we reshape the inputs and outputs to be (-1, shape[-1]) and provide to the superclass implementation a new `batch_size` equal to the product of the batch axes sizes. \"\"\" info = dict ( ** info ) ( x ,), ( dy ,) = info [ \"inputs\" ], info [ \"outputs_tangent\" ] assert x . shape [ 0 ] == batch_size info [ \"inputs\" ] = ( x . reshape ([ - 1 , x . shape [ - 1 ]]),) info [ \"outputs_tangent\" ] = ( dy . reshape ([ - 1 , dy . shape [ - 1 ]]),) super () . update_curvature_matrix_estimate ( info , x . size // x . shape [ - 1 ], ema_old , ema_new , pmap_axis_name )","title":"update_curvature_matrix_estimate()"},{"location":"api/utils/kfac/#vmcnet.utils.kfac.register_batch_dense","text":"Register the weights of a dense layer. The dense layer performs y = wx + b. Source code in vmcnet/utils/kfac.py def register_batch_dense ( y , x , w , b ): \"\"\"Register the weights of a dense layer. The dense layer performs y = wx + b. \"\"\" if b is None : return batch_dense_tag . bind ( y , x , w ) return batch_dense_tag . bind ( y , x , w , b )","title":"register_batch_dense()"},{"location":"api/utils/log_linear_exp/","text":"Helper function for log sum exp trick with weights. log_linear_exp ( signs , vals , weights = None , axis = 0 , register_kfac = True ) Stably compute sign and log(abs(.)) of sum_i(sign_i * w_ij * exp(vals_i)) + b_j. In order to avoid overflow when computing log(abs(sum_i(sign_i * w_ij * exp(vals_i)))), the largest exp(val_i) is divided out from all the values and added back in after the outer log, i.e. log(abs(sum_i(sign_i * w_ij * exp(vals_i - max)))) + max. This trick also avoids the underflow issue of when all vals are small enough that exp(val_i) is approximately 0 for all i. Parameters: Name Type Description Default signs Array array of signs of the input x with shape (..., d, ...), where d is the size of the given axis required vals Array array of log|abs(x)| with shape (..., d, ...), where d is the size of the given axis required weights Array weights of a linear transformation to apply to the given axis, with shape (d, d'). If not provided, a simple sum is taken instead, equivalent to (d, 1) weights equal to 1. Defaults to None. None axis int axis along which to take the sum and max. Defaults to 0. 0 register_kfac bool if weights are not None, whether to register the linear part of the computation with KFAC. Defaults to True. True Returns: Type Description (SLArray) sign of linear combination, log of linear combination. Both outputs have shape (..., d', ...), where d' = 1 if weights is None, and d' = weights.shape[1] otherwise. Source code in vmcnet/utils/log_linear_exp.py def log_linear_exp ( signs : Array , vals : Array , weights : Optional [ Array ] = None , axis : int = 0 , register_kfac : bool = True , ) -> SLArray : \"\"\"Stably compute sign and log(abs(.)) of sum_i(sign_i * w_ij * exp(vals_i)) + b_j. In order to avoid overflow when computing log(abs(sum_i(sign_i * w_ij * exp(vals_i)))), the largest exp(val_i) is divided out from all the values and added back in after the outer log, i.e. log(abs(sum_i(sign_i * w_ij * exp(vals_i - max)))) + max. This trick also avoids the underflow issue of when all vals are small enough that exp(val_i) is approximately 0 for all i. Args: signs (Array): array of signs of the input x with shape (..., d, ...), where d is the size of the given axis vals (Array): array of log|abs(x)| with shape (..., d, ...), where d is the size of the given axis weights (Array, optional): weights of a linear transformation to apply to the given axis, with shape (d, d'). If not provided, a simple sum is taken instead, equivalent to (d, 1) weights equal to 1. Defaults to None. axis (int, optional): axis along which to take the sum and max. Defaults to 0. register_kfac (bool, optional): if weights are not None, whether to register the linear part of the computation with KFAC. Defaults to True. Returns: (SLArray): sign of linear combination, log of linear combination. Both outputs have shape (..., d', ...), where d' = 1 if weights is None, and d' = weights.shape[1] otherwise. \"\"\" max_val = jnp . max ( vals , axis = axis , keepdims = True ) terms_divided_by_max = signs * jnp . exp ( vals - max_val ) if weights is not None : # swap axis and -1 to conform to jnp.dot and register_batch_dense api terms_divided_by_max = jnp . swapaxes ( terms_divided_by_max , axis , - 1 ) transformed_divided_by_max = jnp . dot ( terms_divided_by_max , weights ) if register_kfac : transformed_divided_by_max = register_batch_dense ( transformed_divided_by_max , terms_divided_by_max , weights , None ) # swap axis and -1 back after the contraction and registration transformed_divided_by_max = jnp . swapaxes ( transformed_divided_by_max , axis , - 1 ) else : transformed_divided_by_max = jnp . sum ( terms_divided_by_max , axis = axis , keepdims = True ) signs = jnp . sign ( transformed_divided_by_max ) vals = jnp . log ( jnp . abs ( transformed_divided_by_max )) + max_val return signs , vals","title":"log_linear_exp"},{"location":"api/utils/log_linear_exp/#vmcnet.utils.log_linear_exp.log_linear_exp","text":"Stably compute sign and log(abs(.)) of sum_i(sign_i * w_ij * exp(vals_i)) + b_j. In order to avoid overflow when computing log(abs(sum_i(sign_i * w_ij * exp(vals_i)))), the largest exp(val_i) is divided out from all the values and added back in after the outer log, i.e. log(abs(sum_i(sign_i * w_ij * exp(vals_i - max)))) + max. This trick also avoids the underflow issue of when all vals are small enough that exp(val_i) is approximately 0 for all i. Parameters: Name Type Description Default signs Array array of signs of the input x with shape (..., d, ...), where d is the size of the given axis required vals Array array of log|abs(x)| with shape (..., d, ...), where d is the size of the given axis required weights Array weights of a linear transformation to apply to the given axis, with shape (d, d'). If not provided, a simple sum is taken instead, equivalent to (d, 1) weights equal to 1. Defaults to None. None axis int axis along which to take the sum and max. Defaults to 0. 0 register_kfac bool if weights are not None, whether to register the linear part of the computation with KFAC. Defaults to True. True Returns: Type Description (SLArray) sign of linear combination, log of linear combination. Both outputs have shape (..., d', ...), where d' = 1 if weights is None, and d' = weights.shape[1] otherwise. Source code in vmcnet/utils/log_linear_exp.py def log_linear_exp ( signs : Array , vals : Array , weights : Optional [ Array ] = None , axis : int = 0 , register_kfac : bool = True , ) -> SLArray : \"\"\"Stably compute sign and log(abs(.)) of sum_i(sign_i * w_ij * exp(vals_i)) + b_j. In order to avoid overflow when computing log(abs(sum_i(sign_i * w_ij * exp(vals_i)))), the largest exp(val_i) is divided out from all the values and added back in after the outer log, i.e. log(abs(sum_i(sign_i * w_ij * exp(vals_i - max)))) + max. This trick also avoids the underflow issue of when all vals are small enough that exp(val_i) is approximately 0 for all i. Args: signs (Array): array of signs of the input x with shape (..., d, ...), where d is the size of the given axis vals (Array): array of log|abs(x)| with shape (..., d, ...), where d is the size of the given axis weights (Array, optional): weights of a linear transformation to apply to the given axis, with shape (d, d'). If not provided, a simple sum is taken instead, equivalent to (d, 1) weights equal to 1. Defaults to None. axis (int, optional): axis along which to take the sum and max. Defaults to 0. register_kfac (bool, optional): if weights are not None, whether to register the linear part of the computation with KFAC. Defaults to True. Returns: (SLArray): sign of linear combination, log of linear combination. Both outputs have shape (..., d', ...), where d' = 1 if weights is None, and d' = weights.shape[1] otherwise. \"\"\" max_val = jnp . max ( vals , axis = axis , keepdims = True ) terms_divided_by_max = signs * jnp . exp ( vals - max_val ) if weights is not None : # swap axis and -1 to conform to jnp.dot and register_batch_dense api terms_divided_by_max = jnp . swapaxes ( terms_divided_by_max , axis , - 1 ) transformed_divided_by_max = jnp . dot ( terms_divided_by_max , weights ) if register_kfac : transformed_divided_by_max = register_batch_dense ( transformed_divided_by_max , terms_divided_by_max , weights , None ) # swap axis and -1 back after the contraction and registration transformed_divided_by_max = jnp . swapaxes ( transformed_divided_by_max , axis , - 1 ) else : transformed_divided_by_max = jnp . sum ( terms_divided_by_max , axis = axis , keepdims = True ) signs = jnp . sign ( transformed_divided_by_max ) vals = jnp . log ( jnp . abs ( transformed_divided_by_max )) + max_val return signs , vals","title":"log_linear_exp()"},{"location":"api/utils/pytree_helpers/","text":"Helper functions for pytrees. tree_sum ( tree1 , tree2 ) Leaf-wise sum of two pytrees with the same structure. Source code in vmcnet/utils/pytree_helpers.py def tree_sum ( tree1 : T , tree2 : T ) -> T : \"\"\"Leaf-wise sum of two pytrees with the same structure.\"\"\" return jax . tree_map ( lambda a , b : a + b , tree1 , tree2 ) tree_prod ( tree1 , tree2 ) Leaf-wise product of two pytrees with the same structure. Source code in vmcnet/utils/pytree_helpers.py def tree_prod ( tree1 : T , tree2 : T ) -> T : \"\"\"Leaf-wise product of two pytrees with the same structure.\"\"\" return jax . tree_map ( lambda a , b : a * b , tree1 , tree2 ) multiply_tree_by_scalar ( tree , scalar ) Multiply all leaves of a pytree by a scalar. Source code in vmcnet/utils/pytree_helpers.py def multiply_tree_by_scalar ( tree : T , scalar : jnp . float32 ) -> T : \"\"\"Multiply all leaves of a pytree by a scalar.\"\"\" return jax . tree_map ( lambda x : scalar * x , tree ) tree_inner_product ( tree1 , tree2 ) Inner product of two pytrees with the same structure. Source code in vmcnet/utils/pytree_helpers.py def tree_inner_product ( tree1 : T , tree2 : T ) -> Array : \"\"\"Inner product of two pytrees with the same structure.\"\"\" leaf_inner_prods = jax . tree_map ( lambda a , b : jnp . sum ( a * b ), tree1 , tree2 ) return jnp . sum ( jax . flatten_util . ravel_pytree ( leaf_inner_prods )[ 0 ]) tree_reduce_l1 ( xs ) L1 norm of a pytree as a flattened vector. Source code in vmcnet/utils/pytree_helpers.py def tree_reduce_l1 ( xs : PyTree ) -> jnp . float32 : \"\"\"L1 norm of a pytree as a flattened vector.\"\"\" concat_xs , _ = jax . flatten_util . ravel_pytree ( xs ) return jnp . sum ( jnp . abs ( concat_xs ))","title":"pytree_helpers"},{"location":"api/utils/pytree_helpers/#vmcnet.utils.pytree_helpers.tree_sum","text":"Leaf-wise sum of two pytrees with the same structure. Source code in vmcnet/utils/pytree_helpers.py def tree_sum ( tree1 : T , tree2 : T ) -> T : \"\"\"Leaf-wise sum of two pytrees with the same structure.\"\"\" return jax . tree_map ( lambda a , b : a + b , tree1 , tree2 )","title":"tree_sum()"},{"location":"api/utils/pytree_helpers/#vmcnet.utils.pytree_helpers.tree_prod","text":"Leaf-wise product of two pytrees with the same structure. Source code in vmcnet/utils/pytree_helpers.py def tree_prod ( tree1 : T , tree2 : T ) -> T : \"\"\"Leaf-wise product of two pytrees with the same structure.\"\"\" return jax . tree_map ( lambda a , b : a * b , tree1 , tree2 )","title":"tree_prod()"},{"location":"api/utils/pytree_helpers/#vmcnet.utils.pytree_helpers.multiply_tree_by_scalar","text":"Multiply all leaves of a pytree by a scalar. Source code in vmcnet/utils/pytree_helpers.py def multiply_tree_by_scalar ( tree : T , scalar : jnp . float32 ) -> T : \"\"\"Multiply all leaves of a pytree by a scalar.\"\"\" return jax . tree_map ( lambda x : scalar * x , tree )","title":"multiply_tree_by_scalar()"},{"location":"api/utils/pytree_helpers/#vmcnet.utils.pytree_helpers.tree_inner_product","text":"Inner product of two pytrees with the same structure. Source code in vmcnet/utils/pytree_helpers.py def tree_inner_product ( tree1 : T , tree2 : T ) -> Array : \"\"\"Inner product of two pytrees with the same structure.\"\"\" leaf_inner_prods = jax . tree_map ( lambda a , b : jnp . sum ( a * b ), tree1 , tree2 ) return jnp . sum ( jax . flatten_util . ravel_pytree ( leaf_inner_prods )[ 0 ])","title":"tree_inner_product()"},{"location":"api/utils/pytree_helpers/#vmcnet.utils.pytree_helpers.tree_reduce_l1","text":"L1 norm of a pytree as a flattened vector. Source code in vmcnet/utils/pytree_helpers.py def tree_reduce_l1 ( xs : PyTree ) -> jnp . float32 : \"\"\"L1 norm of a pytree as a flattened vector.\"\"\" concat_xs , _ = jax . flatten_util . ravel_pytree ( xs ) return jnp . sum ( jnp . abs ( concat_xs ))","title":"tree_reduce_l1()"},{"location":"api/utils/slog_helpers/","text":"Helper functions for dealing with (sign, logabs) data. array_to_slog ( x ) Converts a regular array into (sign, logabs) form. Parameters: Name Type Description Default x Array input data. required Returns: Type Description (SLArray) data in form (sign(x), log(abs(x))) Source code in vmcnet/utils/slog_helpers.py def array_to_slog ( x : Array ) -> SLArray : \"\"\"Converts a regular array into (sign, logabs) form. Args: x (Array): input data. Returns: (SLArray): data in form (sign(x), log(abs(x))) \"\"\" return ( jnp . sign ( x ), jnp . log ( jnp . abs ( x ))) array_from_slog ( x ) Converts an slog data tuple into a regular array. Parameters: Name Type Description Default x SLArray input data in slog form. This data looks like (sign(z), log(abs(z))) for some z which represents the underlying data. required Returns: Type Description (Array) the data as a single, regular array. In other words, the z such that x = (sign(z), log(abs(z))) Source code in vmcnet/utils/slog_helpers.py def array_from_slog ( x : SLArray ) -> Array : \"\"\"Converts an slog data tuple into a regular array. Args: x (SLArray): input data in slog form. This data looks like (sign(z), log(abs(z))) for some z which represents the underlying data. Returns: (Array): the data as a single, regular array. In other words, the z such that x = (sign(z), log(abs(z))) \"\"\" return x [ 0 ] * jnp . exp ( x [ 1 ]) array_list_to_slog ( x ) Map an ArrayList to SLArrayList form. Parameters: Name Type Description Default x ArrayList input data as a regular spin-split array. required Returns: Type Description (SLArrayList) same data with each array transformed to slog form. Source code in vmcnet/utils/slog_helpers.py def array_list_to_slog ( x : ArrayList ) -> SLArrayList : \"\"\"Map an ArrayList to SLArrayList form. Args: x (ArrayList): input data as a regular spin-split array. Returns: (SLArrayList): same data with each array transformed to slog form. \"\"\" return [ array_to_slog ( arr ) for arr in x ] array_list_from_slog ( x ) Map a SLArrayList to ArrayList form. Parameters: Name Type Description Default x SLArrayList input data as a list of slog arrays. required Returns: Type Description (ArrayList) same data with slog tuples transformed to single arrays. Source code in vmcnet/utils/slog_helpers.py def array_list_from_slog ( x : SLArrayList ) -> ArrayList : \"\"\"Map a SLArrayList to ArrayList form. Args: x (SLArrayList): input data as a list of slog arrays. Returns: (ArrayList): same data with slog tuples transformed to single arrays. \"\"\" return [ array_from_slog ( slog ) for slog in x ] slog_multiply ( x , y ) Computes the product of two slog array tuples, as another slog array tuple. Signs are multiplied and logs are added. Source code in vmcnet/utils/slog_helpers.py def slog_multiply ( x : SLArray , y : SLArray ) -> SLArray : \"\"\"Computes the product of two slog array tuples, as another slog array tuple. Signs are multiplied and logs are added. \"\"\" ( sx , lx ) = x ( sy , ly ) = y return ( sx * sy , lx + ly ) slog_sum_over_axis ( x , axis = 0 ) Take the sum of a single slog array over a specified axis. Source code in vmcnet/utils/slog_helpers.py def slog_sum_over_axis ( x : SLArray , axis : int = 0 ) -> SLArray : \"\"\"Take the sum of a single slog array over a specified axis.\"\"\" signs , logs = log_linear_exp ( x [ 0 ], x [ 1 ], axis = axis ) return ( jnp . squeeze ( signs , axis = axis ), jnp . squeeze ( logs , axis = axis )) slog_array_list_sum ( x ) Take the sum of a list of SLArrays which are all of the same shape. Source code in vmcnet/utils/slog_helpers.py def slog_array_list_sum ( x : SLArrayList ) -> SLArray : \"\"\"Take the sum of a list of SLArrays which are all of the same shape.\"\"\" stacked_vals = ( jnp . stack ([ a [ 0 ] for a in x ]), jnp . stack ([ a [ 1 ] for a in x ])) return slog_sum_over_axis ( stacked_vals ) slog_sum ( x , y ) Take the sum of two SLArrays which are of the same shape. Source code in vmcnet/utils/slog_helpers.py def slog_sum ( x : SLArray , y : SLArray ) -> SLArray : \"\"\"Take the sum of two SLArrays which are of the same shape.\"\"\" return slog_array_list_sum ([ x , y ])","title":"slog_helpers"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.array_to_slog","text":"Converts a regular array into (sign, logabs) form. Parameters: Name Type Description Default x Array input data. required Returns: Type Description (SLArray) data in form (sign(x), log(abs(x))) Source code in vmcnet/utils/slog_helpers.py def array_to_slog ( x : Array ) -> SLArray : \"\"\"Converts a regular array into (sign, logabs) form. Args: x (Array): input data. Returns: (SLArray): data in form (sign(x), log(abs(x))) \"\"\" return ( jnp . sign ( x ), jnp . log ( jnp . abs ( x )))","title":"array_to_slog()"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.array_from_slog","text":"Converts an slog data tuple into a regular array. Parameters: Name Type Description Default x SLArray input data in slog form. This data looks like (sign(z), log(abs(z))) for some z which represents the underlying data. required Returns: Type Description (Array) the data as a single, regular array. In other words, the z such that x = (sign(z), log(abs(z))) Source code in vmcnet/utils/slog_helpers.py def array_from_slog ( x : SLArray ) -> Array : \"\"\"Converts an slog data tuple into a regular array. Args: x (SLArray): input data in slog form. This data looks like (sign(z), log(abs(z))) for some z which represents the underlying data. Returns: (Array): the data as a single, regular array. In other words, the z such that x = (sign(z), log(abs(z))) \"\"\" return x [ 0 ] * jnp . exp ( x [ 1 ])","title":"array_from_slog()"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.array_list_to_slog","text":"Map an ArrayList to SLArrayList form. Parameters: Name Type Description Default x ArrayList input data as a regular spin-split array. required Returns: Type Description (SLArrayList) same data with each array transformed to slog form. Source code in vmcnet/utils/slog_helpers.py def array_list_to_slog ( x : ArrayList ) -> SLArrayList : \"\"\"Map an ArrayList to SLArrayList form. Args: x (ArrayList): input data as a regular spin-split array. Returns: (SLArrayList): same data with each array transformed to slog form. \"\"\" return [ array_to_slog ( arr ) for arr in x ]","title":"array_list_to_slog()"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.array_list_from_slog","text":"Map a SLArrayList to ArrayList form. Parameters: Name Type Description Default x SLArrayList input data as a list of slog arrays. required Returns: Type Description (ArrayList) same data with slog tuples transformed to single arrays. Source code in vmcnet/utils/slog_helpers.py def array_list_from_slog ( x : SLArrayList ) -> ArrayList : \"\"\"Map a SLArrayList to ArrayList form. Args: x (SLArrayList): input data as a list of slog arrays. Returns: (ArrayList): same data with slog tuples transformed to single arrays. \"\"\" return [ array_from_slog ( slog ) for slog in x ]","title":"array_list_from_slog()"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.slog_multiply","text":"Computes the product of two slog array tuples, as another slog array tuple. Signs are multiplied and logs are added. Source code in vmcnet/utils/slog_helpers.py def slog_multiply ( x : SLArray , y : SLArray ) -> SLArray : \"\"\"Computes the product of two slog array tuples, as another slog array tuple. Signs are multiplied and logs are added. \"\"\" ( sx , lx ) = x ( sy , ly ) = y return ( sx * sy , lx + ly )","title":"slog_multiply()"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.slog_sum_over_axis","text":"Take the sum of a single slog array over a specified axis. Source code in vmcnet/utils/slog_helpers.py def slog_sum_over_axis ( x : SLArray , axis : int = 0 ) -> SLArray : \"\"\"Take the sum of a single slog array over a specified axis.\"\"\" signs , logs = log_linear_exp ( x [ 0 ], x [ 1 ], axis = axis ) return ( jnp . squeeze ( signs , axis = axis ), jnp . squeeze ( logs , axis = axis ))","title":"slog_sum_over_axis()"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.slog_array_list_sum","text":"Take the sum of a list of SLArrays which are all of the same shape. Source code in vmcnet/utils/slog_helpers.py def slog_array_list_sum ( x : SLArrayList ) -> SLArray : \"\"\"Take the sum of a list of SLArrays which are all of the same shape.\"\"\" stacked_vals = ( jnp . stack ([ a [ 0 ] for a in x ]), jnp . stack ([ a [ 1 ] for a in x ])) return slog_sum_over_axis ( stacked_vals )","title":"slog_array_list_sum()"},{"location":"api/utils/slog_helpers/#vmcnet.utils.slog_helpers.slog_sum","text":"Take the sum of two SLArrays which are of the same shape. Source code in vmcnet/utils/slog_helpers.py def slog_sum ( x : SLArray , y : SLArray ) -> SLArray : \"\"\"Take the sum of two SLArrays which are of the same shape.\"\"\" return slog_array_list_sum ([ x , y ])","title":"slog_sum()"}]}